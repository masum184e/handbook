{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970e1c7f-2ff3-4126-917d-4905314adbcb",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Basics\n",
    "    - Image\n",
    "        - Load\n",
    "        - Display\n",
    "        - Save\n",
    "    - Video\n",
    "        - Capture\n",
    "        - Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c69b7-47c0-40d6-926c-1bc6c7994fee",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Installation:**\n",
    "```shell\n",
    "pip install opencv-python\n",
    "```\n",
    "**Import:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02038943-be89-49d8-887b-da946ae01372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f75ab-8a82-47d7-8ce2-631784fe0b6c",
   "metadata": {},
   "source": [
    "# Basics\n",
    "## Image\n",
    "### Load\n",
    "```python\n",
    "cv2.imread(filename, flags)\n",
    "```\n",
    "**Parameters:**\n",
    "- `filename`: The path of the image file.\n",
    "- `flags`: Specifies how the image should be read. Some common values:\n",
    "    - `cv2.IMREAD_COLOR` (default) – Loads a color image (ignores transparency).\n",
    "    - `cv2.IMREAD_GRAYSCALE` – Loads the image in grayscale mode.\n",
    "    - `cv2.IMREAD_UNCHANGED` – Loads the image as it is (including alpha channel, if present).\n",
    "### Display\n",
    "```python\n",
    "cv2.imshow(window_name, image)\n",
    "```\n",
    "**Parameters:**\n",
    "- `window_name`: A string representing the name of the display window.\n",
    "- `image`: The image data read by cv2.imread().\n",
    "### Save\n",
    "```python\n",
    "cv2.imwrite(filename, image)\n",
    "```\n",
    "**Parameters:**\n",
    "- `filename`: Name of the output image file.\n",
    "- `image`: The image data to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f63a4a1-2f28-428e-a8ec-a098e6fd176d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Loaded Successfully!\n",
      "Grayscale image saved.\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"./images/redbox.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image\")\n",
    "else:\n",
    "    print(\"Image Loaded Successfully!\")\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(\"gray_example.jpg\", gray_image)\n",
    "    print(\"Grayscale image saved.\")\n",
    "    cv2.imshow(\"Grayscale Image\", gray_image)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd2997-1665-452a-9a6b-106669f7ffa8",
   "metadata": {},
   "source": [
    "- `openCV.waitKey(0)` -  allows users to display a window for given milliseconds or until any key is pressed. If the parameter value is 0, you have to press any key from your keyboard to destroy the window, untill it will keep open. If the parameter value is other value instead of 0, it will automatically destroy the window after that amount of milliseconds. It return value is the key that was pressed.\n",
    "- `openCV.destroyAllWindows()` - close all open window. [View More](https://www.geeksforgeeks.org/python-opencv-destroyallwindows-function/)\n",
    "- `destroyWindow(windName)` - close a specif window\n",
    "\n",
    "## Video\n",
    "### Capture\n",
    "```python\n",
    "cv2.VideoCapture(source)\n",
    "```\n",
    "**Parameters:**\n",
    "- `source`: Specifies the video source.\n",
    "    - `0` for the default webcam.\n",
    "    - `1, 2, ...` for external cameras.\n",
    "    - `\"filename.mp4\"` to load a video file.\n",
    "### Save\n",
    "```python\n",
    "cv2.VideoWriter(filename, fourcc, fps, frame_size)\n",
    "```\n",
    "**Parameters:**\n",
    "- `filename`: Name of the output file.\n",
    "- `fourcc`: Codec used for compression.\n",
    "- `fps`: Frames per second.\n",
    "- `frame_size`: Width and height of the frame (`width`, `height`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89b5452-f1b5-4193-8c28-c57c6feb2a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Can't receive frame. Exiting...\n",
      "Video End\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"./images/sample.mp4\")\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('webcam_output.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open webcam\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Can't receive frame. Exiting...\")\n",
    "        print(\"Video End\")\n",
    "        break\n",
    "\n",
    "    out.write(frame)\n",
    "    cv2.imshow('Live Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b832e9f-1e9d-4916-aa88-0a6a0bf8201f",
   "metadata": {},
   "source": [
    "- `cv2.VideoWriter_fourcc(*'XVID')` specifies the codec (other options: 'MJPG', 'MP4V').\n",
    "- `cv2.VideoWriter('output.avi', fourcc, 20.0, (width, height))` creates a writer object that saves at 20 FPS.\n",
    "- `cap.isOpened()` checks if the webcam is accessible.\n",
    "- Inside the while loop:\n",
    "    - `cap.read()` captures a frame.\n",
    "    - If `ret` is `True`, the frame is displayed using `cv2.imshow()`.\n",
    "    - The loop continues until the user presses the 'q' key (`cv2.waitKey(1) & 0xFF == ord('q')`).\n",
    "- `cap.release()` releases the camera resource.\n",
    "- `cv2.destroyAllWindows()` closes all OpenCV windows.cap.get(3) and cap.get(4) get the frame width and height.\n",
    "- `out.write(frame)` writes each frame to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e110840-f974-4a50-956d-3a65ca476950",
   "metadata": {},
   "source": [
    "# Drawing\n",
    "## Draw Line\n",
    "- `dtype=np.uint8` - specifies the data type of the elements in the array.\n",
    "- `np.zeros((3, 3))` - creates a 3x3 array filled with zeros\n",
    "- `np.ones((3, 3))` - creates a 3x3 array filled with ones\n",
    "- `np.full((2, 2), 5)` - creates a 2x2 array filled with the value 5\n",
    "- `np.empty((2, 2))` - creates a 2x2 uninitialized array\n",
    "- `np.eye(3)` or `np.identity(3)` - creates a 3x3 identity matrix\n",
    "- `np.ones((512, 512, 3))` - creates 512x512 array which filled with 1x3 array which is filled with the value 1\n",
    "- `255*np.ones((512, 512, 3))` - creates a 3-dimensional with 512 rows, 512 columns and 3 depth or color channels. It means each of the value of 512*512 array have another array which contains a list of 3 value represnt rgb color code. It is used to create white image.\n",
    "## Draw Circle\n",
    "- `thickness=-1` or `thickness=openCV.FILLED` - filled the shape with color\n",
    "## Draw Rectangle\n",
    "- `top_left` - specify the position of top left corner of the rectangle\n",
    "- `bottom_right` - specify the position of bottom right corner of the rectangle\n",
    "## Draw Ellipse\n",
    "- `center_coordinates` - Specifies the center of the ellipse.\n",
    "- `axes_length` - Specifies the length of horizontal & vertical axes\n",
    "- `angle` - Specifies the rotation angle of the ellipse (in degrees) from horizontal line.\n",
    "- `startAngle` - Specifies the angle (in degrees) at which the ellipse arc starts. It defines the beginning of the arc.\n",
    "- `endAngle` - Specifies the angle (in degrees) at which the ellipse arc ends. It defines the termination point of the arc.\n",
    "## Draw Polygon\n",
    "- `isClosed` - A boolean flag indicating whether the last point should be connected to the first point to form a closed loop.\n",
    "- `pts` - it ontains the coordinates of the vertices of the polygon. Each row represents a vertex, and the polygon will be formed by connecting these vertices in the order they appear in the array.\n",
    "## Draw Text\n",
    "- `position` - The coordinates (x, y) where the text should be positioned on the image.\n",
    "- `fontFamily` - The font style to be used for the text. \n",
    "- `fontScale` - The scale factor that multiplies the font size.\n",
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50e47ca-c4a1-4a87-b6f5-32bd9ad9caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a black image\n",
    "image = np.zeros((500, 500, 3), dtype=np.uint8)\n",
    "\n",
    "# Draw shapes\n",
    "cv2.line(image, (50, 50), (450, 50), (0, 255, 0), 5)\n",
    "cv2.rectangle(image, (100, 100), (400, 300), (255, 0, 0), 3)\n",
    "cv2.circle(image, (250, 250), 100, (0, 0, 255), -1)\n",
    "cv2.ellipse(image, (250, 250), (150, 100), 0, 0, 180, (255, 255, 0), 2)\n",
    "\n",
    "# Draw a polygon\n",
    "points = np.array([[100, 300], [200, 400], [300, 400], [400, 300]], np.int32)\n",
    "points = points.reshape((-1, 1, 2))\n",
    "cv2.polylines(image, [points], isClosed=True, color=(0, 255, 255), thickness=3)\n",
    "\n",
    "# Add text\n",
    "cv2.putText(image, 'OpenCV Drawing', (50, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "# Show the image\n",
    "cv2.imshow(\"Drawing Shapes\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b2753-e923-4fe1-b8cc-0fd8d93e36a7",
   "metadata": {},
   "source": [
    "- `np.zeros((500, 500, 3), dtype=np.uint8)`: Creates a black image of size 500x500 with 3 color channels (BGR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec0199-0109-4f64-984f-4ddab90796b5",
   "metadata": {},
   "source": [
    "# Operation\n",
    "## Cropping\n",
    "Cropping refers to extracting a specific region from an image.\n",
    "\n",
    "In OpenCV, images are represented as NumPy arrays of pixels. We can crop an image using array slicing.\n",
    "```python\n",
    "cropped_image = image[y1:y2, x1:x2]\n",
    "```\n",
    "- `(x1, y1)`: Top-left corner of the region to crop.\n",
    "- `(x2, y2)`: Bottom-right corner of the region to crop.\n",
    "- `image[y1:y2, x1:x2]` extracts the required portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51e1e14-3348-4b11-8888-8e4b53cb115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = image[50:300, 100:400]  \n",
    "cv2.imshow(\"Cropped Image\", cropped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa34d8d-3e90-458c-9f4c-212c66110e47",
   "metadata": {},
   "source": [
    "## Ratation\n",
    "Rotation refers to turning an image around its center by a specific angle.\n",
    "\n",
    "To rotate an image:\n",
    "\n",
    "1. Compute the rotation matrix using `cv2.getRotationMatrix2D()`\n",
    "2. Apply the transformation using `cv2.warpAffine()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea101a-6532-4780-89dc-5a9f645724ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image dimensions\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# Define the center of rotation\n",
    "center = (w // 2, h // 2)\n",
    "\n",
    "# Compute the rotation matrix (Rotate 45 degrees)\n",
    "M = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
    "\n",
    "# Apply the affine transformation\n",
    "rotated = cv2.warpAffine(image, M, (w, h))\n",
    "\n",
    "# Show the rotated image\n",
    "cv2.imshow(\"Rotated Image\", rotated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f0147f-ff42-4013-869e-238b943735f6",
   "metadata": {},
   "source": [
    "### Rotation Without Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54753fd3-38b4-47b9-9d97-1ae3ec3e101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the new bounding dimensions\n",
    "new_w = int(h * abs(np.sin(np.radians(45))) + w * abs(np.cos(np.radians(45))))\n",
    "new_h = int(h * abs(np.cos(np.radians(45))) + w * abs(np.sin(np.radians(45))))\n",
    "\n",
    "# Compute the new rotation matrix\n",
    "center = (w // 2, h // 2)\n",
    "M = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
    "\n",
    "# Adjust translation to fit the entire image\n",
    "M[0, 2] += (new_w - w) / 2\n",
    "M[1, 2] += (new_h - h) / 2\n",
    "\n",
    "# Apply the rotation\n",
    "rotated = cv2.warpAffine(image, M, (new_w, new_h))\n",
    "\n",
    "# Show the rotated image\n",
    "cv2.imshow(\"Rotated Image Without Cropping\", rotated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bf598-286f-4508-ab30-82eb22a173af",
   "metadata": {},
   "source": [
    "# MediaPipe\n",
    "MediaPipe is a powerful framework by Google that enables real-time detection of hands, face, and pose landmarks using deep learning.\n",
    "\n",
    "**Installation:**\n",
    "```shell\n",
    "pip install mediapipe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca28a351-6cfb-47cd-a170-d37ce8f24dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995aaf7-6735-49d4-be56-9b8862867cf8",
   "metadata": {},
   "source": [
    "### Hand Landmarks\n",
    "MediaPipe Hands consists of two primary models:\n",
    "- **Palm Detection Model**: It detects the location of the hand in the image.\n",
    "- **Hand Landmark Model**: It predicts 21 keypoints (landmarks) for each detected hand.\n",
    "\n",
    "**Palm Detection Model:**\n",
    "- Identifies the general region where a hand is located.\n",
    "- It does not detect individual fingers or landmarks.\n",
    "- Runs once per video sequence and updates as needed.\n",
    "\n",
    "**Hand Landmark Model:**\n",
    "- Identifies 21 hand landmarks once the palm is detected.\n",
    "- Works frame-by-frame, refining detection over time.\n",
    "\n",
    "![Image](https://ai.google.dev/static/edge/mediapipe/images/solutions/hand-landmarks.png)\n",
    "\n",
    "#### Steps\n",
    "1. **Initialize MediaPipe Hands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb41fff-b966-4fb6-90c3-3be30b48bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090d2ba-5ad2-4493-a736-9ef60c4d5b18",
   "metadata": {},
   "source": [
    "- `mp.solutions.hands` - is used to access the Hands module from MediaPipe's solutions.\n",
    "- `Hands()` initializes the hand tracking model.\n",
    "- `static_image_mode=False`: Detect hands in a continuous video stream.\n",
    "- `max_num_hands=2`: Detect up to 2 hands.\n",
    "- `min_detection_confidence=0.5`: Minimum confidence for detection (range 0.0 to 1.0).\n",
    "- `mp_draw` is used for drawing landmarks on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdec64-4260-4ef4-9ea5-b846a9a5f05d",
   "metadata": {},
   "source": [
    "2. **Process Frame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94947b9e-9bb6-4a14-8eba-cb8835a7d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adb3e2-b580-42d6-ac2b-9e36e7181d1a",
   "metadata": {},
   "source": [
    "- Converts the frame to RGB (because MediaPipe requires RGB input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93daacf5-14d6-41f9-9e20-3d4368bfc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hands.process(frame_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66ac91-494d-4b88-86f1-470a5370694f",
   "metadata": {},
   "source": [
    "The `results` object is of type `mediapipe.python.solution_base.SolutionOutputs` and has the following attributes:\n",
    "\n",
    "a. `results.multi_hand_landmarks`\n",
    "- A list of detected hands, where each hand contains 21 landmarks.\n",
    "- Each landmark has `x, y, z` coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8d480-ae9c-41ca-835d-c33d16993e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        print(hand_landmarks)  # Prints all 21 landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abe6c1-359f-4aa9-9527-5b72945428e0",
   "metadata": {},
   "source": [
    "Each landmark is represented as `landmark.x`, `landmark.y`, and `landmark.z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5bda8-92d6-4605-b710-6f5295e21653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "    print(f\"Landmark {idx}: x={landmark.x}, y={landmark.y}, z={landmark.z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb68c4e-d8fb-4c7a-82d4-b8d15916e526",
   "metadata": {},
   "source": [
    "The `x` and `y` coordinates are normalized (range: `0-1`), so multiply them by image width/height to get pixel values.\n",
    "\n",
    "The `z` value represents depth but is relative to the wrist (not in actual units like meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d256d0f9-478b-4658-afbd-85af36ef3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "    h, w, c = frame.shape\n",
    "    cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "    print(f\"Landmark {idx}: ({cx}, {cy})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268185b-9a63-4d04-a873-222a556cd8ac",
   "metadata": {},
   "source": [
    "b. `results.multi_hand_world_landmarks`\n",
    "- Similar to `multi_hand_landmarks`, but provides 3D coordinates in real-world space\n",
    "\n",
    "c. `results.multi_handedness`\n",
    "- Contains information about which hand (left or right) was detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1aabc-4382-4dff-9f24-2f4e101786ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.multi_handedness:\n",
    "    for idx, hand in enumerate(results.multi_handedness):\n",
    "        print(f\"Hand {idx}: {hand.classification[0].label}\")  # \"Left\" or \"Right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cb974-ec59-4961-85e0-71299254be69",
   "metadata": {},
   "source": [
    "3. **Draw Landmarks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad3c2c-381a-4ceb-9d97-4bdd3fee4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.multi_hand_landmarks:\n",
    "    for hand_landmarks in result.multi_hand_landmarks:\n",
    "        mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b80854-c015-4602-9b46-7b30874979a2",
   "metadata": {},
   "source": [
    "If hands are detected, iterate over each hand and draw landmarks using `mp_draw.draw_landmarks()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
