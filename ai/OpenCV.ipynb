{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4948e77d-8d4b-4e24-bd36-457700d47537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f75ab-8a82-47d7-8ce2-631784fe0b6c",
   "metadata": {},
   "source": [
    "# Basics\n",
    "## Image\n",
    "### Load\n",
    "```python\n",
    "cv2.imread(filename, flags)\n",
    "```\n",
    "**Parameters:**\n",
    "- `filename`: The path of the image file.\n",
    "- `flags`: Specifies how the image should be read. Some common values:\n",
    "    - `cv2.IMREAD_COLOR` (default) – Loads a color image (ignores transparency).\n",
    "    - `cv2.IMREAD_GRAYSCALE` – Loads the image in grayscale mode.\n",
    "    - `cv2.IMREAD_UNCHANGED` – Loads the image as it is (including alpha channel, if present).\n",
    "### Display\n",
    "```python\n",
    "cv2.imshow(window_name, image)\n",
    "```\n",
    "**Parameters:**\n",
    "- `window_name`: A string representing the name of the display window.\n",
    "- `image`: The image data read by cv2.imread().\n",
    "### Save\n",
    "```python\n",
    "cv2.imwrite(filename, image)\n",
    "```\n",
    "**Parameters:**\n",
    "- `filename`: Name of the output image file.\n",
    "- `image`: The image data to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f63a4a1-2f28-428e-a8ec-a098e6fd176d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Loaded Successfully!\n",
      "Grayscale image saved.\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"./images/redbox.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Unable to load image\")\n",
    "else:\n",
    "    print(\"Image Loaded Successfully!\")\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(\"gray_example.jpg\", gray_image)\n",
    "    print(\"Grayscale image saved.\")\n",
    "    cv2.imshow(\"Grayscale Image\", gray_image)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd2997-1665-452a-9a6b-106669f7ffa8",
   "metadata": {},
   "source": [
    "- `openCV.waitKey(0)` -  allows users to display a window for given milliseconds or until any key is pressed. If the parameter value is 0, you have to press any key from your keyboard to destroy the window, untill it will keep open. If the parameter value is other value instead of 0, it will automatically destroy the window after that amount of milliseconds. It return value is the key that was pressed.\n",
    "- `openCV.destroyAllWindows()` - close all open window. [View More](https://www.geeksforgeeks.org/python-opencv-destroyallwindows-function/)\n",
    "- `destroyWindow(windName)` - close a specif window\n",
    "\n",
    "## Video\n",
    "### Capture\n",
    "```python\n",
    "cv2.VideoCapture(source)\n",
    "```\n",
    "**Parameters:**\n",
    "- `source`: Specifies the video source.\n",
    "    - `0` for the default webcam.\n",
    "    - `1, 2, ...` for external cameras.\n",
    "    - `\"filename.mp4\"` to load a video file.\n",
    "### Save\n",
    "```python\n",
    "cv2.VideoWriter(filename, fourcc, fps, frame_size)\n",
    "```\n",
    "**Parameters:**\n",
    "- `filename`: Name of the output file.\n",
    "- `fourcc`: Codec used for compression.\n",
    "- `fps`: Frames per second.\n",
    "- `frame_size`: Width and height of the frame (`width`, `height`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b89b5452-f1b5-4193-8c28-c57c6feb2a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Can't receive frame. Exiting...\n",
      "Video End\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"./images/sample.mp4\")\n",
    "\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('webcam_output.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open webcam\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Can't receive frame. Exiting...\")\n",
    "        print(\"Video End\")\n",
    "        break\n",
    "\n",
    "    out.write(frame)\n",
    "    cv2.imshow('Live Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b832e9f-1e9d-4916-aa88-0a6a0bf8201f",
   "metadata": {},
   "source": [
    "- `cv2.VideoWriter_fourcc(*'XVID')` specifies the codec (other options: 'MJPG', 'MP4V').\n",
    "- `cv2.VideoWriter('output.avi', fourcc, 20.0, (width, height))` creates a writer object that saves at 20 FPS.\n",
    "- `cap.isOpened()` checks if the webcam is accessible.\n",
    "- Inside the while loop:\n",
    "    - `cap.read()` captures a frame.\n",
    "    - If `ret` is `True`, the frame is displayed using `cv2.imshow()`.\n",
    "    - The loop continues until the user presses the 'q' key (`cv2.waitKey(1) & 0xFF == ord('q')`).\n",
    "- `cap.release()` releases the camera resource.\n",
    "- `cv2.destroyAllWindows()` closes all OpenCV windows.cap.get(3) and cap.get(4) get the frame width and height.\n",
    "- `out.write(frame)` writes each frame to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bf598-286f-4508-ab30-82eb22a173af",
   "metadata": {},
   "source": [
    "# MediaPipe\n",
    "MediaPipe is a powerful framework by Google that enables real-time detection of hands, face, and pose landmarks using deep learning.\n",
    "\n",
    "**Installation:**\n",
    "```shell\n",
    "pip install mediapipe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca28a351-6cfb-47cd-a170-d37ce8f24dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995aaf7-6735-49d4-be56-9b8862867cf8",
   "metadata": {},
   "source": [
    "### Hand Landmarks\n",
    "MediaPipe Hands consists of two primary models:\n",
    "- **Palm Detection Model**: It detects the location of the hand in the image.\n",
    "- **Hand Landmark Model**: It predicts 21 keypoints (landmarks) for each detected hand.\n",
    "\n",
    "**Palm Detection Model:**\n",
    "- Identifies the general region where a hand is located.\n",
    "- It does not detect individual fingers or landmarks.\n",
    "- Runs once per video sequence and updates as needed.\n",
    "\n",
    "**Hand Landmark Model:**\n",
    "- Identifies 21 hand landmarks once the palm is detected.\n",
    "- Works frame-by-frame, refining detection over time.\n",
    "\n",
    "![Image](https://ai.google.dev/static/edge/mediapipe/images/solutions/hand-landmarks.png)\n",
    "\n",
    "#### Steps\n",
    "1. **Initialize MediaPipe Hands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb41fff-b966-4fb6-90c3-3be30b48bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2090d2ba-5ad2-4493-a736-9ef60c4d5b18",
   "metadata": {},
   "source": [
    "- `mp.solutions.hands` - is used to access the Hands module from MediaPipe's solutions.\n",
    "- `Hands()` initializes the hand tracking model.\n",
    "- `static_image_mode=False`: Detect hands in a continuous video stream.\n",
    "- `max_num_hands=2`: Detect up to 2 hands.\n",
    "- `min_detection_confidence=0.5`: Minimum confidence for detection (range 0.0 to 1.0).\n",
    "- `mp_draw` is used for drawing landmarks on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdec64-4260-4ef4-9ea5-b846a9a5f05d",
   "metadata": {},
   "source": [
    "2. **Process Frame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94947b9e-9bb6-4a14-8eba-cb8835a7d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adb3e2-b580-42d6-ac2b-9e36e7181d1a",
   "metadata": {},
   "source": [
    "- Converts the frame to RGB (because MediaPipe requires RGB input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93daacf5-14d6-41f9-9e20-3d4368bfc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hands.process(frame_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66ac91-494d-4b88-86f1-470a5370694f",
   "metadata": {},
   "source": [
    "The `results` object is of type `mediapipe.python.solution_base.SolutionOutputs` and has the following attributes:\n",
    "\n",
    "a. `results.multi_hand_landmarks`\n",
    "- A list of detected hands, where each hand contains 21 landmarks.\n",
    "- Each landmark has `x, y, z` coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8d480-ae9c-41ca-835d-c33d16993e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        print(hand_landmarks)  # Prints all 21 landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abe6c1-359f-4aa9-9527-5b72945428e0",
   "metadata": {},
   "source": [
    "Each landmark is represented as `landmark.x`, `landmark.y`, and `landmark.z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5bda8-92d6-4605-b710-6f5295e21653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "    print(f\"Landmark {idx}: x={landmark.x}, y={landmark.y}, z={landmark.z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb68c4e-d8fb-4c7a-82d4-b8d15916e526",
   "metadata": {},
   "source": [
    "The `x` and `y` coordinates are normalized (range: `0-1`), so multiply them by image width/height to get pixel values.\n",
    "\n",
    "The `z` value represents depth but is relative to the wrist (not in actual units like meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d256d0f9-478b-4658-afbd-85af36ef3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "    h, w, c = frame.shape\n",
    "    cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "    print(f\"Landmark {idx}: ({cx}, {cy})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268185b-9a63-4d04-a873-222a556cd8ac",
   "metadata": {},
   "source": [
    "b. `results.multi_hand_world_landmarks`\n",
    "- Similar to `multi_hand_landmarks`, but provides 3D coordinates in real-world space\n",
    "\n",
    "c. `results.multi_handedness`\n",
    "- Contains information about which hand (left or right) was detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1aabc-4382-4dff-9f24-2f4e101786ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.multi_handedness:\n",
    "    for idx, hand in enumerate(results.multi_handedness):\n",
    "        print(f\"Hand {idx}: {hand.classification[0].label}\")  # \"Left\" or \"Right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cb974-ec59-4961-85e0-71299254be69",
   "metadata": {},
   "source": [
    "3. **Draw Landmarks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad3c2c-381a-4ceb-9d97-4bdd3fee4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.multi_hand_landmarks:\n",
    "    for hand_landmarks in result.multi_hand_landmarks:\n",
    "        mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b80854-c015-4602-9b46-7b30874979a2",
   "metadata": {},
   "source": [
    "If hands are detected, iterate over each hand and draw landmarks using `mp_draw.draw_landmarks()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
