{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- What is Machine Learning?\n",
    "- How Does Machine Learning Work?\n",
    "- Types of Machine Learning\n",
    "- Key Concepts of Machine Learning\n",
    "- Machine Learning Algorithm\n",
    "    - Supervised Learning Algorithm\n",
    "    - Unsupervised Learning Algorithm\n",
    "    - Reinforcement Learning Algorithm\n",
    "    - Deep Learning Algorithm\n",
    "- Machine Learning Workflow\n",
    "- Deep Learning\n",
    "- Applications of Machine Learning\n",
    "- Visualization technique according to algorithm\n",
    "- Evaluation Metrics according to algorithm\n",
    "- Confusion Matrix\n",
    "- Algorithm implmentation steps\n",
    "\n",
    "# What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on developing systems that can learn from and make decisions based on data. It is built on the idea that systems can automatically learn patterns and insights from data without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does Machine Learning Work?\n",
    "Machine learning relies on data and algorithms:\n",
    "\n",
    "- __Data__: Data is the backbone of ML. It can be numerical data, text, images, audio, etc. The more data a model has, the better it can learn the underlying patterns.\n",
    "    - __Continous Data__: Continuous data can take any value within a given range. It can include fractions and decimals, allowing for infinitely many possible values.\n",
    "        - not countable, measureable\n",
    "    - __Discreate Data__: Discrete data can only take specific values, typically whole numbers\n",
    "        - countable, not measuarable\n",
    "\n",
    "    For linear regression both input and output variable are continous, for logistic regressoin input variable can be both discreate and continous but output variable always be discrete.\n",
    "- __Algorithms__: These are the mathematical or statistical methods that process the data, find patterns, and make predictions. Different types of algorithms are used depending on the problem and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Machine Learning\n",
    "Machine learning is generally classified into three main categories:\n",
    "1. __Supervised Learning__: Algorithm is trained on labeled data, meaning the training `data has input-output pairs`. The model learns the relationship between inputs (features) and outputs (labels).\n",
    "\n",
    "2. __Unsupervised Learning__: Algorithm is given data without labeled responses. It tries to `learn the underlying structure from the data`, often by grouping similar data points or discovering patterns.\n",
    "\n",
    "3. __Reinforcement Learning__: An agent learns to `interact with an environment` by performing actions and `receiving rewards or penalties`. The goal is to learn a policy that maximizes cumulative rewards over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts of Machine Learning\n",
    "- __Features__: Input variables used to make predictions. \n",
    "- __Labels__: Output variables that the model tries to predict.\n",
    "- __Training Data__: A dataset used to train the model.\n",
    "- __Testing Data__: A separate dataset used to evaluate the model's performance after training. It checks how well the model generalizes to new, unseen data.\n",
    "- __Validation Data__: A seperate dataset used to tune the hyperparameters to prevent overfitting.\n",
    "- __Model Evaluation Metrics:__\n",
    "    Common metrics to evaluate model performance include:\n",
    "    - __*Accuracy*__: The percentage of correct predictions (used for classification).\n",
    "    - __*Precision and Recall*__: Precision measures the proportion of positive identifications that are actually correct, while recall measures the proportion of actual positives that are correctly identified.\n",
    "    - __*Mean Squared Error (MSE)*__: A metric for regression tasks that measures the average squared difference between the predicted and actual values.\n",
    "- __Overfitting__: When a model learns the training data too well, including its noise and outliers, leading to poor performance on new data. Itâ€™s like memorizing rather than learning.\n",
    "- __Underfitting__: When a model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithm\n",
    "Machine learning algorithms are the mathematical models that process data, learn from patterns, and make predictions or decisions. They are categorized based on the learning task (supervised, unsupervised, or reinforcement learning) and the type of problem they solve (regression, classification, clustering, etc.).\n",
    "\n",
    "## 1. Supervised Learning Algorithm\n",
    "### Regression Algorithms\n",
    "- __Linear Regression__ Predict house prices, stock prices.\n",
    "- Multiple Linear Regression\n",
    "- __Polynomial Regression__ The relationship between input and output is not linear.\n",
    "### Classificatoin Algorithms\n",
    "- __Logistic Regression__ Binary Classification like spam detection\n",
    "- __Naive Bayes__ Multi-class Classification like text classification (spam filtering, sentiment analysis\n",
    "### Both Regression & Classification\n",
    "- __Support Vector(SVM/SVR)__ Text classification, image recognition.\n",
    "- __K-Nearest Neighbors (KNN)__ Image recognition, recommendation systems.\n",
    "- __Decision Tree__ Customer segmentation, credit risk analysis.\n",
    "- __Random Forest__ Fraud detection, stock market prediction.\n",
    "## 2. Unsupervised Learning Algorithm\n",
    "### 2.1 Clustering Algorithms\n",
    "- __K-Means__ - Customer segmentation, document clustering.\n",
    "- __Hierarchical Clustering__ Gene expression analysis, market segmentation.\n",
    "### 2.2 Dimensionality Reduction Algorithms\n",
    "- __Principal Component Analysis (PCA)__ - Feature reduction for large datasets, image compression.\n",
    "- Linear Discriminant Analysis (LCA)\n",
    "## 3. Reinforcement Learning Algorithm\n",
    "- Upper Confidence Bound Algorithm (UCBA)\n",
    "- Thompson Sampling\n",
    "## 4. Deep Learning Algorithm\n",
    "- __Artificial Neural Networks (ANNs)__ Predicting stock prices, detecting fraud.\n",
    "- __Convolutional Neural Networks (CNNs)__ Image recognition, object detection.\n",
    "- __Recurrent Neural Networks (RNNs)__ Time series forecasting, language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow\n",
    "The general process of creating an ML model includes:\n",
    "\n",
    "1. __Data Collection__: Gathering relevant data.\n",
    "2. __Data Preprocessing__: Cleaning, transforming, and organizing data (handling missing values, scaling features).\n",
    "3. __Feature Engineering__: Selecting or creating important features that have the most impact on predictions.\n",
    "4. __Model Selection__: Choosing the right algorithm based on the problem type (regression, classification, clustering, etc.).\n",
    "5. __Training the Model__: Feeding training data to the model to learn patterns.\n",
    "6. __Model Evaluation__: Testing the model on unseen data and measuring its performance using evaluation metrics.\n",
    "7. __Hyperparameter Tuning__: Adjusting the parameters that control the learning process to improve model performance.\n",
    "8. __Deployment__: Integrating the model into a real-world system or application for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Deep learning is a subset of machine learning that uses neural networks with many layers (deep neural networks). It's especially powerful for handling large datasets and complex data like images, audio, and text.\n",
    "\n",
    "- __Convolutional Neural Networks (CNNs)__: Best for image data.\n",
    "- __Recurrent Neural Networks (RNNs)__: Useful for sequential data like time series or natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Machine Learning\n",
    "- __Computer Vision__: Image recognition, facial recognition, self-driving cars.\n",
    "- __Natural Language Processing (NLP)__: Chatbots, sentiment analysis, language translation.\n",
    "- __Recommendation Systems__: Suggesting products or content based on user behavior (e.g., Netflix, Amazon).\n",
    "- __Healthcare__: Predicting disease progression, drug discovery, personalized medicine.\n",
    "- __Finance__: Fraud detection, stock market predictions, algorithmic trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization technique according to algorithm\n",
    "Here's a list of common machine learning algorithms along with the corresponding plots or graphs that can be used to represent their outputs or performance. Each algorithm typically has a specific type of visualization that is most informative.\n",
    "\n",
    "| **Algorithm**                  | **Corresponding Plot/Graph**                        | **Description**                                                  |\n",
    "|--------------------------------|-----------------------------------------------------|------------------------------------------------------------------|\n",
    "| **Linear Regression**          | Scatter Plot with Regression Line                   | Shows the relationship between independent and dependent variables. |\n",
    "| **Logistic Regression**        | ROC Curve                                          | Visualizes the trade-off between true positive rate and false positive rate. |\n",
    "| **Decision Tree**              | Decision Tree Diagram                               | Illustrates the splits made at each node based on feature values. |\n",
    "| **Random Forest**              | Feature Importance Bar Plot                         | Shows the importance scores of features in the model.             |\n",
    "| **Support Vector Machine (SVM)** | Decision Boundary Plot                              | Visualizes the hyperplane that separates classes in the feature space. |\n",
    "| **K-Nearest Neighbors (KNN)**  | Scatter Plot with KNN Decision Boundary            | Displays the decision boundaries and data points classified by KNN. |\n",
    "| **K-Means Clustering**         | Scatter Plot with Cluster Centroids                | Visualizes the clusters formed, often with centroids marked.     |\n",
    "| **Principal Component Analysis (PCA)** | 2D/3D Scatter Plot of Principal Components | Shows the reduced-dimensional representation of data.             |\n",
    "| **Naive Bayes**                | Confusion Matrix                                   | Visualizes the performance of the classifier on test data.        |\n",
    "| **Neural Networks**            | Loss Curve                                          | Plots training and validation loss over epochs during training.   |\n",
    "| **Gradient Boosting**          | Learning Curve                                      | Shows the model's performance as the number of boosting iterations increases. |\n",
    "| **XGBoost**                    | Feature Importance Plot                             | Displays the importance of features in the model's predictions.   |\n",
    "| **Time Series Analysis**       | Time Series Plot                                   | Displays the data points over time to visualize trends or seasonality. |\n",
    "| **Association Rule Learning**  | Network Graph                                      | Visualizes relationships between items or features in the dataset. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics according to algorithm\n",
    "Evaluation metrics in machine learning are quantitative measures used to assess the performance of a model on a given task, helping determine how well it makes predictions based on the data it has been trained on.\n",
    "\n",
    "Here is a list of common machine learning algorithms along with their corresponding evaluation metrics, depending on whether they are used for regression, classification, or both:\n",
    "\n",
    "| **Algorithm**                    | **Evaluation Metrics**                                        |\n",
    "|----------------------------------|---------------------------------------------------------------|\n",
    "| **Linear Regression**            | MAE, MSE, RMSE, RÂ²                                            |\n",
    "| **Logistic Regression**          | Accuracy, Precision, Recall, F1-Score, ROC-AUC                |\n",
    "| **KNN**                          | MAE, MSE, RMSE, RÂ² (Regression), Accuracy, Precision, Recall (Classification) |\n",
    "| **Decision Trees**               | MAE, MSE, RMSE, RÂ² (Regression), Accuracy, Confusion Matrix (Classification) |\n",
    "| **Random Forest**                | MAE, MSE, RMSE, RÂ², Feature Importance, Accuracy, Precision, Recall |\n",
    "| **SVM**                          | MAE, MSE, RMSE, RÂ² (Regression), Accuracy, Precision, Recall (Classification) |\n",
    "| **Gradient Boosting (XGBoost)**  | MAE, MSE, RMSE, RÂ², Feature Importance, Accuracy, ROC-AUC     |\n",
    "| **Naive Bayes**                  | Accuracy, Precision, Recall, F1-Score, ROC-AUC                |\n",
    "| **Neural Networks**              | MAE, MSE, RMSE, RÂ² (Regression), Accuracy, Log Loss (Classification) |\n",
    "| **K-Means**                      | Silhouette Score, Inertia, Davies-Bouldin Index               |\n",
    "| **PCA**                          | Explained Variance Ratio, Scree Plot                          |\n",
    "| **Hierarchical Clustering**      | Dendrogram, Silhouette Score, Davies-Bouldin Index            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification algorithm. It helps in understanding the true performance by showing where the model is getting confused when making predictions.\n",
    "\n",
    "### Structure of the Confusion Matrix\n",
    "\n",
    "For a binary classification problem, the confusion matrix is a 2x2 table with four outcomes:\n",
    "\n",
    "|                        | **Predicted Positive** | **Predicted Negative** |\n",
    "|------------------------|------------------------|------------------------|\n",
    "| **Actual Positive**    | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative**    | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "#### Definitions\n",
    "- **True Positive (TP)**: The model predicted \"Positive\" and it was actually \"Positive.\"\n",
    "- **False Negative (FN)**: The model predicted \"Negative\" but it was actually \"Positive.\"\n",
    "- **False Positive (FP)**: The model predicted \"Positive\" but it was actually \"Negative.\"\n",
    "- **True Negative (TN)**: The model predicted \"Negative\" and it was actually \"Negative.\"\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose we have a binary classification problem where we're trying to predict whether an email is **spam** or **not spam**. \n",
    "- Positive (P): Spam emails\n",
    "- Negative (N): Not spam emails\n",
    "\n",
    "Let's say after running a classification algorithm on a test set of 100 emails, we get the following confusion matrix:\n",
    "\n",
    "|                        | **Predicted Spam**     | **Predicted Not Spam** |\n",
    "|------------------------|------------------------|------------------------|\n",
    "| **Actual Spam**        | 40 (TP)                | 10 (FN)                |\n",
    "| **Actual Not Spam**    | 5 (FP)                 | 45 (TN)                |\n",
    "\n",
    "In this case:\n",
    "- **True Positive (TP)** = 40: The model correctly predicted 40 spam emails as spam.\n",
    "- **False Negative (FN)** = 10: The model predicted 10 emails as not spam, but they were actually spam.\n",
    "- **False Positive (FP)** = 5: The model predicted 5 emails as spam, but they were actually not spam.\n",
    "- **True Negative (TN)** = 45: The model correctly predicted 45 emails as not spam.\n",
    "\n",
    "### Key Metrics from the Confusion Matrix\n",
    "\n",
    "1. **Accuracy**: The proportion of correct predictions (both true positives and true negatives) out of the total predictions.\n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{40 + 45}{40 + 45 + 5 + 10} = \\frac{85}{100} = 0.85 \\text{ (or 85\\%)}\n",
    "   $$\n",
    "   So, the model is correct 85% of the time.\n",
    "\n",
    "2. **Precision**: The proportion of predicted positives (spam) that are actually positive (spam).\n",
    "   $$\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP} = \\frac{40}{40 + 5} = \\frac{40}{45} = 0.89 \\text{ (or 89\\%)}\n",
    "   $$\n",
    "   So, when the model predicts spam, itâ€™s correct 89% of the time.\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate)**: The proportion of actual positives (spam) that are correctly identified.\n",
    "   $$\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN} = \\frac{40}{40 + 10} = \\frac{40}{50} = 0.80 \\text{ (or 80\\%)}\n",
    "   $$\n",
    "   So, the model correctly identifies 80% of the actual spam emails.\n",
    "\n",
    "4. **F1 Score**: A balance between Precision and Recall, especially useful when you have imbalanced datasets.\n",
    "   $$\n",
    "   F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\times \\frac{0.89 \\times 0.80}{0.89 + 0.80} = 2 \\times \\frac{0.712}{1.69} = 0.84\n",
    "   $$\n",
    "   The F1 score here is 0.84 (or 84\\%).\n",
    "\n",
    "5. **False Positive Rate (FPR)**: The proportion of actual negatives (not spam) that are incorrectly classified as positive (spam).\n",
    "   $$\n",
    "   \\text{FPR} = \\frac{FP}{FP + TN} = \\frac{5}{5 + 45} = \\frac{5}{50} = 0.10 \\text{ (or 10\\%)}\n",
    "   $$\n",
    "   So, 10% of non-spam emails were incorrectly classified as spam.\n",
    "\n",
    "A confusion matrix is extremely useful for understanding the details of how a classification model is performing, especially in cases where accuracy alone might be misleading. By analyzing Precision, Recall, F1 Score, and other metrics, we can understand how well the model balances its predictions and where it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Actual labels (1: Spam, 0: Not Spam)\n",
    "y_true = np.array([1]*50 + [0]*50)  # 50 spam emails, 50 not spam\n",
    "# Predicted labels (let's assume the model made some predictions)\n",
    "y_pred = np.array([1]*40 + [0]*10 + [1]*5 + [0]*45)  # 40 TP, 10 FN, 5 FP, 45 TN\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues' , yticklabels=['Spam', 'Not Spam'], xticklabels=['Spam', 'Not Spam']  )\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm implmentation steps\n",
    "\n",
    "1. Import the Required Libraries\n",
    "2. Load the Dataset\n",
    "3. Extract Features\n",
    "4. Preprocess Data\n",
    "5. Split the Data into Training and Test Sets\n",
    "6. Create the Regression Model\n",
    "7. Train the Model\n",
    "8. Make Predictions\n",
    "9. Evaluate the Model\n",
    "10. Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
