{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31ef227-d37d-4195-a197-3ae1fe675c2c",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- What is Deep Learning?\n",
    "- Neural Network\n",
    "    - Neuron\n",
    "    - Perceptron\n",
    "    - Neural Network\n",
    "    - Imlementation Steps\n",
    "    - Activation Function\n",
    "    - Loss Function\n",
    "    - Gradient Descent\n",
    "- Train Deep Neural Network\n",
    "    - Data Augmentation\n",
    "    - Batch Normalization\n",
    "    - Dropout Regularization\n",
    "    - Optimizers\n",
    "- Convolutional Neural Networks\n",
    "- Recurrent Neural Networks\n",
    "- Natural Language Processing\n",
    "- Transfer learning\n",
    "- Reinforcement Learning\n",
    "- Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658a68a-3a40-4dd8-bab3-fae2a57d9333",
   "metadata": {},
   "source": [
    "# What is Deep Learning?\n",
    "Deep Learning is a subset of machine learning, which itself is a branch of artificial intelligence (AI). It focuses on teaching computers to learn and make decisions by mimicking the way the human brain works. It uses artificial neural networks (ANNs) to process data and make predictions or classifications. These networks consist of layers of interconnected nodes (neurons), where each layer learns to extract and process different features from the input data.\n",
    "\n",
    "Deep learning is particularly powerful for tasks where feature extraction is challenging or where data is unstructured, such as images, audio, and text. It has enabled significant advancements in fields like natural language processing (NLP), computer vision, and speech recognition.\n",
    "\n",
    "## Differences between Machine Learning and Deep Learning\n",
    "| **Aspect**               | **Machine Learning (ML)**                              | **Deep Learning (DL)**                             |\n",
    "|--------------------------|-------------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**           | A subset of AI focused on enabling machines to learn from data using algorithms. | A subset of ML that uses neural networks with multiple layers to model complex patterns. |\n",
    "| **Data Dependency**      | Performs well with smaller datasets.                  | Requires large amounts of labeled data to perform effectively. |\n",
    "| **Feature Engineering**  | Relies on manual feature extraction and selection.     | Automatically extracts features through multiple layers of the network. |\n",
    "| **Model Complexity**     | Models are simpler (e.g., linear regression, SVM).     | Models are complex with deep neural network architectures. |\n",
    "| **Computation Power**    | Requires less computational power.                     | Requires significant computational resources (GPUs/TPUs). |\n",
    "| **Training Time**        | Faster training times for most models.                 | Training can be time-consuming due to large datasets and model complexity. |\n",
    "| **Interpretability**     | Easier to interpret and understand model decisions.    | Often considered a \"black box\" with lower interpretability. |\n",
    "| **Applications**         | Suitable for tabular data (e.g., fraud detection, customer segmentation). | Excels in unstructured data like images, text, audio (e.g., image recognition, NLP). |\n",
    "| **Scalability**          | Limited scalability with increasing data complexity.   | Highly scalable for large and complex datasets. |\n",
    "| **Example Algorithms**   | Linear Regression, Decision Trees, Random Forests, SVM. | Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers. |\n",
    "\n",
    "## Applications\n",
    "| **Field**              | **Application**                    | **Deep Learning Model**         |\n",
    "|------------------------|------------------------------------|---------------------------------|\n",
    "| **Computer Vision**     | Object detection, facial recognition | Convolutional Neural Networks (CNNs) |\n",
    "| **Natural Language Processing (NLP)** | Language translation, sentiment analysis | Recurrent Neural Networks (RNNs), Transformers |\n",
    "| **Speech Recognition** | Voice assistants, transcription     | RNNs, Transformers             |\n",
    "| **Generative Models**  | Image and text generation           | Generative Adversarial Networks (GANs), Transformers |\n",
    "| **Autonomous Vehicles** | Perception and path planning        | CNNs, YOLO, RNNs               |\n",
    "| **Healthcare**         | Disease diagnosis, drug discovery   | CNNs, AlphaFold                |\n",
    "| **Recommendation Systems**     | E-commerce, streaming services      | Neural Collaborative Filtering |\n",
    "| **Finance**            | Fraud detection, stock prediction   | Autoencoders, RNNs             |\n",
    "\n",
    "## Frameworks\n",
    "### 1. TensorFlow\n",
    "TensorFlow is an open-source deep learning framework developed by Google Brain. It is widely used in research and production for creating complex machine learning models.\n",
    "### 2. PyTorch\n",
    "PyTorch, developed by Facebook‚Äôs AI Research (FAIR), is known for its dynamic computation graph and flexibility. It is popular among researchers for its intuitive design.\n",
    "### 3. Keras\n",
    "Keras is a high-level API for building and training neural networks. Initially developed independently, it is now integrated into TensorFlow as `tf.keras`. Keras focuses on user-friendliness and rapid prototyping.\n",
    "### Comparison\n",
    "| **Feature**               | **TensorFlow**                       | **PyTorch**                          | **Keras**                                |\n",
    "|---------------------------|--------------------------------------|--------------------------------------|-----------------------------------------|\n",
    "| **Ease of Use**           | Moderate                            | High                                 | Very High                               |\n",
    "| **Computation Graph**     | Static (can be dynamic via `tf.function`) | Dynamic                              | High-level, static (via TensorFlow)     |\n",
    "| **Flexibility**           | High                                | Very High                            | Moderate                                |\n",
    "| **Debugging**             | Challenging                         | Easy (real-time debugging)           | Easy (abstracted)                       |\n",
    "| **Deployment**            | Excellent (TensorFlow Serving, Lite) | Moderate                            | Integrated with TensorFlow (via `tf.keras`) |\n",
    "| **Best For**              | Production, scalability             | Research, experimentation            | Beginners, rapid prototyping            |\n",
    "\n",
    "### Choosing the Right Framework\n",
    "- **TensorFlow**: If you need a production-ready system or plan to deploy on mobile/IoT devices.\n",
    "- **PyTorch**: If you prioritize flexibility and debugging, or are working on research projects.\n",
    "- **Keras**: If you are a beginner or need to quickly prototype a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90152b54-0fa9-4e58-bb99-01c502fbb83f",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Neuron\n",
    "A neuron in deep learning is the basic building block of artificial neural networks, inspired by the biological neurons in the human brain. It is a computational unit that takes inputs, processes them, and produces an output.\n",
    "\n",
    "### Components of a Neuron\n",
    "#### 1. Inputs (\\$ ùë•_1, x_2, ... , x_n \\$):\n",
    "- These represent features or data points.\n",
    "- For example, in image classification, they could be pixel values.\n",
    "#### 2. Weights (\\$ w_1, w_2, ... , w_n \\$):\n",
    "- Each input has an associated weight that signifies its importance.\n",
    "- Weights are learned during training.\n",
    "#### 3. Bias (\\$ b \\$):\n",
    "- Bias helps the model shift the activation function, improving learning capabilities.\n",
    "- It acts as an offset.\n",
    "#### 4. Summation Function:\n",
    "The neuron computes a weighted sum of inputs:\n",
    "\n",
    "\\$\n",
    " z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
    "\\$\n",
    "\n",
    "#### 5. Activation Function \n",
    "- This introduces non-linearity, allowing the network to learn complex patterns.\n",
    "- Common activation functions include:\n",
    "    - Sigmoid: Outputs values between 0 and 1.\n",
    "    - ReLU (Rectified Linear Unit): Outputs max(0, z)\n",
    "    - Tanh: Outputs values between -1 and 1.\n",
    "- Without activation functions, the neuron would act as a simple linear equation. This limits the network to learning only linear relationships. Activation functions introduce non-linearity, enabling the network to capture more complex patterns.\n",
    "#### 6. Output:\n",
    "The final output is:\n",
    "\n",
    "\\$\n",
    "y = f(z)\n",
    "\\$\n",
    "### Neurons in a Layer\n",
    "A single neuron is rarely used alone.\n",
    "Multiple neurons are stacked to form layers in a neural network.\n",
    "Outputs from one layer become inputs to the next.\n",
    "\n",
    "## Perceptron\n",
    "A perceptron is the fundamental building block of neural networks. It models a single neuron and is inspired by the biological neurons in the human brain. It works as a binary classifier.\n",
    "\n",
    "Components of a Perceptron\n",
    "1. Inputs ($\\ x_1, x_2, ..., x_n $\\ ): Features of the data.\n",
    "2. Weights ($\\ w_1, w_2, ..., w_n $\\ ): Each input is assigned a weight that signifies its importance.\n",
    "3. Summation Function ($\\ z = \\sum_i{(w_i x_i )+ b} $\\): The weighted sum of inputs plus a bias term b.\n",
    "4. Activation Function (f(z)): Applies a threshold to determine the output (0 or 1).\n",
    "\n",
    "### Differences between perceptron and neuron\n",
    "| **Aspect**         | **Perceptron**                                   | **Neuron in Neural Network**                       |\n",
    "|---------------------|------------------------------------------------|---------------------------------------------------|\n",
    "| **Complexity**      | Simple model for binary classification.         | Generalized for more complex tasks.              |\n",
    "| **Activation**      | Step function (binary output).                  | Non-linear functions (e.g., ReLU, sigmoid).       |\n",
    "| **Capability**      | Solves only linearly separable problems.         | Solves both linear and non-linear problems.       |\n",
    "| **Learning**        | Simple weight adjustment (e.g., perceptron rule).| Uses backpropagation and gradient descent.        |\n",
    "| **Usage**           | Standalone classifier (single-layer).           | Forms building blocks of deep neural networks.    |\n",
    "| **Output**          | Binary (0 or 1).                                | Continuous or probabilistic values.              |\n",
    "\n",
    "## Neural Network\n",
    "A neural network is a computational model designed to simulate the way the human brain processes information. It consists of layers of interconnected nodes (neurons), organized into an input layer, one or more hidden layers, and an output layer. \n",
    "### Components of a Neural Network\n",
    "#### 1. Input Layer:\n",
    "- Accepts raw data as input. Each neuron in this layer corresponds to a feature in the input data.\n",
    "- Example: If you are predicting house prices, features like \"size\", \"location\", and \"number of rooms\" will be inputs.\n",
    "#### 2. Hidden Layers:\n",
    "- Process data using weights, biases, and activation functions to learn intermediate representations.\n",
    "- The number of layers and neurons depends on the complexity of the task.\n",
    "#### 3 Output Layer:\n",
    "- Provides the final result of the model.\n",
    "- Example: In classification, this layer outputs probabilities or class labels.\n",
    "#### 4. Weights and Biases:\n",
    "- Weights determine the importance of a connection between neurons.\n",
    "- Bias adjusts the weighted sum output for flexibility in learning.\n",
    "#### 5. Activation Function:\n",
    "Introduces non-linearity, enabling the network to solve complex problems.\n",
    "Common activation functions: Sigmoid, ReLU, Tanh, Softmax.\n",
    "### How a Neural Network Works: Step-by-Step\n",
    "#### 1.Forward Propagation:\n",
    "- Data flows through the network from the input layer to the output layer.\n",
    "- Each neuron computes the weighted sum of inputs, applies an activation function, and sends the output to the next layer.\n",
    "#### 2. Loss Calculation:\n",
    "- A loss function measures the error between the predicted and actual values.\n",
    "#### 3. Backpropagation:\n",
    "- The network adjusts weights and biases to minimize the error.\n",
    "- This involves calculating gradients using the chain rule and updating parameters with an optimization algorithm like gradient descent.\n",
    "#### 4. Training:\n",
    "- The process of forward propagation, loss calculation, and backpropagation repeats over multiple iterations (epochs) until the network learns the patterns in the data.\n",
    "\n",
    "## Imlementation Steps\n",
    "1. Import the Required Libraries\n",
    "2. Load the Dataset\n",
    "3. Extract Features\n",
    "4. Preprocess Data\n",
    "5. Split the Data into Training and Test Sets\n",
    "6. Define the Deep Learning Model\n",
    "7. Compile the Model\n",
    "8. Train the Model\n",
    "9. Evaluate the Model\n",
    "10. Make Predictions\n",
    "11. Visualization\n",
    "12. Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87d116-029d-4d7b-92f1-551532f5214d",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "An activation function in a neural network defines the output of a node (neuron) given an input or a set of inputs. It determines whether a neuron should be activated or not based on the input it receives. Activation functions introduce non-linearity into the model, enabling it to learn and perform complex tasks.\n",
    "\n",
    "Without activation functions, neural networks would behave like a linear regression model, unable to capture the non-linear patterns in data.\n",
    "\n",
    "### Types\n",
    "#### 1. Linear Activation Function:\n",
    "- Formula: ùëì(ùë•) = ùë•\n",
    "- Problem: Does not introduce non-linearity; all layers collapse into a single layer.\n",
    "#### 2. Non-Linear Activation Functions: These are the most commonly used.\n",
    "- **Sigmoid**:\n",
    "    -Formula: \\$ \\frac{1}{1 + e^{-x}} \\$\n",
    "    - Range: 0 to 1\n",
    "    - Usage: Good for binary classification; used in the output layer.\n",
    "    - Drawbacks: Can cause vanishing gradients.\n",
    "- **Tanh (Hyperbolic Tangent)**:\n",
    "    - Formula: ùëì(ùë•) = tanh(ùë•) = \\$ \\frac{e^x + e^{-x}}{e^x - e^{-x}} \\$\n",
    "    - Range: ‚àí1 to 1\n",
    "    - Usage: Good for hidden layers; addresses vanishing gradients better than sigmoid.\n",
    "    - Drawbacks: Still susceptible to vanishing gradients for very large or small inputs.\n",
    "- **ReLU (Rectified Linear Unit)**:\n",
    "    - Formula: f(x)=max(0,x)\n",
    "    - Range: 0 to ‚àû\n",
    "    - Usage: Widely used in hidden layers; fast to compute.\n",
    "    - Drawbacks: Can suffer from \"dead neurons\" where gradients become zero for inputs less than 0.\n",
    "- **Leaky ReLU**:\n",
    "    - Formula: f(x)=x if x>0, otherwise f(x)=Œ±x(Œ±>0)\n",
    "    - Usage: Solves the \"dead neurons\" issue by allowing a small gradient for negative inputs.\n",
    "- **Softmax**:\n",
    "    - Formula: \\$ \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\$\n",
    "    - Range: 0 to 1\n",
    "    - Usage: Used in the output layer for multi-class classification; ensures probabilities sum to 1.\n",
    "\n",
    "### Example\n",
    "Suppose we have a neural network layer receiving the input vector x = [1, 2, 3] and the weights w = [0.5, 0.1, -0.4] with a bias b = 0.2.\n",
    "\n",
    "#### Step 1: Compute the pre-activation output (z):\n",
    "\\$\n",
    "z = w \\cdot x + b\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "z = (1 \\times 0.5) + (-2 \\times 0.1) + (3 \\times -0.4) + 0.2\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "z = 0.5 - 0.2 - 1.2 + 0.2 = -0.7\n",
    "\\$\n",
    "\n",
    "#### Step 2: Apply an activation function.\n",
    "- **Sigmoid**:\n",
    "  \\$\n",
    "  f(z) = \\frac{1}{1 + e^{0.7}} \\approx 0.332\n",
    "    \\$\n",
    "- **ReLU**:\n",
    "    \\$\n",
    "  f(z) = \\max(0, -0.7) = 0\n",
    "    \\$\n",
    "- **Tanh**:\n",
    "    \\$\n",
    "  f(z) = \\tanh(-0.7) \\approx -0.604\n",
    "    \\$\n",
    "\n",
    "#### Explanation of Outputs:\n",
    "- **Sigmoid** squashes the value to a range between 0 and 1, indicating activation strength.\n",
    "- **ReLU** outputs 0, meaning the neuron is inactive for negative input.\n",
    "- **Tanh** outputs a negative value, showing activation while preserving sign.\n",
    "\n",
    "### Choosing an Activation Function\n",
    "\n",
    "- **Hidden Layers**:\n",
    "  - **ReLU**: Default choice due to simplicity and efficiency.\n",
    "  - **Leaky ReLU**: Use if the dead neuron issue arises.\n",
    "- **Output Layer**:\n",
    "  - **Sigmoid**: For binary classification.\n",
    "  - **Softmax**: For multi-class classification.\n",
    "  - **Linear**: For regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8022d-f15f-4896-b591-442d0752c38e",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "A loss function in deep learning quantifies how well the predicted outputs of a neural network match the actual target values. It is a critical component that guides the optimization process during training by measuring the error or deviation between predictions and ground truths. The goal of training a neural network is to minimize this loss.\n",
    "### Types\n",
    "Loss functions can be categorized based on the type of task:\n",
    "#### 1. Regression Loss Functions\n",
    "- MSE\n",
    "- MAE\n",
    "#### 2. Classification Loss Functions\n",
    "**Binary Cross-Entropy:**\n",
    "\n",
    "\\$\n",
    "\\text{BCE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "\\$\n",
    "   \n",
    "**Categorical Cross-Entropy:**\n",
    "\n",
    "\\$\n",
    "\\text{CCE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "\\$\n",
    "   \n",
    "**Hinge Loss:**\n",
    "\n",
    "\\$\n",
    "\\text{Hinge Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i)\n",
    "\\$\n",
    "\n",
    "### Choosing a Loss Function\n",
    "**1. Regression**:\n",
    "- **MSE** for smoother models that penalize large errors.\n",
    "- **MAE** for robust models against outliers.\n",
    "\n",
    "**2. Classification**:\n",
    "- **Binary Cross-Entropy** for binary problems.\n",
    "- **Categorical Cross-Entropy** for multi-class problems.\n",
    "- **Hinge Loss** for margin-based classifiers like SVMs.\n",
    "\n",
    "**3. Custom Tasks**:\n",
    "- Combine loss functions or define your own based on specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0fe00-e4f9-4c1e-98cf-7aa7cb40696e",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning and deep learning models by iteratively updating model parameters (weights and biases) in the direction of the negative gradient of the loss function. This process enables the model to learn the best parameters for making accurate predictions.\n",
    "\n",
    "### Key Concepts\n",
    "**1. Loss Function**: A function that quantifies the error between predicted and actual values. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\n",
    "**2. Gradient**: A vector of partial derivatives of the loss function with respect to model parameters. It indicates the direction and rate of change of the loss function.\n",
    "**3. Learning Rate (Œ∑)**:\n",
    "- A small positive value that determines the step size for updating parameters.\n",
    "- A high learning rate can cause overshooting, while a low learning rate slows convergence.\n",
    "- \n",
    "### Step-by-Step Process\n",
    "**1. Initialize Parameters**: Start with random values for weights (w) and biases (b).\n",
    "\n",
    "**2. Compute Predictions**: Use the model to predict outputs (\\$ \\hat{y} \\$).\n",
    "\n",
    "**3. Calculate Loss:** Compute the loss using a predefined loss function, such as MSE.\n",
    "\n",
    "**4. Compute Gradients**: Calculate the derivatives of the loss with respect to each paramete: \n",
    "\n",
    "\\$\n",
    "\\frac{‚àÇw}{‚àÇL}, \\frac{‚àÇL}{‚àÇb}\n",
    "\\$\n",
    "\n",
    "**5. Update Parameters:** Adjust the parameters in the direction of the negative gradient:\n",
    "\n",
    "\\$\n",
    "w=w‚àíŒ∑.\\frac{‚àÇw}{‚àÇL}‚ãÖ \n",
    "b=b‚àíŒ∑.\\frac{‚àÇL}{‚àÇb}‚ãÖ \n",
    "\\$\n",
    "**6. Repeat**: Iterate over the dataset multiple times (epochs) until the loss converges to a minimum or a stopping criterion is met.\n",
    "\n",
    "### Variants\n",
    "#### Batch Gradient Descent:\n",
    "- Computes the gradient using the entire dataset in one iteration.\n",
    "- Pros: Stable convergence.\n",
    "- Cons: Slow for large datasets.\n",
    "#### Stochastic Gradient Descent (SGD):\n",
    "- Computes the gradient for one data point at a time.\n",
    "- Pros: Faster updates, suitable for large datasets.\n",
    "- Cons: Noisy convergence.\n",
    "#### Mini-Batch Gradient Descent:\n",
    "- Computes the gradient for small batches of data.\n",
    "- Pros: Combines the advantages of batch and stochastic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cd633-912b-4f54-8a2d-5c45eea505b1",
   "metadata": {},
   "source": [
    "# Train Deep Neural Network\n",
    "## Data Augmentation\n",
    "Data augmentation generates additional training examples by applying transformations to existing data. It helps the model generalize better by exposing it to variations.\n",
    "### Why Use Data Augmentation?\n",
    "- Reduces overfitting.\n",
    "- Increases dataset size, especially when data is limited.\n",
    "- Improves model robustness to variations.\n",
    "### Common Data Augmentation Techniques\n",
    "#### Image Data\n",
    "- **Flipping**: Horizontal or vertical flips.\n",
    "- **Rotation**: Rotate images by random angles.\n",
    "- **Scaling**: Zoom in/out on images.\n",
    "- **Cropping**: Randomly crop parts of images.\n",
    "- **Brightness Adjustment**: Vary brightness to mimic lighting conditions.\n",
    "- **Noise Addition**: Add Gaussian noise to simulate variability.\n",
    "#### Text Data\n",
    "- **Synonym Replacement**: Replace words with their synonyms.\n",
    "- **Back Translation**: Translate text to another language and back.\n",
    "- **Random Deletion**: Remove random words from sentences.\n",
    "#### Time-Series Data:\n",
    "- **Jittering**: Add random noise.\n",
    "- **Time Warping**: Stretch or compress time intervals.\n",
    "- **Random Sampling**: Drop some data points randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec557747-c4f9-4886-96e6-25acd88fbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Image Data\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Example Image Data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load image (as numpy array)\n",
    "img = np.array(Image.open('example.jpg'))\n",
    "img = img.reshape((1,) + img.shape)  # Reshape for the generator\n",
    "\n",
    "for batch in datagen.flow(img, batch_size=1):\n",
    "    augmented_image = batch[0]  # Get the augmented image\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae1c93-a422-450f-a772-2734f8cdbe0b",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "Batch Normalization (BN) is a technique to improve the training of deep neural networks by normalizing intermediate layers.\n",
    "\n",
    "### Why Batch Normalization?\n",
    "1. **Internal Covariate Shift**:\n",
    "- During training, the distribution of inputs to each layer changes due to updates in the parameters of the previous layers. This phenomenon is called internal covariate shift.\n",
    "- BN addresses this by normalizing the input to each layer, reducing dependency on the initialization of weights.\n",
    "\n",
    "2. **Improved Gradient Flow**: BN reduces the risk of vanishing or exploding gradients, enabling deeper networks to train efficiently.\n",
    "\n",
    "3. **Faster Convergence**: By stabilizing the input distribution, BN allows the network to use higher learning rates.\n",
    "\n",
    "4. **Regularization Effect**: BN introduces noise to the layer‚Äôs input during training, acting as a form of regularization and reducing the need for dropout.\n",
    "\n",
    "### Where to Apply Batch Normalization\n",
    "- BN is typically applied after the affine transformation (e.g., Wx+b) and before the activation function.\n",
    "- It is commonly used in fully connected layers, convolutional layers, and sometimes recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f304e-e5ac-4698-9cb8-67a524e9f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128),\n",
    "    BatchNormalization(),  # Apply Batch Normalization\n",
    "    Activation('relu'),    # Activation after BN\n",
    "    Dense(64),\n",
    "    BatchNormalization(),  # Apply Batch Normalization\n",
    "    Activation('relu'),\n",
    "    Dense(10, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6c281-4c76-4b90-bf51-d5dc09648cb2",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "Dropout is a regularization technique where a fraction of the neurons in a layer are randomly \"dropped out\" (set to zero) during each training iteration. This prevents the network from becoming overly dependent on specific neurons, leading to better generalization.\n",
    "###  How Dropout Works\n",
    "- During training, for each forward pass, a random subset of neurons is ignored.\n",
    "- During inference (testing), all neurons are used, but their outputs are scaled by the dropout rate to account for the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f76d3d-b769-4c89-a9f6-7bb68f98c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),  # Dropout with rate 0.2\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d03dd1-a6a0-4304-9acd-6321bd547613",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "Optimizers are algorithms or methods used to update the weights and biases of a neural network to minimize the loss function.\n",
    "### SGD:\n",
    "- Uses a fixed learning rate for updates.\n",
    "- Training may be slower and prone to oscillations.\n",
    "- simple and effective for convex problems but can struggle with complex deep learning models.\n",
    "\n",
    "### RMSprop:\n",
    "- Adapts learning rates for each parameter.\n",
    "- Handles non-stationary objectives well, leading to faster convergence.\n",
    "- adapts learning rates dynamically, making it suitable for deep neural networks.\n",
    "### Adam:\n",
    "- Combines momentum and adaptive learning rates.\n",
    "- Typically achieves the best results with minimal hyperparameter tuning.\n",
    "- versatile and widely used optimizer that combines the strengths of SGD and RMSprop, making it the default choice for most tasks.\n",
    "\n",
    "### Comparison of SGD, RMSprop, and Adam\n",
    "| Feature                | SGD                     | RMSprop            | Adam                        |\n",
    "|------------------------|-------------------------|--------------------|-----------------------------|\n",
    "| **Learning Rate**      | Fixed or Decayed        | Adaptive           | Adaptive                   |\n",
    "| **Momentum**           | Optional               | No                 | Yes                        |\n",
    "| **Handling Gradients** | Oscillations in Path    | Smoothing by Gradients | Combines Momentum + RMS   |\n",
    "| **Performance**        | Slower                 | Faster in Practice | Generally Best for DL      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e82632-b339-4578-9c11-de813f506bdb",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for tasks involving spatial data, such as images and videos.\n",
    "## Components\n",
    "### 1. Convolutional Layers\n",
    "- **Purpose**: Extract features from input images by applying filters (kernels) that slide across the input.\n",
    "- **Operation**:\n",
    "    - The filter computes a weighted sum of the pixel values it covers.\n",
    "    - This produces a \"feature map\" that highlights certain features of the image (e.g., edges, textures).\n",
    "- **Key Parameters**:\n",
    "    - Kernel size: Dimensions of the filter (e.g., 3x3, 5x5).\n",
    "    - Stride: Step size for the filter's movement.\n",
    "    - Padding: Adding borders to the input to preserve dimensions.\n",
    "### 2. Activation Function\n",
    "### 3. Pooling Layers\n",
    "- **Purpose**: Downsample the feature maps to reduce dimensionality and computation while retaining important information.\n",
    "- **Types**:\n",
    "    - **Max Pooling**: Takes the maximum value in a region.\n",
    "    - **Average Pooling**: Takes the average value in a region.\n",
    "### 4. Fully Connected Layers\n",
    "- **Purpose**: Combine features extracted by the convolutional layers to perform final predictions.\n",
    "- The output from the previous layers is flattened and passed through dense layers.\n",
    "### 5. Dropout (Regularization)\n",
    "- **Purpose**: Prevent overfitting by randomly deactivating neurons during training.\n",
    "## Workflow\n",
    "1. **Input**: An image (e.g., 32x32x3 for a color image with 3 channels: RGB).\n",
    "2. **Convolution**: Apply filters to extract feature maps.\n",
    "3. **Activation (ReLU)**: Introduce non-linearity.\n",
    "4. **Pooling**: Downsample feature maps.\n",
    "5. **Repeat Steps 2-4**: Extract higher-level features.\n",
    "6. **Flatten**: Convert the feature maps to a 1D vector.\n",
    "7. **Fully Connected Layer**: Predict class probabilities or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894fb5bd-574a-4f27-8ae1-6573bf67e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd324b-e326-49b1-b3dc-a3e12411eecd",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks specifically designed to handle sequential data, such as time-series data, natural language, and speech. The primary feature of RNNs is their ability to retain a \"memory\" of previous inputs, making them well-suited for tasks where context is important.\n",
    "\n",
    "## Components\n",
    "RNNs process sequences of data by maintaining a hidden state that is updated at each step of the sequence. The hidden state acts as the memory, capturing information about the previous steps. The network updates this hidden state by applying the same set of weights to each input in the sequence, ensuring parameter sharing across time.\n",
    "\n",
    "1. **Input Sequence**: A series of data points [$\\ x_1, x_2, ..., x_n $\\], where ùëá is the length of the sequence.\n",
    "2. **Hidden State**: A vector $\\ h_t $\\  that stores information about the sequence up to time ùë°.\n",
    "3. **Weights**:\n",
    "- ùëä_xh: Weights for the input to hidden layer.\n",
    "- ùëä_hh: Weights for the hidden state transition.\n",
    "- ùëä_hy: Weights for the hidden to output layer.\n",
    "## Advantages of RNN\n",
    "- **Temporal Memory**: Captures temporal dependencies in sequential data.\n",
    "- **Parameter Sharing**: The same weights are used across time, reducing the number of parameters.\n",
    "- **Adaptability**: Works with variable-length sequences.\n",
    "## Challenges of RNN\n",
    "- **Vanishing/Exploding Gradients**: During backpropagation, gradients can become very small (vanish) or very large (explode), making learning difficult.\n",
    "- **Short-term Memory**: Standard RNNs struggle with capturing long-term dependencies in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625526d-5bb5-46d4-8199-68d1788bc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=10, output_dim=8, input_length=4),  # Embedding layer\n",
    "    SimpleRNN(units=16, return_sequences=False),            # RNN layer\n",
    "    Dense(1, activation='sigmoid')                          # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450292f6-cfc3-42e5-b447-7a266f5d9020",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "LSTM is an RNN variant with an additional mechanism called a memory cell that helps retain information over long periods. The flow of information in an LSTM is controlled by gates: the forget gate, input gate, and output gate.\n",
    "### Structure \n",
    "#### 1. Memory Cell ($\\ C_t $\\)\n",
    "- Stores long-term information across time steps.\n",
    "- Can be updated, forgotten, or output depending on the gating mechanism.\n",
    "#### 2. Hidden State ($\\ h_t $\\)\n",
    "- Represents the short-term memory of the LSTM at time t.\n",
    "#### 3. Gates:\n",
    "- **Forget Gate** ($\\ f_t $\\): Decides which information to discard from the memory cell.\n",
    "- **Input Gate** ($\\ i_t $\\): Determines which new information to add to the memory cell.\n",
    "- **Output Gate** ($\\ f_t $\\): Controls the amount of memory to pass to the next layer or time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb29772-fe9e-4cc0-b05a-5932dbfd4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    LSTM(32, activation='tanh', return_sequences=True, input_shape=(time_steps - 1, 1)),\n",
    "    Dense(1)  # Output a single value at each time step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ada5d-3f82-47f4-a09e-81b033c3930d",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units\n",
    "GRUs are a variant of RNNs that use gating mechanisms to control the flow of information within the network. These gates decide what information to keep, update, or discard, enabling the GRU to retain important long-term dependencies and forget irrelevant details.\n",
    "### Structure \n",
    "#### 1. Update Gate ($\\ z_t $\\)\n",
    "Controls how much of the past information should be carried forward to the future.\n",
    "#### 2. Reset Gate ($\\ r_t $\\)\n",
    "Determines how much of the past information to forget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954dfc5-abd7-4a6c-9fc5-c396aff071ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    GRU(32, activation='tanh', return_sequences=True, input_shape=(time_steps - 1, 1)),\n",
    "    Dense(1)  # Output a single value at each time step\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc936e97-ad11-4679-a930-6c94afa5d0e0",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "Natural Language Processing (NLP) is a field that focuses on the interaction between human language and computers. NLP aims to enable machines to understand, interpret, and generate human language in a meaningful way. When combined with deep learning, NLP leverages neural networks to model complex language structures and learn patterns from vast amounts of textual data.\n",
    "\n",
    "## Concepts\n",
    "### 1. Text Representation\n",
    "- **Bag of Words (BoW)**: Represents text as a set of unique words with their frequency.\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weighs words based on their importance in a document.\n",
    "- **Word Embeddings**: Dense vector representations of words, capturing semantic meaning (e.g., Word2Vec, GloVe).\n",
    "- **Contextual Embeddings**: Dynamic embeddings that depend on the sentence context (e.g., BERT, GPT).\n",
    "### 2. Key NLP Tasks\n",
    "- **Text Classification**: Classify text into predefined categories (e.g., spam detection).\n",
    "- **Sentiment Analysis**: Determine the sentiment (positive, negative, neutral).\n",
    "- **Named Entity Recognition (NER)**: Identify entities like names, dates, or locations.\n",
    "- **Machine Translation**: Translate text from one language to another.\n",
    "- **Question Answering**: Find answers to questions based on input text or a dataset.\n",
    "- **Language Generation**: Generate coherent and contextually relevant text.\n",
    "### 3. Deep Learning Models for NLP\n",
    "- **Recurrent Neural Networks (RNNs)**: Handle sequential data but struggle with long-term dependencies.\n",
    "- **LSTMs/GRUs**: Extensions of RNNs that overcome vanishing gradient issues.\n",
    "- **Convolutional Neural Networks (CNNs)**: Extract patterns in text, often used for classification tasks.\n",
    "- **Transformers**: Attention-based models that excel in parallelizing computations and understanding context.\n",
    "\n",
    "## Workflow\n",
    "### 1. Data Preprocessing\n",
    "- **Tokenization**: Splitting text into words or subwords.\n",
    "- **Stop-word Removal**: Removing commonly used words (optional).\n",
    "- **Lemmatization/Stemming**: Reducing words to their base form.\n",
    "### 2. Text Representation\n",
    "Convert raw text into a numerical format suitable for neural networks, such as word embeddings or sequences of token IDs.\n",
    "### 3. Model Selection\n",
    "Choose a deep learning architecture based on the task (e.g., BERT for contextual understanding).\n",
    "### 4. Training\n",
    "Use labeled data to train the model. Optimize parameters using loss functions and backpropagation.\n",
    "### 5. Evaluation:\n",
    "Assess model performance using metrics such as accuracy, F1-score, or BLEU (for translation tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30f5b7-a7d9-41ea-a538-2a76de14c9a3",
   "metadata": {},
   "source": [
    "# Transfer Learnin\n",
    "Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second, related task. It leverages the knowledge learned from a pre-trained model on a large dataset and applies it to a new but related problem, often with a smaller dataset. \n",
    "## How Does Transfer Learning Work?\n",
    "### 1. Pre-training a Base Model\n",
    "- A model is trained on a large dataset (e.g., ImageNet for image classification or Wikipedia for language processing).\n",
    "- This process extracts general features relevant to the domain (e.g., edges and shapes for images, grammar for text).\n",
    "### 2. Fine-tuning the Model\n",
    "- The pre-trained model is adjusted (fine-tuned) for a specific task using a smaller dataset.\n",
    "- Depending on the task, layers of the model may be frozen (to retain learned features) or updated (to specialize in the new task)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801261ae-6938-4acb-a639-4524799b1d69",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The goal is to learn a policy that maximizes cumulative reward over time.\n",
    "## Concepts\n",
    "1. **Agent**: The entity that learns and takes actions in the environment (e.g., a robot, an AI playing a game).\n",
    "2. **Environment**: The external system with which the agent interacts and receives feedback.\n",
    "3. **State (S)**: A representation of the current situation in the environment.\n",
    "4. **Action (A)**: The set of all possible moves the agent can make at a given state.\n",
    "5. **Reward (R)**: Feedback from the environment indicating the success of an action. Positive reward for desirable outcomes and negative for undesirable ones.\n",
    "6. **Policy (œÄ)**: A strategy that defines how the agent chooses actions based on its current state.\n",
    "7. **Value Function (V)**: A prediction of future rewards from a given state.\n",
    "8. **Q-Value (Q)**: A prediction of future rewards from taking a specific action in a given state.\n",
    "## How Does Reinforcement Learning Work?\n",
    "### Interaction:\n",
    "- The agent observes a state from the environment.\n",
    "- It takes an action based on its policy.\n",
    "- The environment transitions to a new state and provides a reward.\n",
    "### Learning:\n",
    "- The agent uses this feedback to update its policy and improve future actions.\n",
    "- It aims to maximize the cumulative reward over time.\n",
    "## Types of Reinforcement Learning\n",
    "### Model-Free RL\n",
    "- No prior knowledge of the environment.\n",
    "- Examples: Q-Learning, SARSA.\n",
    "### Model-Based RL:\n",
    "- The agent builds a model of the environment.\n",
    "- Examples: Planning algorithms like Dyna-Q.\n",
    "## Algorithms \n",
    "- Q-Learning\n",
    "- Deep Q-Learning\n",
    "- Policy Gradient\n",
    "- Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d9674-7fdd-4dc1-98e7-9257896b8393",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "Model deployment is the process of integrating a trained deep learning model into a production environment where it can provide predictions (inference) on real-world data. Deployment ensures the model serves its intended purpose, such as predicting outcomes, classifying data, or recommending products in a live system.\n",
    "##  `.h5` (HDF5) File\n",
    "HDF5 (Hierarchical Data Format version 5) is a file format and set of tools for managing complex data. In deep learning, the `.h5` format is used to store the weights, architecture, and training configuration of a model, allowing it to be easily saved and loaded for later use.\n",
    "\n",
    "The `.h5` extension specifically refers to the file that contains the model saved in HDF5 format, which is commonly used in TensorFlow and Keras.\n",
    "\n",
    "## Key Features of `.h5` Files\n",
    "- **Storage of Model Architecture**: It saves the structure of the model, including layers, activation functions, etc.\n",
    "- **Storage of Model Weights**: It stores the learned weights (parameters) of the model.\n",
    "- **Supports Saving Model Configuration**: It saves optimizer information, training history, and other configurations that might be needed for further training or inference.\n",
    "- **Cross-Platform Compatibility**: HDF5 is a widely supported format and can be used across different platforms, making it easy to share models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614aa3c-3d92-49c9-ad29-978945d09055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('model.h5')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8a555-965a-4638-9ed5-7a7d356c000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1299b1-f37b-4131-bac8-ff834313d577",
   "metadata": {},
   "source": [
    "### When to Use .h5 Files\n",
    "- **TensorFlow/Keras Models**: If you're using TensorFlow or Keras, `.h5` is the default and recommended file format for saving and loading models.\n",
    "- **Full Model**: Use `.h5` when you need to save both the architecture and the weights, and you want to preserve training-related configurations like the optimizer and state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7adcb-9b9c-4ba7-8449-fb2cab529c93",
   "metadata": {},
   "source": [
    "## `.pkl` (Pickle) File\n",
    "Pickle is a Python-specific serialization format that can be used to store and load Python objects. In the context of deep learning, `.pkl` files can store models, but unlike `.h5`, it doesn't inherently store the model's architecture, optimizer, or other configurations. Instead, it serializes the entire Python object, which can include a trained model.\n",
    "\n",
    "Pickle is generally used in PyTorch, scikit-learn, or other custom Python models where you manually serialize the model object.\n",
    "\n",
    "### Key Features of `.pkl` Files\n",
    "- **General-Purpose Serialization**: Pickle can be used to serialize any Python object, including complex machine learning models. It doesn‚Äôt require a specific deep learning framework like Keras or PyTorch, though you need to ensure the same framework version when loading the model.\n",
    "- **More Flexible**: Since Pickle saves the entire Python object (including the class definition, methods, and attributes), it‚Äôs more flexible for saving custom models or models built using libraries that don't follow a strict architecture (e.g., scikit-learn, PyTorch, etc.).\n",
    "- **Limited to Python**: Pickle files are Python-specific and can't be easily used in other languages or systems without Python runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6c53e-8a5b-4019-abba-c86a0e26f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using pickle\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8b00b-08cb-433b-ae42-38835dc91be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from pickle file\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    loaded_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb1682-a0fa-4781-85de-9a77c52cea33",
   "metadata": {},
   "source": [
    "### When to Use .pkl Files\n",
    "- **Custom Models**: If you're using libraries that don't support the `.h5` format (e.g., PyTorch), `.pkl` is a good choice for saving models.\n",
    "- **Python-Specific Use**: Pickle is ideal when working within a Python-only environment and you need to save custom-trained models, including models from scikit-learn, PyTorch, or custom code.\n",
    "- **Flexibility**: Pickle is flexible and can save not only the model weights but also additional objects related to training or the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
