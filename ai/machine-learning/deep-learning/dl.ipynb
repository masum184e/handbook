{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31ef227-d37d-4195-a197-3ae1fe675c2c",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- What is Deep Learning?\n",
    "- Neural Network\n",
    "    - Neuron\n",
    "    - Neural Network\n",
    "    - Imlementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658a68a-3a40-4dd8-bab3-fae2a57d9333",
   "metadata": {},
   "source": [
    "# What is Deep Learning?\n",
    "Deep Learning is a subset of machine learning, which itself is a branch of artificial intelligence (AI). It focuses on teaching computers to learn and make decisions by mimicking the way the human brain works. It uses artificial neural networks (ANNs) to process data and make predictions or classifications. These networks consist of layers of interconnected nodes (neurons), where each layer learns to extract and process different features from the input data.\n",
    "\n",
    "Deep learning is particularly powerful for tasks where feature extraction is challenging or where data is unstructured, such as images, audio, and text. It has enabled significant advancements in fields like natural language processing (NLP), computer vision, and speech recognition.\n",
    "\n",
    "## Differences between Machine Learning and Deep Learning\n",
    "| **Aspect**               | **Machine Learning (ML)**                              | **Deep Learning (DL)**                             |\n",
    "|--------------------------|-------------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**           | A subset of AI focused on enabling machines to learn from data using algorithms. | A subset of ML that uses neural networks with multiple layers to model complex patterns. |\n",
    "| **Data Dependency**      | Performs well with smaller datasets.                  | Requires large amounts of labeled data to perform effectively. |\n",
    "| **Feature Engineering**  | Relies on manual feature extraction and selection.     | Automatically extracts features through multiple layers of the network. |\n",
    "| **Model Complexity**     | Models are simpler (e.g., linear regression, SVM).     | Models are complex with deep neural network architectures. |\n",
    "| **Computation Power**    | Requires less computational power.                     | Requires significant computational resources (GPUs/TPUs). |\n",
    "| **Training Time**        | Faster training times for most models.                 | Training can be time-consuming due to large datasets and model complexity. |\n",
    "| **Interpretability**     | Easier to interpret and understand model decisions.    | Often considered a \"black box\" with lower interpretability. |\n",
    "| **Applications**         | Suitable for tabular data (e.g., fraud detection, customer segmentation). | Excels in unstructured data like images, text, audio (e.g., image recognition, NLP). |\n",
    "| **Scalability**          | Limited scalability with increasing data complexity.   | Highly scalable for large and complex datasets. |\n",
    "| **Example Algorithms**   | Linear Regression, Decision Trees, Random Forests, SVM. | Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers. |\n",
    "\n",
    "## Applications\n",
    "| **Field**              | **Application**                    | **Deep Learning Model**         |\n",
    "|------------------------|------------------------------------|---------------------------------|\n",
    "| **Computer Vision**     | Object detection, facial recognition | Convolutional Neural Networks (CNNs) |\n",
    "| **Natural Language Processing (NLP)** | Language translation, sentiment analysis | Recurrent Neural Networks (RNNs), Transformers |\n",
    "| **Speech Recognition** | Voice assistants, transcription     | RNNs, Transformers             |\n",
    "| **Generative Models**  | Image and text generation           | Generative Adversarial Networks (GANs), Transformers |\n",
    "| **Autonomous Vehicles** | Perception and path planning        | CNNs, YOLO, RNNs               |\n",
    "| **Healthcare**         | Disease diagnosis, drug discovery   | CNNs, AlphaFold                |\n",
    "| **Recommendation Systems**     | E-commerce, streaming services      | Neural Collaborative Filtering |\n",
    "| **Finance**            | Fraud detection, stock prediction   | Autoencoders, RNNs             |\n",
    "\n",
    "## Frameworks\n",
    "### 1. TensorFlow\n",
    "TensorFlow is an open-source deep learning framework developed by Google Brain. It is widely used in research and production for creating complex machine learning models.\n",
    "### 2. PyTorch\n",
    "PyTorch, developed by Facebook‚Äôs AI Research (FAIR), is known for its dynamic computation graph and flexibility. It is popular among researchers for its intuitive design.\n",
    "### 3. Keras\n",
    "Keras is a high-level API for building and training neural networks. Initially developed independently, it is now integrated into TensorFlow as `tf.keras`. Keras focuses on user-friendliness and rapid prototyping.\n",
    "### Comparison\n",
    "| **Feature**               | **TensorFlow**                       | **PyTorch**                          | **Keras**                                |\n",
    "|---------------------------|--------------------------------------|--------------------------------------|-----------------------------------------|\n",
    "| **Ease of Use**           | Moderate                            | High                                 | Very High                               |\n",
    "| **Computation Graph**     | Static (can be dynamic via `tf.function`) | Dynamic                              | High-level, static (via TensorFlow)     |\n",
    "| **Flexibility**           | High                                | Very High                            | Moderate                                |\n",
    "| **Debugging**             | Challenging                         | Easy (real-time debugging)           | Easy (abstracted)                       |\n",
    "| **Deployment**            | Excellent (TensorFlow Serving, Lite) | Moderate                            | Integrated with TensorFlow (via `tf.keras`) |\n",
    "| **Best For**              | Production, scalability             | Research, experimentation            | Beginners, rapid prototyping            |\n",
    "\n",
    "### Choosing the Right Framework\n",
    "- **TensorFlow**: If you need a production-ready system or plan to deploy on mobile/IoT devices.\n",
    "- **PyTorch**: If you prioritize flexibility and debugging, or are working on research projects.\n",
    "- **Keras**: If you are a beginner or need to quickly prototype a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90152b54-0fa9-4e58-bb99-01c502fbb83f",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Neuron\n",
    "A neuron in deep learning is the basic building block of artificial neural networks, inspired by the biological neurons in the human brain. It is a computational unit that takes inputs, processes them, and produces an output.\n",
    "\n",
    "### Components of a Neuron\n",
    "#### 1. Inputs (\\$ ùë•_1, x_2, ... , x_n \\$):\n",
    "- These represent features or data points.\n",
    "- For example, in image classification, they could be pixel values.\n",
    "#### 2. Weights (\\$ w_1, w_2, ... , w_n \\$):\n",
    "- Each input has an associated weight that signifies its importance.\n",
    "- Weights are learned during training.\n",
    "#### 3. Bias (\\$ b \\$):\n",
    "- Bias helps the model shift the activation function, improving learning capabilities.\n",
    "- It acts as an offset.\n",
    "#### 4. Summation Function:\n",
    "The neuron computes a weighted sum of inputs:\n",
    "\n",
    "\\$\n",
    " z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
    "\\$\n",
    "\n",
    "#### 5. Activation Function \n",
    "- This introduces non-linearity, allowing the network to learn complex patterns.\n",
    "- Common activation functions include:\n",
    "    - Sigmoid: Outputs values between 0 and 1.\n",
    "    - ReLU (Rectified Linear Unit): Outputs max(0, z)\n",
    "    - Tanh: Outputs values between -1 and 1.\n",
    "- Without activation functions, the neuron would act as a simple linear equation. This limits the network to learning only linear relationships. Activation functions introduce non-linearity, enabling the network to capture more complex patterns.\n",
    "#### 6. Output:\n",
    "The final output is:\n",
    "\n",
    "\\$\n",
    "y = f(z)\n",
    "\\$\n",
    "### Neurons in a Layer\n",
    "A single neuron is rarely used alone.\n",
    "Multiple neurons are stacked to form layers in a neural network.\n",
    "Outputs from one layer become inputs to the next.\n",
    "\n",
    "## Neural Network\n",
    "A neural network is a computational model designed to simulate the way the human brain processes information. It consists of layers of interconnected nodes (neurons), organized into an input layer, one or more hidden layers, and an output layer. \n",
    "### Components of a Neural Network\n",
    "#### 1. Input Layer:\n",
    "- Accepts raw data as input. Each neuron in this layer corresponds to a feature in the input data.\n",
    "- Example: If you are predicting house prices, features like \"size\", \"location\", and \"number of rooms\" will be inputs.\n",
    "#### 2. Hidden Layers:\n",
    "- Process data using weights, biases, and activation functions to learn intermediate representations.\n",
    "- The number of layers and neurons depends on the complexity of the task.\n",
    "#### 3 Output Layer:\n",
    "- Provides the final result of the model.\n",
    "- Example: In classification, this layer outputs probabilities or class labels.\n",
    "#### 4. Weights and Biases:\n",
    "- Weights determine the importance of a connection between neurons.\n",
    "- Bias adjusts the weighted sum output for flexibility in learning.\n",
    "#### 5. Activation Function:\n",
    "Introduces non-linearity, enabling the network to solve complex problems.\n",
    "Common activation functions: Sigmoid, ReLU, Tanh, Softmax.\n",
    "### How a Neural Network Works: Step-by-Step\n",
    "#### 1.Forward Propagation:\n",
    "- Data flows through the network from the input layer to the output layer.\n",
    "- Each neuron computes the weighted sum of inputs, applies an activation function, and sends the output to the next layer.\n",
    "#### 2. Loss Calculation:\n",
    "- A loss function measures the error between the predicted and actual values.\n",
    "#### 3. Backpropagation:\n",
    "- The network adjusts weights and biases to minimize the error.\n",
    "- This involves calculating gradients using the chain rule and updating parameters with an optimization algorithm like gradient descent.\n",
    "#### 4. Training:\n",
    "- The process of forward propagation, loss calculation, and backpropagation repeats over multiple iterations (epochs) until the network learns the patterns in the data.\n",
    "\n",
    "## Imlementation Steps\n",
    "1. Import the Required Libraries\n",
    "2. Load the Dataset\n",
    "3. Extract Features\n",
    "4. Preprocess Data\n",
    "5. Split the Data into Training and Test Sets\n",
    "6. Define the Deep Learning Model\n",
    "7. Compile the Model\n",
    "8. Train the Model\n",
    "9. Evaluate the Model\n",
    "10. Make Predictions\n",
    "11. Visualization\n",
    "12. Save the Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
