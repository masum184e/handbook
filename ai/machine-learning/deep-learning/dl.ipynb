{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31ef227-d37d-4195-a197-3ae1fe675c2c",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- What is Deep Learning?\n",
    "- Neural Network\n",
    "    - Neuron\n",
    "    - Perceptron\n",
    "    - Neural Network\n",
    "    - Imlementation Steps\n",
    "    - Activation Function\n",
    "    - Loss Function\n",
    "    - Gradient Descent\n",
    "- Train Deep Neural Network\n",
    "    - Data Augmentation\n",
    "    - Batch Normalization\n",
    "    - Dropout Regularization\n",
    "    - Optimizers\n",
    "- CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658a68a-3a40-4dd8-bab3-fae2a57d9333",
   "metadata": {},
   "source": [
    "# What is Deep Learning?\n",
    "Deep Learning is a subset of machine learning, which itself is a branch of artificial intelligence (AI). It focuses on teaching computers to learn and make decisions by mimicking the way the human brain works. It uses artificial neural networks (ANNs) to process data and make predictions or classifications. These networks consist of layers of interconnected nodes (neurons), where each layer learns to extract and process different features from the input data.\n",
    "\n",
    "Deep learning is particularly powerful for tasks where feature extraction is challenging or where data is unstructured, such as images, audio, and text. It has enabled significant advancements in fields like natural language processing (NLP), computer vision, and speech recognition.\n",
    "\n",
    "## Differences between Machine Learning and Deep Learning\n",
    "| **Aspect**               | **Machine Learning (ML)**                              | **Deep Learning (DL)**                             |\n",
    "|--------------------------|-------------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**           | A subset of AI focused on enabling machines to learn from data using algorithms. | A subset of ML that uses neural networks with multiple layers to model complex patterns. |\n",
    "| **Data Dependency**      | Performs well with smaller datasets.                  | Requires large amounts of labeled data to perform effectively. |\n",
    "| **Feature Engineering**  | Relies on manual feature extraction and selection.     | Automatically extracts features through multiple layers of the network. |\n",
    "| **Model Complexity**     | Models are simpler (e.g., linear regression, SVM).     | Models are complex with deep neural network architectures. |\n",
    "| **Computation Power**    | Requires less computational power.                     | Requires significant computational resources (GPUs/TPUs). |\n",
    "| **Training Time**        | Faster training times for most models.                 | Training can be time-consuming due to large datasets and model complexity. |\n",
    "| **Interpretability**     | Easier to interpret and understand model decisions.    | Often considered a \"black box\" with lower interpretability. |\n",
    "| **Applications**         | Suitable for tabular data (e.g., fraud detection, customer segmentation). | Excels in unstructured data like images, text, audio (e.g., image recognition, NLP). |\n",
    "| **Scalability**          | Limited scalability with increasing data complexity.   | Highly scalable for large and complex datasets. |\n",
    "| **Example Algorithms**   | Linear Regression, Decision Trees, Random Forests, SVM. | Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers. |\n",
    "\n",
    "## Applications\n",
    "| **Field**              | **Application**                    | **Deep Learning Model**         |\n",
    "|------------------------|------------------------------------|---------------------------------|\n",
    "| **Computer Vision**     | Object detection, facial recognition | Convolutional Neural Networks (CNNs) |\n",
    "| **Natural Language Processing (NLP)** | Language translation, sentiment analysis | Recurrent Neural Networks (RNNs), Transformers |\n",
    "| **Speech Recognition** | Voice assistants, transcription     | RNNs, Transformers             |\n",
    "| **Generative Models**  | Image and text generation           | Generative Adversarial Networks (GANs), Transformers |\n",
    "| **Autonomous Vehicles** | Perception and path planning        | CNNs, YOLO, RNNs               |\n",
    "| **Healthcare**         | Disease diagnosis, drug discovery   | CNNs, AlphaFold                |\n",
    "| **Recommendation Systems**     | E-commerce, streaming services      | Neural Collaborative Filtering |\n",
    "| **Finance**            | Fraud detection, stock prediction   | Autoencoders, RNNs             |\n",
    "\n",
    "## Frameworks\n",
    "### 1. TensorFlow\n",
    "TensorFlow is an open-source deep learning framework developed by Google Brain. It is widely used in research and production for creating complex machine learning models.\n",
    "### 2. PyTorch\n",
    "PyTorch, developed by Facebook‚Äôs AI Research (FAIR), is known for its dynamic computation graph and flexibility. It is popular among researchers for its intuitive design.\n",
    "### 3. Keras\n",
    "Keras is a high-level API for building and training neural networks. Initially developed independently, it is now integrated into TensorFlow as `tf.keras`. Keras focuses on user-friendliness and rapid prototyping.\n",
    "### Comparison\n",
    "| **Feature**               | **TensorFlow**                       | **PyTorch**                          | **Keras**                                |\n",
    "|---------------------------|--------------------------------------|--------------------------------------|-----------------------------------------|\n",
    "| **Ease of Use**           | Moderate                            | High                                 | Very High                               |\n",
    "| **Computation Graph**     | Static (can be dynamic via `tf.function`) | Dynamic                              | High-level, static (via TensorFlow)     |\n",
    "| **Flexibility**           | High                                | Very High                            | Moderate                                |\n",
    "| **Debugging**             | Challenging                         | Easy (real-time debugging)           | Easy (abstracted)                       |\n",
    "| **Deployment**            | Excellent (TensorFlow Serving, Lite) | Moderate                            | Integrated with TensorFlow (via `tf.keras`) |\n",
    "| **Best For**              | Production, scalability             | Research, experimentation            | Beginners, rapid prototyping            |\n",
    "\n",
    "### Choosing the Right Framework\n",
    "- **TensorFlow**: If you need a production-ready system or plan to deploy on mobile/IoT devices.\n",
    "- **PyTorch**: If you prioritize flexibility and debugging, or are working on research projects.\n",
    "- **Keras**: If you are a beginner or need to quickly prototype a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90152b54-0fa9-4e58-bb99-01c502fbb83f",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Neuron\n",
    "A neuron in deep learning is the basic building block of artificial neural networks, inspired by the biological neurons in the human brain. It is a computational unit that takes inputs, processes them, and produces an output.\n",
    "\n",
    "### Components of a Neuron\n",
    "#### 1. Inputs (\\$ ùë•_1, x_2, ... , x_n \\$):\n",
    "- These represent features or data points.\n",
    "- For example, in image classification, they could be pixel values.\n",
    "#### 2. Weights (\\$ w_1, w_2, ... , w_n \\$):\n",
    "- Each input has an associated weight that signifies its importance.\n",
    "- Weights are learned during training.\n",
    "#### 3. Bias (\\$ b \\$):\n",
    "- Bias helps the model shift the activation function, improving learning capabilities.\n",
    "- It acts as an offset.\n",
    "#### 4. Summation Function:\n",
    "The neuron computes a weighted sum of inputs:\n",
    "\n",
    "\\$\n",
    " z = \\sum_{i=1}^{n} w_i \\cdot x_i + b\n",
    "\\$\n",
    "\n",
    "#### 5. Activation Function \n",
    "- This introduces non-linearity, allowing the network to learn complex patterns.\n",
    "- Common activation functions include:\n",
    "    - Sigmoid: Outputs values between 0 and 1.\n",
    "    - ReLU (Rectified Linear Unit): Outputs max(0, z)\n",
    "    - Tanh: Outputs values between -1 and 1.\n",
    "- Without activation functions, the neuron would act as a simple linear equation. This limits the network to learning only linear relationships. Activation functions introduce non-linearity, enabling the network to capture more complex patterns.\n",
    "#### 6. Output:\n",
    "The final output is:\n",
    "\n",
    "\\$\n",
    "y = f(z)\n",
    "\\$\n",
    "### Neurons in a Layer\n",
    "A single neuron is rarely used alone.\n",
    "Multiple neurons are stacked to form layers in a neural network.\n",
    "Outputs from one layer become inputs to the next.\n",
    "\n",
    "## Perceptron\n",
    "A perceptron is the fundamental building block of neural networks. It models a single neuron and is inspired by the biological neurons in the human brain. It works as a binary classifier.\n",
    "\n",
    "Components of a Perceptron\n",
    "1. Inputs ($\\ x_1, x_2, ..., x_n $\\ ): Features of the data.\n",
    "2. Weights ($\\ w_1, w_2, ..., w_n $\\ ): Each input is assigned a weight that signifies its importance.\n",
    "3. Summation Function ($\\ z = \\sum_i{(w_i x_i )+ b} $\\): The weighted sum of inputs plus a bias term b.\n",
    "4. Activation Function (f(z)): Applies a threshold to determine the output (0 or 1).\n",
    "\n",
    "### Differences between perceptron and neuron\n",
    "| **Aspect**         | **Perceptron**                                   | **Neuron in Neural Network**                       |\n",
    "|---------------------|------------------------------------------------|---------------------------------------------------|\n",
    "| **Complexity**      | Simple model for binary classification.         | Generalized for more complex tasks.              |\n",
    "| **Activation**      | Step function (binary output).                  | Non-linear functions (e.g., ReLU, sigmoid).       |\n",
    "| **Capability**      | Solves only linearly separable problems.         | Solves both linear and non-linear problems.       |\n",
    "| **Learning**        | Simple weight adjustment (e.g., perceptron rule).| Uses backpropagation and gradient descent.        |\n",
    "| **Usage**           | Standalone classifier (single-layer).           | Forms building blocks of deep neural networks.    |\n",
    "| **Output**          | Binary (0 or 1).                                | Continuous or probabilistic values.              |\n",
    "\n",
    "## Neural Network\n",
    "A neural network is a computational model designed to simulate the way the human brain processes information. It consists of layers of interconnected nodes (neurons), organized into an input layer, one or more hidden layers, and an output layer. \n",
    "### Components of a Neural Network\n",
    "#### 1. Input Layer:\n",
    "- Accepts raw data as input. Each neuron in this layer corresponds to a feature in the input data.\n",
    "- Example: If you are predicting house prices, features like \"size\", \"location\", and \"number of rooms\" will be inputs.\n",
    "#### 2. Hidden Layers:\n",
    "- Process data using weights, biases, and activation functions to learn intermediate representations.\n",
    "- The number of layers and neurons depends on the complexity of the task.\n",
    "#### 3 Output Layer:\n",
    "- Provides the final result of the model.\n",
    "- Example: In classification, this layer outputs probabilities or class labels.\n",
    "#### 4. Weights and Biases:\n",
    "- Weights determine the importance of a connection between neurons.\n",
    "- Bias adjusts the weighted sum output for flexibility in learning.\n",
    "#### 5. Activation Function:\n",
    "Introduces non-linearity, enabling the network to solve complex problems.\n",
    "Common activation functions: Sigmoid, ReLU, Tanh, Softmax.\n",
    "### How a Neural Network Works: Step-by-Step\n",
    "#### 1.Forward Propagation:\n",
    "- Data flows through the network from the input layer to the output layer.\n",
    "- Each neuron computes the weighted sum of inputs, applies an activation function, and sends the output to the next layer.\n",
    "#### 2. Loss Calculation:\n",
    "- A loss function measures the error between the predicted and actual values.\n",
    "#### 3. Backpropagation:\n",
    "- The network adjusts weights and biases to minimize the error.\n",
    "- This involves calculating gradients using the chain rule and updating parameters with an optimization algorithm like gradient descent.\n",
    "#### 4. Training:\n",
    "- The process of forward propagation, loss calculation, and backpropagation repeats over multiple iterations (epochs) until the network learns the patterns in the data.\n",
    "\n",
    "## Imlementation Steps\n",
    "1. Import the Required Libraries\n",
    "2. Load the Dataset\n",
    "3. Extract Features\n",
    "4. Preprocess Data\n",
    "5. Split the Data into Training and Test Sets\n",
    "6. Define the Deep Learning Model\n",
    "7. Compile the Model\n",
    "8. Train the Model\n",
    "9. Evaluate the Model\n",
    "10. Make Predictions\n",
    "11. Visualization\n",
    "12. Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87d116-029d-4d7b-92f1-551532f5214d",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "An activation function in a neural network defines the output of a node (neuron) given an input or a set of inputs. It determines whether a neuron should be activated or not based on the input it receives. Activation functions introduce non-linearity into the model, enabling it to learn and perform complex tasks.\n",
    "\n",
    "Without activation functions, neural networks would behave like a linear regression model, unable to capture the non-linear patterns in data.\n",
    "\n",
    "### Types\n",
    "#### 1. Linear Activation Function:\n",
    "- Formula: ùëì(ùë•) = ùë•\n",
    "- Problem: Does not introduce non-linearity; all layers collapse into a single layer.\n",
    "#### 2. Non-Linear Activation Functions: These are the most commonly used.\n",
    "- **Sigmoid**:\n",
    "    -Formula: \\$ \\frac{1}{1 + e^{-x}} \\$\n",
    "    - Range: 0 to 1\n",
    "    - Usage: Good for binary classification; used in the output layer.\n",
    "    - Drawbacks: Can cause vanishing gradients.\n",
    "- **Tanh (Hyperbolic Tangent)**:\n",
    "    - Formula: ùëì(ùë•) = tanh(ùë•) = \\$ \\frac{e^x + e^{-x}}{e^x - e^{-x}} \\$\n",
    "    - Range: ‚àí1 to 1\n",
    "    - Usage: Good for hidden layers; addresses vanishing gradients better than sigmoid.\n",
    "    - Drawbacks: Still susceptible to vanishing gradients for very large or small inputs.\n",
    "- **ReLU (Rectified Linear Unit)**:\n",
    "    - Formula: f(x)=max(0,x)\n",
    "    - Range: 0 to ‚àû\n",
    "    - Usage: Widely used in hidden layers; fast to compute.\n",
    "    - Drawbacks: Can suffer from \"dead neurons\" where gradients become zero for inputs less than 0.\n",
    "- **Leaky ReLU**:\n",
    "    - Formula: f(x)=x if x>0, otherwise f(x)=Œ±x(Œ±>0)\n",
    "    - Usage: Solves the \"dead neurons\" issue by allowing a small gradient for negative inputs.\n",
    "- **Softmax**:\n",
    "    - Formula: \\$ \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\$\n",
    "    - Range: 0 to 1\n",
    "    - Usage: Used in the output layer for multi-class classification; ensures probabilities sum to 1.\n",
    "\n",
    "### Example\n",
    "Suppose we have a neural network layer receiving the input vector x = [1, 2, 3] and the weights w = [0.5, 0.1, -0.4] with a bias b = 0.2.\n",
    "\n",
    "#### Step 1: Compute the pre-activation output (z):\n",
    "\\$\n",
    "z = w \\cdot x + b\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "z = (1 \\times 0.5) + (-2 \\times 0.1) + (3 \\times -0.4) + 0.2\n",
    "\\$\n",
    "\n",
    "\\$\n",
    "z = 0.5 - 0.2 - 1.2 + 0.2 = -0.7\n",
    "\\$\n",
    "\n",
    "#### Step 2: Apply an activation function.\n",
    "- **Sigmoid**:\n",
    "  \\$\n",
    "  f(z) = \\frac{1}{1 + e^{0.7}} \\approx 0.332\n",
    "    \\$\n",
    "- **ReLU**:\n",
    "    \\$\n",
    "  f(z) = \\max(0, -0.7) = 0\n",
    "    \\$\n",
    "- **Tanh**:\n",
    "    \\$\n",
    "  f(z) = \\tanh(-0.7) \\approx -0.604\n",
    "    \\$\n",
    "\n",
    "#### Explanation of Outputs:\n",
    "- **Sigmoid** squashes the value to a range between 0 and 1, indicating activation strength.\n",
    "- **ReLU** outputs 0, meaning the neuron is inactive for negative input.\n",
    "- **Tanh** outputs a negative value, showing activation while preserving sign.\n",
    "\n",
    "### Choosing an Activation Function\n",
    "\n",
    "- **Hidden Layers**:\n",
    "  - **ReLU**: Default choice due to simplicity and efficiency.\n",
    "  - **Leaky ReLU**: Use if the dead neuron issue arises.\n",
    "- **Output Layer**:\n",
    "  - **Sigmoid**: For binary classification.\n",
    "  - **Softmax**: For multi-class classification.\n",
    "  - **Linear**: For regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8022d-f15f-4896-b591-442d0752c38e",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "A loss function in deep learning quantifies how well the predicted outputs of a neural network match the actual target values. It is a critical component that guides the optimization process during training by measuring the error or deviation between predictions and ground truths. The goal of training a neural network is to minimize this loss.\n",
    "### Types\n",
    "Loss functions can be categorized based on the type of task:\n",
    "#### 1. Regression Loss Functions\n",
    "- MSE\n",
    "- MAE\n",
    "#### 2. Classification Loss Functions\n",
    "**Binary Cross-Entropy:**\n",
    "\n",
    "\\$\n",
    "\\text{BCE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "\\$\n",
    "   \n",
    "**Categorical Cross-Entropy:**\n",
    "\n",
    "\\$\n",
    "\\text{CCE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "\\$\n",
    "   \n",
    "**Hinge Loss:**\n",
    "\n",
    "\\$\n",
    "\\text{Hinge Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i)\n",
    "\\$\n",
    "\n",
    "### Choosing a Loss Function\n",
    "**1. Regression**:\n",
    "- **MSE** for smoother models that penalize large errors.\n",
    "- **MAE** for robust models against outliers.\n",
    "\n",
    "**2. Classification**:\n",
    "- **Binary Cross-Entropy** for binary problems.\n",
    "- **Categorical Cross-Entropy** for multi-class problems.\n",
    "- **Hinge Loss** for margin-based classifiers like SVMs.\n",
    "\n",
    "**3. Custom Tasks**:\n",
    "- Combine loss functions or define your own based on specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0fe00-e4f9-4c1e-98cf-7aa7cb40696e",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning and deep learning models by iteratively updating model parameters (weights and biases) in the direction of the negative gradient of the loss function. This process enables the model to learn the best parameters for making accurate predictions.\n",
    "\n",
    "### Key Concepts\n",
    "**1. Loss Function**: A function that quantifies the error between predicted and actual values. Common loss functions include Mean Squared Error (MSE) for regression and Cross-Entropy for classification.\n",
    "**2. Gradient**: A vector of partial derivatives of the loss function with respect to model parameters. It indicates the direction and rate of change of the loss function.\n",
    "**3. Learning Rate (Œ∑)**:\n",
    "- A small positive value that determines the step size for updating parameters.\n",
    "- A high learning rate can cause overshooting, while a low learning rate slows convergence.\n",
    "- \n",
    "### Step-by-Step Process\n",
    "**1. Initialize Parameters**: Start with random values for weights (w) and biases (b).\n",
    "\n",
    "**2. Compute Predictions**: Use the model to predict outputs (\\$ \\hat{y} \\$).\n",
    "\n",
    "**3. Calculate Loss:** Compute the loss using a predefined loss function, such as MSE.\n",
    "\n",
    "**4. Compute Gradients**: Calculate the derivatives of the loss with respect to each paramete: \n",
    "\n",
    "\\$\n",
    "\\frac{‚àÇw}{‚àÇL}, \\frac{‚àÇL}{‚àÇb}\n",
    "\\$\n",
    "\n",
    "**5. Update Parameters:** Adjust the parameters in the direction of the negative gradient:\n",
    "\n",
    "\\$\n",
    "w=w‚àíŒ∑.\\frac{‚àÇw}{‚àÇL}‚ãÖ \n",
    "b=b‚àíŒ∑.\\frac{‚àÇL}{‚àÇb}‚ãÖ \n",
    "\\$\n",
    "**6. Repeat**: Iterate over the dataset multiple times (epochs) until the loss converges to a minimum or a stopping criterion is met.\n",
    "\n",
    "### Variants\n",
    "#### Batch Gradient Descent:\n",
    "- Computes the gradient using the entire dataset in one iteration.\n",
    "- Pros: Stable convergence.\n",
    "- Cons: Slow for large datasets.\n",
    "#### Stochastic Gradient Descent (SGD):\n",
    "- Computes the gradient for one data point at a time.\n",
    "- Pros: Faster updates, suitable for large datasets.\n",
    "- Cons: Noisy convergence.\n",
    "#### Mini-Batch Gradient Descent:\n",
    "- Computes the gradient for small batches of data.\n",
    "- Pros: Combines the advantages of batch and stochastic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cd633-912b-4f54-8a2d-5c45eea505b1",
   "metadata": {},
   "source": [
    "# Train Deep Neural Network\n",
    "## Data Augmentation\n",
    "Data augmentation generates additional training examples by applying transformations to existing data. It helps the model generalize better by exposing it to variations.\n",
    "### Why Use Data Augmentation?\n",
    "- Reduces overfitting.\n",
    "- Increases dataset size, especially when data is limited.\n",
    "- Improves model robustness to variations.\n",
    "### Common Data Augmentation Techniques\n",
    "#### Image Data\n",
    "- **Flipping**: Horizontal or vertical flips.\n",
    "- **Rotation**: Rotate images by random angles.\n",
    "- **Scaling**: Zoom in/out on images.\n",
    "- **Cropping**: Randomly crop parts of images.\n",
    "- **Brightness Adjustment**: Vary brightness to mimic lighting conditions.\n",
    "- **Noise Addition**: Add Gaussian noise to simulate variability.\n",
    "#### Text Data\n",
    "- **Synonym Replacement**: Replace words with their synonyms.\n",
    "- **Back Translation**: Translate text to another language and back.\n",
    "- **Random Deletion**: Remove random words from sentences.\n",
    "#### Time-Series Data:\n",
    "- **Jittering**: Add random noise.\n",
    "- **Time Warping**: Stretch or compress time intervals.\n",
    "- **Random Sampling**: Drop some data points randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec557747-c4f9-4886-96e6-25acd88fbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Image Data\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Example Image Data\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load image (as numpy array)\n",
    "img = np.array(Image.open('example.jpg'))\n",
    "img = img.reshape((1,) + img.shape)  # Reshape for the generator\n",
    "\n",
    "for batch in datagen.flow(img, batch_size=1):\n",
    "    augmented_image = batch[0]  # Get the augmented image\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae1c93-a422-450f-a772-2734f8cdbe0b",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "Batch Normalization (BN) is a technique to improve the training of deep neural networks by normalizing intermediate layers.\n",
    "\n",
    "### Why Batch Normalization?\n",
    "1. **Internal Covariate Shift**:\n",
    "- During training, the distribution of inputs to each layer changes due to updates in the parameters of the previous layers. This phenomenon is called internal covariate shift.\n",
    "- BN addresses this by normalizing the input to each layer, reducing dependency on the initialization of weights.\n",
    "\n",
    "2. **Improved Gradient Flow**: BN reduces the risk of vanishing or exploding gradients, enabling deeper networks to train efficiently.\n",
    "\n",
    "3. **Faster Convergence**: By stabilizing the input distribution, BN allows the network to use higher learning rates.\n",
    "\n",
    "4. **Regularization Effect**: BN introduces noise to the layer‚Äôs input during training, acting as a form of regularization and reducing the need for dropout.\n",
    "\n",
    "### Where to Apply Batch Normalization\n",
    "- BN is typically applied after the affine transformation (e.g., Wx+b) and before the activation function.\n",
    "- It is commonly used in fully connected layers, convolutional layers, and sometimes recurrent layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f304e-e5ac-4698-9cb8-67a524e9f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128),\n",
    "    BatchNormalization(),  # Apply Batch Normalization\n",
    "    Activation('relu'),    # Activation after BN\n",
    "    Dense(64),\n",
    "    BatchNormalization(),  # Apply Batch Normalization\n",
    "    Activation('relu'),\n",
    "    Dense(10, activation='softmax')  # Output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6c281-4c76-4b90-bf51-d5dc09648cb2",
   "metadata": {},
   "source": [
    "## Dropout Regularization\n",
    "Dropout is a regularization technique where a fraction of the neurons in a layer are randomly \"dropped out\" (set to zero) during each training iteration. This prevents the network from becoming overly dependent on specific neurons, leading to better generalization.\n",
    "###  How Dropout Works\n",
    "- During training, for each forward pass, a random subset of neurons is ignored.\n",
    "- During inference (testing), all neurons are used, but their outputs are scaled by the dropout rate to account for the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f76d3d-b769-4c89-a9f6-7bb68f98c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),  # Dropout with rate 0.2\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d03dd1-a6a0-4304-9acd-6321bd547613",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "Optimizers are algorithms or methods used to update the weights and biases of a neural network to minimize the loss function.\n",
    "### SGD:\n",
    "- Uses a fixed learning rate for updates.\n",
    "- Training may be slower and prone to oscillations.\n",
    "- simple and effective for convex problems but can struggle with complex deep learning models.\n",
    "\n",
    "### RMSprop:\n",
    "- Adapts learning rates for each parameter.\n",
    "- Handles non-stationary objectives well, leading to faster convergence.\n",
    "- adapts learning rates dynamically, making it suitable for deep neural networks.\n",
    "### Adam:\n",
    "- Combines momentum and adaptive learning rates.\n",
    "- Typically achieves the best results with minimal hyperparameter tuning.\n",
    "- versatile and widely used optimizer that combines the strengths of SGD and RMSprop, making it the default choice for most tasks.\n",
    "\n",
    "### Comparison of SGD, RMSprop, and Adam\n",
    "| Feature                | SGD                     | RMSprop            | Adam                        |\n",
    "|------------------------|-------------------------|--------------------|-----------------------------|\n",
    "| **Learning Rate**      | Fixed or Decayed        | Adaptive           | Adaptive                   |\n",
    "| **Momentum**           | Optional               | No                 | Yes                        |\n",
    "| **Handling Gradients** | Oscillations in Path    | Smoothing by Gradients | Combines Momentum + RMS   |\n",
    "| **Performance**        | Slower                 | Faster in Practice | Generally Best for DL      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e82632-b339-4578-9c11-de813f506bdb",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for tasks involving spatial data, such as images and videos.\n",
    "## Components\n",
    "### 1. Convolutional Layers\n",
    "- **Purpose**: Extract features from input images by applying filters (kernels) that slide across the input.\n",
    "- **Operation**:\n",
    "    - The filter computes a weighted sum of the pixel values it covers.\n",
    "    - This produces a \"feature map\" that highlights certain features of the image (e.g., edges, textures).\n",
    "- **Key Parameters**:\n",
    "    - Kernel size: Dimensions of the filter (e.g., 3x3, 5x5).\n",
    "    - Stride: Step size for the filter's movement.\n",
    "    - Padding: Adding borders to the input to preserve dimensions.\n",
    "### 2. Activation Function\n",
    "### 3. Pooling Layers\n",
    "- **Purpose**: Downsample the feature maps to reduce dimensionality and computation while retaining important information.\n",
    "- **Types**:\n",
    "    - **Max Pooling**: Takes the maximum value in a region.\n",
    "    - **Average Pooling**: Takes the average value in a region.\n",
    "### 4. Fully Connected Layers\n",
    "- **Purpose**: Combine features extracted by the convolutional layers to perform final predictions.\n",
    "- The output from the previous layers is flattened and passed through dense layers.\n",
    "### 5. Dropout (Regularization)\n",
    "- **Purpose**: Prevent overfitting by randomly deactivating neurons during training.\n",
    "## Workflow\n",
    "1. **Input**: An image (e.g., 32x32x3 for a color image with 3 channels: RGB).\n",
    "2. **Convolution**: Apply filters to extract feature maps.\n",
    "3. **Activation (ReLU)**: Introduce non-linearity.\n",
    "4. **Pooling**: Downsample feature maps.\n",
    "5. **Repeat Steps 2-4**: Extract higher-level features.\n",
    "6. **Flatten**: Convert the feature maps to a 1D vector.\n",
    "7. **Fully Connected Layer**: Predict class probabilities or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894fb5bd-574a-4f27-8ae1-6573bf67e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
