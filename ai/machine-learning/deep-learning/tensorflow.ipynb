{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b54d5e5-4a1b-4158-b03b-55868d5fcabc",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Introduction\n",
    "    - Core Concepts\n",
    "    - Workflow\n",
    "    - Structure\n",
    "- Tensorflow Basics\n",
    "    - Characterstics of Tensors\n",
    "    - Creating Tensors\n",
    "    - Tensor Operations\n",
    "    - Constants\n",
    "    - Variables\n",
    "    - Data Types\n",
    "    - Shape\n",
    "    - Casting\n",
    "- Data Handling\n",
    "    - Input Pipelines using `tf.data`\n",
    "    - Components\n",
    "    - Creating Dataset\n",
    "    - Transforming Dataset\n",
    "    - Input Pipeline\n",
    "    - Data Augmentation\n",
    "    - Datasets\n",
    "- Deep Learning Basics\n",
    "    - Building neural networks with Keras\n",
    "- Convolutional Neural Networks\n",
    "    - Pretrained models\n",
    "- Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68d7fa-ba2b-4c3e-a7f2-e440c2bcdc07",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "TensorFlow is an open-source framework developed by Google for machine learning and deep learning tasks.\n",
    "\n",
    "## Core Concepts\n",
    "1. **Tensors**: These are multi-dimensional arrays (similar to NumPy arrays) that serve as the primary data structure in TensorFlow.\n",
    "2. **Graphs**: TensorFlow represents computations as data flow graphs, where nodes represent operations, and edges represent tensors.\n",
    "3. **Eager Execution**: TensorFlow supports eager execution, which allows operations to execute immediately, making debugging easier and more intuitive.\n",
    "4. **Keras API**: TensorFlow provides Keras as a high-level API for building and training machine learning models.\n",
    "\n",
    "## Workflow\n",
    "### 1. Data Preparation\n",
    "- Import and preprocess data to make it suitable for training.\n",
    "- Tasks include normalization, augmentation, and splitting datasets into training, validation, and testing subsets.\n",
    "### 2. Model Building\n",
    "- Create a model architecture. TensorFlow provides APIs like:\n",
    "    - Sequential API: For simple, linear stacks of layers.\n",
    "    - Functional API: For more complex models with branching and shared layers.\n",
    "    - Model Subclassing: For custom model designs.\n",
    "### 3. Model Compilation\n",
    "- Loss Function: Measures the difference between predicted and actual values.\n",
    "- Optimizer: Updates model weights to minimize the loss function.\n",
    "- Metrics: Evaluates model performance.\n",
    "### 4. Model Training\n",
    "- Train the model on the training dataset using the fit method.\n",
    "- Monitor performance on a validation dataset.\n",
    "### 5. Model Evaluation\n",
    "- Test the trained model on unseen data (test dataset) to evaluate its generalization.\n",
    "### 6. Model Prediction\n",
    "- Use the trained model to make predictions on new data.\n",
    "### 7. Model Deployment\n",
    "- Deploy the model to production using TensorFlow Serving, TensorFlow Lite (for mobile), or TensorFlow.js (for web).\n",
    "\n",
    "## Structure\n",
    "### 1. Core Components\n",
    "- **Tensors**: Multi-dimensional arrays that are the fundamental data structure in TensorFlow.\n",
    "- **Operations (Ops)**: Nodes in the computation graph that perform mathematical computations.\n",
    "- **Graphs**: TensorFlow represents computations as a data flow graph, though eager execution mode allows immediate computation for simplicity.\n",
    "### 2. APIs\n",
    "#### High-Level APIs:\n",
    "- `tf.keras`: For building and training machine learning models.\n",
    "- `tf.data`: For efficient data input pipelines.\n",
    "- `tf.losses`, `tf.metrics`, `tf.optimizers`: For managing losses, metrics, and optimization.\n",
    "#### Low-Level APIs:\n",
    "- `tf.math`: For mathematical operations.\n",
    "- `tf.raw_ops`: For accessing low-level operations.\n",
    "### 3. Layers\n",
    "Layers are building blocks of models. Examples include:\n",
    "- **Dense**: Fully connected layers.\n",
    "- **Conv2D**: Convolutional layers.\n",
    "- **LSTM**: Recurrent layers.\n",
    "### 4. Checkpoints and Saved Models\n",
    "- **Checkpoints**: Save model weights periodically during training.\n",
    "- **Saved Model Format**: Save the complete model, including architecture, weights, and optimizer configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a965671-81b6-4e48-b866-8ae32dded762",
   "metadata": {},
   "source": [
    "# Tensorflow Basics\n",
    "\n",
    "Tensors are the fundamental data structure in TensorFlow. They are multi-dimensional arrays or matrices, similar to NumPy arrays but with added capabilities for GPU acceleration. Tensors represent the input, output, and intermediate states of machine learning computations in TensorFlow.\n",
    "\n",
    "## Tensor\n",
    "\n",
    "A tensor is a multi-dimensional array (like a matrix) and a fundamental data structure in TensorFlow. Itâ€™s designed to handle high-dimensional data and operations on that data, much like vectors and matrices in linear algebra.\n",
    "\n",
    "## Characteristics of Tensors\n",
    "\n",
    "### 1. Rank (Dimensionality)\n",
    "\n",
    "- The number of dimensions in a tensor.\n",
    "- Examples:\n",
    "  - Scalar: Rank 0 (e.g., `3`)\n",
    "  - Vector: Rank 1 (e.g., `[1, 2, 3]`)\n",
    "  - Matrix: Rank 2 (e.g., `[[1, 2], [3, 4]]`)\n",
    "  - Higher dimensions: Rank 3+ (e.g., a 3D tensor for images, `[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]`)\n",
    "\n",
    "### 2. Shape\n",
    "\n",
    "- The size of each dimension in a tensor.\n",
    "- Example: A tensor with shape (3, 4) has 3 rows and 4 columns.\n",
    "\n",
    "### 3. Data Type\n",
    "\n",
    "- Tensors support various data types, such as float32, int32, bool, etc.\n",
    "\n",
    "### 4. Immutability:\n",
    "\n",
    "- Tensors are immutable; their values cannot be changed after creation.\n",
    "\n",
    "## Creating Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e0f50-f926-4710-a832-644bdafa5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python objects\n",
    "import tensorflow as tf\n",
    "scalar = tf.constant(3)  # Scalar\n",
    "vector = tf.constant([1, 2, 3])  # Vector\n",
    "matrix = tf.constant([[1, 2], [3, 4]])  # Matrix\n",
    "tensor = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # 3D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a0b6e-e321-4147-bdd6-7c548435a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorFlow functions\n",
    "zeros = tf.zeros([2, 3])  # 2x3 tensor of zeros\n",
    "ones = tf.ones([2, 3])  # 2x3 tensor of ones\n",
    "random = tf.random.uniform([2, 3], minval=0, maxval=10)  # Random tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb265d-d394-4654-839f-2686f9f2c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting NumPy arrays\n",
    "import numpy as np\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "tf_tensor = tf.convert_to_tensor(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4aecb-7ff7-4e09-b377-3d6c224cd677",
   "metadata": {},
   "source": [
    "## Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924a39b-8750-42f0-ab8e-f082e141d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Math Operations\n",
    "a = tf.constant([1, 2, 3])\n",
    "b = tf.constant([4, 5, 6])\n",
    "\n",
    "add = tf.add(a, b)  # [5, 7, 9]\n",
    "sub = tf.subtract(a, b)  # [-3, -3, -3]\n",
    "mul = tf.multiply(a, b)  # [4, 10, 18]\n",
    "div = tf.divide(a, b)  # [0.25, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ed6c3-c9e1-476e-b168-9f696ab255c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Operations\n",
    "mat1 = tf.constant([[1, 2], [3, 4]])\n",
    "mat2 = tf.constant([[5, 6], [7, 8]])\n",
    "\n",
    "mat_mul = tf.matmul(mat1, mat2)  # Matrix multiplication\n",
    "transpose = tf.transpose(mat1)  # Transpose of mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5439546-4c34-4f6a-939d-27cef726f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise Operations\n",
    "square = tf.square(a)  # Square each element\n",
    "sqrt = tf.sqrt(tf.cast(a, tf.float32))  # Square root (requires float data type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79304f1f-f7f8-457e-ab1a-c00ba3bdbc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction Operations\n",
    "reduce_sum = tf.reduce_sum(a)  # Sum of elements: 6\n",
    "reduce_mean = tf.reduce_mean(a)  # Mean of elements: 2.0\n",
    "reduce_max = tf.reduce_max(a)  # Maximum element: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bcd45-df17-4702-8ed7-054226024949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and Manipulating Tensors\n",
    "reshape = tf.reshape(mat1, [1, 4])  # Reshape to 1x4\n",
    "expand_dims = tf.expand_dims(a, axis=0)  # Add a new dimension: [[1, 2, 3]]\n",
    "squeeze = tf.squeeze(expand_dims)  # Remove dimensions of size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bd8fe2-ce68-4395-9cae-8859b5457395",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85285f56-f58c-4ef6-b3f7-579c9a72138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Constants\n",
    "# Scalar constant\n",
    "scalar = tf.constant(3, dtype=tf.int32)\n",
    "\n",
    "# Vector constant\n",
    "vector = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "\n",
    "# Matrix constant\n",
    "matrix = tf.constant([[1, 2], [3, 4]], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f7a6d-e0cd-410d-8ab1-3d8b19e753b0",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1cb12-e392-4b51-b850-304df625b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Variable\n",
    "# Scalar variable\n",
    "scalar_var = tf.Variable(5, dtype=tf.int32)\n",
    "\n",
    "# Vector variable\n",
    "vector_var = tf.Variable([1.0, 2.0, 3.0], dtype=tf.float32)\n",
    "\n",
    "# Matrix variable\n",
    "matrix_var = tf.Variable([[1, 2], [3, 4]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b68d61-aa1d-45f2-9111-2e2c67eeb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Variable\n",
    "# Assign a new value to a variable\n",
    "scalar_var.assign(10)\n",
    "\n",
    "# Increment a variable\n",
    "scalar_var.assign_add(5)  # Add 5 to the variable\n",
    "print(\"Updated value:\", scalar_var.numpy())  # Output: 15\n",
    "\n",
    "# Decrement a variable\n",
    "scalar_var.assign_sub(3)  # Subtract 3 from the variable\n",
    "print(\"Decremented value:\", scalar_var.numpy())  # Output: 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbaea16-d3af-4b19-a699-c442c7ebb7ca",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "**Numeric Data Types**\n",
    "- `tf.float16`: 16-bit floating point.\n",
    "- `tf.float32`: 32-bit floating point (default for most operations).\n",
    "- `tf.float64`: 64-bit floating point (higher precision).\n",
    "- `tf.int8`: 8-bit signed integer.\n",
    "- `tf.int16`: 16-bit signed integer.\n",
    "- `tf.int32`: 32-bit signed integer.\n",
    "- `tf.int64`: 64-bit signed integer.\n",
    "- `tf.uint8`: 8-bit unsigned integer.\n",
    "**Boolean Data Type**\n",
    "- `tf.bool`: Represents True or False.\n",
    "**String Data Type**\n",
    "- `tf.string`: Represents textual data, which is variable in size.\n",
    "**Complex Numbers**\n",
    "- `tf.complex64`: 64-bit complex numbers.\n",
    "- `tf.complex128`: 128-bit complex numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3501b9-9465-4216-bb22-47ddf11f91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float tensor\n",
    "float_tensor = tf.constant(3.14, dtype=tf.float32)\n",
    "\n",
    "# Integer tensor\n",
    "int_tensor = tf.constant([1, 2, 3], dtype=tf.int32)\n",
    "\n",
    "# Boolean tensor\n",
    "bool_tensor = tf.constant([True, False, True], dtype=tf.bool)\n",
    "\n",
    "# String tensor\n",
    "string_tensor = tf.constant([\"TensorFlow\", \"is\", \"fun\"], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe258b-ef84-4457-b687-334cb1189de9",
   "metadata": {},
   "source": [
    "## Shapes\n",
    "- **Scalar (Rank 0)**: No dimensions. Example: `3` (Shape: `()`).\n",
    "- **Vector (Rank 1)**: One dimension. Example: `[1, 2, 3]` (Shape: `(3,)`).\n",
    "- **Matrix (Rank 2)**: Two dimensions. Example: `[[1, 2], [3, 4]]` (Shape: `(2, 2)`).\n",
    "- **Tensor (Rank 3+)**: Three or more dimensions. Example: A batch of images (Shape: `(batch_size, height, width, channels)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db678c-d928-404c-934c-3b1b299982de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar\n",
    "scalar = tf.constant(5)  # Shape: ()\n",
    "\n",
    "# Vector\n",
    "vector = tf.constant([1, 2, 3])  # Shape: (3,)\n",
    "\n",
    "# Matrix\n",
    "matrix = tf.constant([[1, 2], [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "# 3D Tensor\n",
    "tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # Shape: (2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3ad4a-dc0f-4454-afc1-6609e599f2d9",
   "metadata": {},
   "source": [
    "## Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85f2b5-c9c1-43fa-9aa4-e67e20baec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type Casting\n",
    "x_float = tf.cast(x, dtype=tf.float32)  # Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb8f68-5856-4f17-aa5f-9fd3f3df6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Casting\n",
    "matrix = tf.reshape(vector, [2, 3])  # Shape: (2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2132d2-b4a3-47a5-8fad-54ab5de503d6",
   "metadata": {},
   "source": [
    "# Data Handling\n",
    "## Input Pipelines Using `tf.data`\n",
    "The `tf.data` API in TensorFlow is a powerful and flexible tool for building input pipelines. It allows you to efficiently load, preprocess, and feed data into your machine learning models. This API is designed to handle large datasets, batch data efficiently, and process data in parallel to optimize model training.\n",
    "\n",
    "## Components\n",
    "1. `tf.data.Dataset`: Represents a sequence of elements, where each element contains one or more tf.Tensor objects.\n",
    "2. **Operations**: Allow transformation of datasets, such as shuffling, batching, mapping, and filtering.\n",
    "3. **Iterators**: Provide a way to iterate through the elements of the dataset.\n",
    "## Creating a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31427dbf-ff21-4431-b683-aedace5a8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Tensors\n",
    "data = tf.constant([1, 2, 3, 4, 5])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for element in dataset:\n",
    "    print(element.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6cb61-a10a-4735-a123-a0e739942dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FIle - Text dataset example\n",
    "text_dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n",
    "for line in text_dataset.take(3):  # Read first 3 lines\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a82d45-b0a9-4ebd-8147-ce4950ada506",
   "metadata": {},
   "source": [
    "## Transforming a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2671fa5-29eb-452e-8fff-83bed05608f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "batched_dataset = dataset.batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0843c-0eb9-448f-ae68-8f3797241f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7d7ee-d54b-4a48-92b7-8e2f23e4810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "mapped_dataset = dataset.map(lambda x: x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea9b4a-fea7-4006-9a8d-2639b4555753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching\n",
    "prefetched_dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704086b4-bf0b-45d7-9b3c-f388e1a454f3",
   "metadata": {},
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83070d-6e6f-4db3-935a-c36fa4f008bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input pipeline for image data.\n",
    "# Step 1: Simulate image data and labels\n",
    "image_data = tf.random.uniform([100, 64, 64, 3])  # 100 images of size 64x64x3\n",
    "labels = tf.random.uniform([100], maxval=10, dtype=tf.int32)  # 100 labels (0-9)\n",
    "\n",
    "# Step 2: Create a dataset from tensors\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_data, labels))\n",
    "\n",
    "# Step 3: Shuffle, batch, and prefetch\n",
    "batch_size = 16\n",
    "input_pipeline = (\n",
    "    dataset\n",
    "    .shuffle(buffer_size=100)  # Shuffle the dataset\n",
    "    .batch(batch_size)         # Create batches\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)  # Prefetch for performance\n",
    ")\n",
    "\n",
    "# Step 4: Iterate through the dataset\n",
    "for batch_images, batch_labels in input_pipeline.take(1):  # Process one batch\n",
    "    print(\"Batch Images Shape:\", batch_images.shape)\n",
    "    print(\"Batch Labels Shape:\", batch_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa0861-dd8c-43a6-b99d-0ef898eba720",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Data augmentation is a technique to artificially increase the size and diversity of a dataset by applying various transformations to the data. In TensorFlow, augmentation is commonly applied to image datasets to improve model generalization and reduce overfitting. TensorFlow provides built-in functions in tf.image and the tensorflow.keras.preprocessing module for efficient data augmentation.\n",
    "### Common Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6a79e-c64f-4238-b5d9-2a981719b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example image\n",
    "image = tf.io.read_file(\"image.jpg\")\n",
    "image = tf.image.decode_jpeg(image)\n",
    "\n",
    "# Horizontal flip\n",
    "flipped_horizontally = tf.image.flip_left_right(image)\n",
    "\n",
    "# Vertical flip\n",
    "flipped_vertically = tf.image.flip_up_down(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654eec96-3104-4eac-b1cc-06d39b14c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate image by 90 degrees\n",
    "rotated_image = tf.image.rot90(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d19216-9a25-4a81-9759-33eb0681d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central crop\n",
    "center_cropped = tf.image.central_crop(image, central_fraction=0.5)\n",
    "\n",
    "# Random crop\n",
    "random_cropped = tf.image.random_crop(image, size=[100, 100, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7da3ca-ebe3-41de-9ff8-1b0d8c31ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize while maintaining aspect ratio with padding\n",
    "resized_image = tf.image.resize_with_pad(image, target_height=128, target_width=128)\n",
    "\n",
    "# Resize with cropping\n",
    "resized_image_cropped = tf.image.resize_with_crop_or_pad(image, target_height=128, target_width=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16b7c7-80b6-4f37-af39-943a76242ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust brightness\n",
    "brightness_adjusted = tf.image.adjust_brightness(image, delta=0.1)\n",
    "\n",
    "# Adjust contrast\n",
    "contrast_adjusted = tf.image.adjust_contrast(image, contrast_factor=2)\n",
    "\n",
    "# Adjust saturation\n",
    "saturation_adjusted = tf.image.adjust_saturation(image, saturation_factor=2)\n",
    "\n",
    "# Adjust hue\n",
    "hue_adjusted = tf.image.adjust_hue(image, delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165bce95-3c00-4d3a-b5b2-7b674f456040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random noise\n",
    "noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1)\n",
    "noisy_image = tf.clip_by_value(image + noise, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ac080-ced4-4b35-9b19-110a6f6b0d81",
   "metadata": {},
   "source": [
    "### Data Augmentation with `tf.keras.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c2fcd-9170-42dd-85e6-70975e190ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define an ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Example: Apply augmentation to a single image\n",
    "image = tf.keras.preprocessing.image.load_img(\"image.jpg\")\n",
    "image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "image_array = image_array.reshape((1,) + image_array.shape)\n",
    "\n",
    "# Generate augmented images\n",
    "for batch in datagen.flow(image_array, batch_size=1):\n",
    "    break  # Stop after generating one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cbce77-4c8b-497a-983c-898eeacfe647",
   "metadata": {},
   "source": [
    "### Data Augmentation with `tf.data` Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045fc75-1b16-4f35-820a-6df1ad1cb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Example dataset\n",
    "image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]\n",
    "labels = [0, 1, 0]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "def preprocess_and_augment(image_path, label):\n",
    "    # Load and decode the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    \n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    \n",
    "    # Apply augmentations\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing and augmentation\n",
    "dataset = dataset.map(preprocess_and_augment)\n",
    "\n",
    "# Batch and prefetch\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7b0ef-ff40-4afc-83ff-bd8936e3186a",
   "metadata": {},
   "source": [
    "### Data Augmentation with `tf.keras.layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26143f73-2c8e-4b1f-aaa0-ec204c206c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define augmentation layers\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "])\n",
    "\n",
    "# Apply augmentation to an image\n",
    "augmented_image = data_augmentation(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6226049-ba2a-4397-9a5e-6e17a05ab357",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Working with datasets in TensorFlow often involves using the `tf.data` API, which is a powerful and flexible way to handle data pipelines. It is designed to process large datasets efficiently and can be used for tasks such as data loading, preprocessing, and augmentation.\n",
    "\n",
    "It is a powerful tool for building input pipelines to handle large datasets efficiently. It simplifies the process of loading, preprocessing, and iterating over data, making it ideal for preparing data for machine learning models, especially when working with large and complex datasets.\n",
    "\n",
    "## Key Concepts\n",
    "1. **Data Representation**:\n",
    "    - The `Dataset` API represents a sequence of data elements, where each element consists of one or more components. Components can be single values, vectors, or even complex data structures like images and labels.\n",
    "    - This API allows users to create datasets from arrays, files, or generators and transform them with functions that prepare the data for machine learning models.\n",
    "\n",
    "2. **Creating a Dataset**:\n",
    "    - You can create datasets from different sources such as in-memory data (e.g., lists, arrays), TFRecord files, or text files.\n",
    "    - The `Dataset.from_tensor_slices()` method is commonly used to create datasets from in-memory data, such as lists or NumPy arrays.\n",
    "\n",
    "3. **Transformations**: The API provides various transformations to process data efficiently, such as\n",
    "    - `map()`: Apply a function to each element in the dataset.\n",
    "    - `batch()`: Combines consecutive elements into batches, enabling mini-batch processing.\n",
    "    - `shuffle()`: Randomly shuffles the data to improve model training and reduce overfitting.\n",
    "    - `repeat()`: Repeats the dataset multiple times, useful for training over multiple epochs.\n",
    "    \n",
    "4. **Optimizing Pipelines**:\n",
    "    - The API includes optimizations like prefetching, parallel mapping, and caching.\n",
    "    - `prefetch()` helps overlap data preparation with model training by fetching the next -batch while the current batch is being processed.\n",
    "    - `cache()` can store the dataset in memory after the first epoch, speeding up subsequent epochs.\n",
    "\n",
    "5. **Iteration**: A `tf.data.Dataset` object is iterable, allowing you to loop through it directly or retrieve individual batches in a session or eager execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a345e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: features (inputs) and labels (targets)\n",
    "features = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0]], dtype=np.float32)\n",
    "labels = np.array([0, 1, 0, 1], dtype=np.int32)\n",
    "\n",
    "# Step 1: Create a Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Step 2: Shuffle, Batch, and Repeat\n",
    "batch_size = 2\n",
    "dataset = dataset.shuffle(buffer_size=4).batch(batch_size).repeat(2)  # Shuffle, batch, and repeat\n",
    "\n",
    "# Step 3: Map Transformation (Optional)\n",
    "# Apply a simple map transformation to add noise to features (e.g., for data augmentation)\n",
    "def add_noise(features, labels):\n",
    "    noise = tf.random.normal(shape=tf.shape(features), mean=0.0, stddev=0.1)\n",
    "    features = features + noise\n",
    "    return features, labels\n",
    "\n",
    "dataset = dataset.map(add_noise)\n",
    "\n",
    "# Step 4: Prefetch for Optimized Performance\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Step 5: Iterate over the Dataset\n",
    "for batch in dataset:\n",
    "    print(\"Features:\", batch[0].numpy())\n",
    "    print(\"Labels:\", batch[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c09c0a",
   "metadata": {},
   "source": [
    "### Creating a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a2683-eff2-498f-afe8-d8bb7cc8e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset from Python list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Create dataset from NumPy array\n",
    "array_data = np.array([6, 7, 8, 9, 10])\n",
    "numpy_dataset = tf.data.Dataset.from_tensor_slices(array_data)\n",
    "\n",
    "# Reading from a text file\n",
    "text_dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n",
    "\n",
    "# From Generators\n",
    "def data_generator():\n",
    "    for i in range(10):\n",
    "        yield i\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(data_generator, output_types=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1ec42-dec2-4081-9604-065010aa3544",
   "metadata": {},
   "source": [
    "### Transforming a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f58dd-b6fe-45ac-85c9-58c00742465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "mapped_dataset = dataset.map(square)\n",
    "\n",
    "# Batching\n",
    "batched_dataset = dataset.batch(3)  # Each batch will contain 3 elements\n",
    "\n",
    "# Shuffling\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=10)  # Buffer size determines the randomness\n",
    "\n",
    "# Repeating\n",
    "repeated_dataset = dataset.repeat(3)  # Repeat the dataset 3 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72b131-8ef9-40d6-828f-81053c5d81f2",
   "metadata": {},
   "source": [
    "### Iterating Through the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906080f-d8b8-4337-b7b5-1703f371b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in batched_dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9ef24-c021-4e61-9ef7-5e2ded1399aa",
   "metadata": {},
   "source": [
    "### Prefetching for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba7645-d558-4b93-9af3-3bc87854378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefetched_dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5476a5a8-4647-44c6-9ff1-4105631ca8d2",
   "metadata": {},
   "source": [
    "# Deep Learning Basics\n",
    "\n",
    "## Building neural networks with Keras\n",
    "\n",
    "Keras supports three main ways to define a neural network:\n",
    "\n",
    "- **Sequential API**: A linear stack of layers.\n",
    "- **Functional API**: For more complex architectures.\n",
    "- **Model Subclassing**: For custom models.\n",
    "\n",
    "### Sequential Constructor\n",
    "\n",
    "The `Sequential` API in Keras is an easy and intuitive way to build and train simple neural network models layer by layer. This API is ideal for models that consist of a single input and a single output and follow a linear stack of layers.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Sequential Model**:\n",
    "   - The `Sequential` model is a linear stack of layers that are added one after another.\n",
    "   - It is particularly useful for straightforward models like feedforward neural networks (dense layers) and simple convolutional or recurrent layers.\n",
    "2. Layer Stacking:\n",
    "   - You define the model by stacking different types of layers in the order you want them to execute.\n",
    "   - Layers include Dense (fully connected layers), Conv2D (convolutional layers), Flatten, Dropout, and more.\n",
    "3. **Compilation**:\n",
    "   - After defining the layers, you compile the model to configure the learning process. This involves setting the optimizer, loss function, and metrics.\n",
    "   - The optimizer (e.g., `adam`, `sgd`) controls how the model learns.\n",
    "   - The loss function (e.g., `binary_crossentropy`, `mean_squared_error`) measures the modelâ€™s error and guides the training.\n",
    "   - Metrics (e.g., `accuracy`) are used to evaluate the modelâ€™s performance.\n",
    "4. **Training the Model**:\n",
    "   - After compiling, the model can be trained on data using the `fit()` method. This involves passing the input data, target labels, batch size, and number of epochs.\n",
    "   - During training, the model optimizes its weights to reduce the loss function value.\n",
    "5. **Evaluation and Prediction**:\n",
    "   - Once trained, the modelâ€™s performance can be evaluated on a test set using `evaluate()`.\n",
    "   - Predictions for new data can be made using `predict()`.\n",
    "\n",
    "#### Arguments\n",
    "\n",
    "1. **Add Layers -** `tf.keras.layers.Dense`\n",
    "   ```python\n",
    "   tf.keras.layers.Dense(\n",
    "       units,\n",
    "       activation=None,\n",
    "       use_bias=True,\n",
    "       kernel_initializer='glorot_uniform',\n",
    "       bias_initializer='zeros',\n",
    "       kernel_regularizer=None,\n",
    "       bias_regularizer=None,\n",
    "       activity_regularizer=None,\n",
    "       kernel_constraint=None,\n",
    "       bias_constraint=None\n",
    "   )\n",
    "   ```\n",
    "   - `units`: Number of neurons in the layer.\n",
    "2. **Convolutional Layers -** `Conv2D`, `Conv1D`\n",
    "   ```python\n",
    "       tf.keras.layers.Conv2D(\n",
    "       filters,\n",
    "       kernel_size,\n",
    "       strides=(1, 1),\n",
    "       padding='valid',\n",
    "       activation=None,\n",
    "       use_bias=True,\n",
    "       kernel_initializer='glorot_uniform',\n",
    "       ...\n",
    "   )\n",
    "   ```\n",
    "   - `filters`: Number of filters (output channels).\n",
    "   - `kernel_size`: Size of the convolutional kernel.\n",
    "   - `strides`: Stride of the convolution.\n",
    "   - `padding`: `'valid'` or `'same'`.\n",
    "   - `activation`: Activation function.\n",
    "3. **Pooling Layers -** `MaxPooling2D`, `AveragePooling2D`\n",
    "   ```python\n",
    "   tf.keras.layers.MaxPooling2D(\n",
    "       pool_size=(2, 2),\n",
    "       strides=None,\n",
    "       padding='valid',\n",
    "       ...\n",
    "   )\n",
    "   ```\n",
    "   - `pool_size`: Size of the pooling window.\n",
    "   - `strides`: Strides of the pooling operation.\n",
    "4. **Dropout Layer**\n",
    "   ```python\n",
    "   tf.keras.layers.Dropout(\n",
    "       rate,\n",
    "       noise_shape=None,\n",
    "       seed=None\n",
    "   )\n",
    "   ```\n",
    "   - `rate`: Fraction of input units to drop.\n",
    "   - `noise_shape`: Shape of the noise mask.\n",
    "   - `seed`: Random seed for reproducibility.\n",
    "5. **Flatten Layer**\n",
    "   ```python\n",
    "   tf.keras.layers.Flatten(\n",
    "       data_format=None\n",
    "   )\n",
    "   ```\n",
    "   - `data_format`: `'channels_last'` or `'channels_first'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a64db7-a605-4146-a5b8-66119cfc35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Neural Network with Sequential API\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebd4c1-2c1c-457a-bdc6-54fd2d89c29a",
   "metadata": {},
   "source": [
    "- `Dense Layer`: Fully connected layer. `128` and `64` are the number of neurons.\n",
    "- `activation`: Specifies the activation function.\n",
    "- `input_shape`: Input dimension for the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e61506-7762-4681-a20d-7b01a1dfce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793aa254-d8ca-4bd4-983b-f9e3474f75fc",
   "metadata": {},
   "source": [
    "- `optimizer`: Algorithm for updating weights (e.g., `adam`).\n",
    "- `loss`: Loss function (e.g., `sparse_categorical_crossentropy` for classification).\n",
    "- `metrics`: Metrics to evaluate during training (e.g., `accuracy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3261a0-e5ff-435b-8208-419518d7eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model - Example dataset (e.g., MNIST)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784) / 255.0  # Flatten and normalize\n",
    "x_test = x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5b37a-47bf-47f2-a00c-87e6014d2376",
   "metadata": {},
   "source": [
    "- `fit`: Trains the model.\n",
    "    - `epochs`: Number of iterations over the dataset.\n",
    "    - `batch_size`: Number of samples per gradient update.\n",
    "    - `validation_split`: Fraction of data for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0992034-fdcb-4c9f-be85-5bb599bf0a54",
   "metadata": {},
   "source": [
    "### Functional API\n",
    "The Functional API allows defining models with shared layers, multiple inputs/outputs, or non-linear architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fa305-ad25-4975-b4dd-a2a8bd51cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(784,))\n",
    "x = layers.Dense(128, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a7a4e-109a-4c59-93be-ed7ef4bd2a54",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "## Pretrained models\n",
    "- **MobileNetV2**: Optimized for mobile and edge devices, lightweight with decent accuracy.\n",
    "- **ResNet (Residual Networks)**: Introduces skip connections to train very deep networks effectively.\n",
    "- **VGG16/VGG19**: Known for simplicity and effective performance on ImageNet.\n",
    "- **Inception**: Employs multi-scale convolution filters for robust feature extraction.\n",
    "- **EfficientNet**: Balances accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5429dce-0b8d-4d7d-aaf2-84a37126ee79",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "Recurrent Neural Networks (RNNs) are used to handle sequential data such as time series, text, or speech. TensorFlow provides a high-level API to implement RNNs using `tf.keras.layers.RNN`, `tf.keras.layers.SimpleRNN`, `tf.keras.layers.LSTM`, and `tf.keras.layers.GRU`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
