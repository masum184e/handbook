{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1490d553-0e72-4495-83d7-2e0ec3aaf23a",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- NLP\n",
    "- Regular Expressions\n",
    "- Tokenization\n",
    "- Stemming\n",
    "    - PorterStemmer\n",
    "    - SnowballStemmer\n",
    "    - LancasterStemmer\n",
    "- Lemmatization\n",
    "- Parts Of Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Text Representation\n",
    "    - Bag of Words\n",
    "    - Label Encoding\n",
    "    - One-Hot Encoding\n",
    "- Stop words\n",
    "- Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4385ea0-a993-484b-b00f-f3e48f89be2d",
   "metadata": {},
   "source": [
    "# NLP\n",
    "A field of AI that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret and generate human languages in a way that is both meaningful and useful.\n",
    "\n",
    "## Applications of NLP\n",
    "- Search Engines\n",
    "- Chatbot\n",
    "- Language Translation\n",
    "- Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ef9fa-c982-440d-b9d9-5e3ab80a20b8",
   "metadata": {},
   "source": [
    "# Regular Expressions\n",
    "- `.` - matches any charecter except a newline\n",
    "- `\\w` - matches any word charecter(alphanumaric-equivalent to `[a-zA-Z0-9_]`)\n",
    "- `\\d` - matches any digit(`[0-9])\n",
    "- `\\s` - matches any whitespace character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf4f41a-2d86-4976-bf3a-eefab87282fc",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "It involves splitting text into smaller units, known as tokens. This token can be phrases, sentences or other meaningful units, depending on the granularity of the tokenization.\n",
    "## Types\n",
    "- Word Tokenization\n",
    "- Sentence Tokenization\n",
    "- Subword Tokenization\n",
    "- Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b509cb-ea28-4685-a414-9243a11bd829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', 'was', 'a', 'sunny', 'day']\n",
      "Sentence Tokens: ['The quick brown fox jumps over the lazy dog.', 'It was a sunny day']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "sentence=\"The quick brown fox jumps over the lazy dog. It was a sunny day\"\n",
    "\n",
    "words=word_tokenize(sentence)\n",
    "sentences=sent_tokenize(sentence)\n",
    "print(\"Word Tokens: \",end=\"\")\n",
    "print(words)\n",
    "print(\"Sentence Tokens: \",end=\"\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf831af1-37dc-4eaf-a1f3-280cfd287fd1",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "A text normalization technique used to reduce words to their base/root form. It simplify text data by reducing derived words to a common base form so that they can be analyzed as a single item.\n",
    "\n",
    "Stemming algorithms typically remove common word suffixes(int, ly, ed) to transform a word into its root form.\n",
    "\n",
    "__Example:__ `running` -> `run`, `better` -> `bet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd28487e-c9e0-4854-bde8-2a7ed2caef02",
   "metadata": {},
   "source": [
    "## PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1439f44f-1758-4581-866f-d9aa03b95a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b3899a-3653-4824-9701-0b03faee0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> the\n",
      "quick -> quick\n",
      "brown -> brown\n",
      "fox -> fox\n",
      "jumps -> jump\n",
      "over -> over\n",
      "the -> the\n",
      "lazy -> lazi\n",
      "dog -> dog\n",
      ". -> .\n",
      "It -> it\n",
      "was -> wa\n",
      "a -> a\n",
      "sunny -> sunni\n",
      "day -> day\n"
     ]
    }
   ],
   "source": [
    "porter=PorterStemmer()\n",
    "for word in words:\n",
    "    print(f\"{word} -> {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4423102-d538-4144-8d9c-c379a3c0c421",
   "metadata": {},
   "source": [
    "## SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9576bf-997d-4f59-a2aa-2684cadf4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball=SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "    print(f\"{word}->{snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43410db-306f-462a-9c23-d578bb94fa4f",
   "metadata": {},
   "source": [
    "## LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9c2d7-6dea-4d1d-be47-60ff5b9b0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "for word in words:\n",
    "    print(f\"{word}->{lancaster.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e68160-7516-4942-a046-ecb32798afa0",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "A text normalization technique used to reduce words to their base form but unlike stemming, it considers the context and morphological analysis of words, aiming to reduce words to their meaningful root forms.\n",
    "\n",
    "__Example:__ `running` -> `run`, `better` -> `good`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581f508-f37b-4df8-801d-820c43938960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322a017-5bb9-4650-8a4b-443db616be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eae4e2-4b69-410e-8e05-cfa09949a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    pos = get_wordnet_pos(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, pos)\n",
    "    print(f\"{word}->{lancaster.stem(lemmatized_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa020a0-d4a7-40d7-baeb-6e9b7bc2a0a7",
   "metadata": {},
   "source": [
    "# Parts Of Speech Tagging\n",
    "It involves assigning parts of speech to each word in a sentence or text.\n",
    "## Tags\n",
    "- `NN` - Noun\n",
    "- `VB` - Verb\n",
    "- `JJ` - Adjective\n",
    "- `RB` - Adverb\n",
    "- `PRP` - Pronoun\n",
    "- `IN` - Preposition\n",
    "- `CC` - Conjunction\n",
    "- `DT` - Determiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb452ab-35dc-4346-9021-94e06de3948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "for word in words:\n",
    "    print(f\"{word} -> {pos_tag([word])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bada8-de3e-4270-be91-d1100072c8b0",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "It involves identifying and `classifying` named entities in text into `predefined categories` such as persons, organizations, locations, dates and more.\n",
    "## Categories of Named Entities\n",
    "- `PER` - Person\n",
    "- `ORG` - Oragnization\n",
    "- `LOC` - Location\n",
    "- `DATE/TIME` - Date/Time\n",
    "- `MONEY` - Monetary Values\n",
    "- `PERCENT` - Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786b0f6-830f-40b3-aeb8-85c863aa07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6fa613d-ee57-4b5c-a7c3-a0173f9c4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792b8622-7dbf-4046-8494-a5dcd3cc6725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4576dcef-f9e8-4bc2-8c9f-e7d06c7c9b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "Barack Obama PERSON\n",
      "August 4, 1961 DATE\n",
      "Honolulu GPE\n",
      "Hawaii GPE\n"
     ]
    }
   ],
   "source": [
    "# doc=nlp(sentence)\n",
    "doc=nlp(\"Apple is looking at buying U.K. startup for $1 billion. Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc63d1a-29a4-48b3-9e47-4626b8bca48a",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "It convert textual data into a format that ml alogorithms can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7f479e-b7f4-4a49-8770-19ec9c6ab20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\n",
    "    \"Natural Language Processing is fascinating.\",\n",
    "    \"Text represntation is crucial in NLP.\",\n",
    "    \"Word embeddings are a powerful tool in NLP\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcea968-f0d5-4d87-99b3-1f1f83076de5",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "It represent text as a collection of words without considering the order. It counts the frequency of each word in a document. The result is a vector where each dimension corresponds to a unique word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68c1bab-873b-465d-8ca6-047d9eb00893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37a30762-bbaf-4c4e-b4f2-a46793e6d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "bow_matrix=vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80502e1c-8fd6-4f9e-811f-be3a7edb4311",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "It refers to the set of all unique words found across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d54d29da-3b9f-46e8-b61c-701286ef8f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are' 'crucial' 'embeddings' 'fascinating' 'in' 'is' 'language' 'natural'\n",
      " 'nlp' 'powerful' 'processing' 'represntation' 'text' 'tool' 'word']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd509403-4ee8-449e-9624-8a82a677c0bd",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "The vector has the same length as the number of unique words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6cf1d6-048e-4414-8a40-88ba788bba4c",
   "metadata": {},
   "source": [
    "__First Sentence:__ Natural Language Processing is fascinating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a1ccc-8213-4543-8d81-58dec3d95fcd",
   "metadata": {},
   "source": [
    "- `are`         - not present - 0\n",
    "- `crucial`     - not present - 0\n",
    "- `embeddings`  - not present - 0\n",
    "- `fascinating` - present     - 1\n",
    "- `in`          - not prsent  - 0\n",
    "- `is`          - not present - 0\n",
    "- `language`    - present     - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb6c4f09-c9d7-4e32-b97f-b86312a2f0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 1 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 1 0 0 1 0 0 1 1 0 0]\n",
      " [1 0 1 0 1 0 0 0 1 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482134d-cb5e-48d8-9c94-f64555a8a605",
   "metadata": {},
   "source": [
    "Each word in the sentence appear __one__ time which is why the array appear only 0 and 1, if any word appear multiple time, the number will be shown in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a99b3b0f-9210-474a-8cc1-725a030e1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 13)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498863e1-9674-4d49-9bd9-efa4414ac78d",
   "metadata": {},
   "source": [
    "#### `(0, 7)\t1`\n",
    "- `0` refers to the sentence\n",
    "- `7` refers to the word at index 7 in vocabulary\n",
    "- `1` refers it appears __1__ time in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e27141-cf26-4c23-a081-fb05a02a93d1",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "It convert categorical data into numerical data assigning a unique integer value to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee84bed-4938-4893-bf44-17c33558da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d059e0c3-fcf9-4660-b7f1-4d963bf3e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data = {\n",
    "    \"Color\": [\"Red\", \"Blue\", \"Green\", \"Blue\", \"Red\"],\n",
    "    \"Fruit\": [\"Apple\", \"Banana\", \"Cherry\", \"Date\", \"Elderberry\"],\n",
    "    \"Brand\": [\"Apple\", \"Samsung\", \"Google\", \"OnePlus\", \"Huawei\"],\n",
    "    \"Education\": [\"High School\", \"Associate's Degree\", \"Bachelor's Degree\", \"Master's Degree\", \"PhD\"],\n",
    "    \"Country\": [\"USA\", \"Canada\", \"India\", \"Germany\", \"Australia\"]\n",
    "}\n",
    "df=pd.DataFrame(categorical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3d36f76-be04-49d9-87fc-aebf6de1c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder=LabelEncoder()\n",
    "label_encoded_data={}\n",
    "for column in df.columns:\n",
    "    label_encoded_data[column]=label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "713a8f9d-21f3-4b21-83dd-0b5524737006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color  Fruit  Brand  Education  Country\n",
      "0      2      0      0          2        4\n",
      "1      0      1      4          0        1\n",
      "2      1      2      1          1        3\n",
      "3      0      3      3          3        2\n",
      "4      2      4      2          4        0\n"
     ]
    }
   ],
   "source": [
    "encoded_df = pd.DataFrame(label_encoded_data)\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1744468a-2edd-4ffb-b21d-41313afb6638",
   "metadata": {},
   "source": [
    "Let's take a feature called `Color` with the following values: `['Red', 'Blue', 'Green', 'Blue', 'Red']`.\n",
    "\n",
    "Each unique category in the feature is assigned an integer label. The assignment is typically done alphabetically or in the order in which the categories appear in the data. For the Color feature:\n",
    "- Blue → 0\n",
    "- Green → 1\n",
    "- Red → 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba12bc6-885b-44b3-bab1-b9f2348f46a3",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "It convert categorical data into numerical format.\n",
    "\n",
    "Unlike label encoding, which assigns an integer to each category, one-hot encoding creates a binary column for each category.\n",
    "\n",
    "For each unique category in the feature, one-hot encoding creates a new binary column. Each column corresponds to one category and has a binary value:\n",
    "- If the instance belongs to that category, the column value is 1.\n",
    "- If the instance does not belong to that category, the column value is 0\n",
    "\n",
    "__For Example:__\n",
    "- `Blue`  -> [0, 1, 0, 1, 0]\n",
    "- `Green` -> [0, 0, 1, 0, 0]\n",
    "- `Red`   -> [1, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "313f6889-c55b-4448-a662-c496af054884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Fruit    Brand           Education    Country  Color_Blue  \\\n",
      "0       Apple    Apple         High School        USA       False   \n",
      "1      Banana  Samsung  Associate's Degree     Canada        True   \n",
      "2      Cherry   Google   Bachelor's Degree      India       False   \n",
      "3        Date  OnePlus     Master's Degree    Germany        True   \n",
      "4  Elderberry   Huawei                 PhD  Australia       False   \n",
      "\n",
      "   Color_Green  Color_Red  \n",
      "0        False       True  \n",
      "1        False      False  \n",
      "2         True      False  \n",
      "3        False      False  \n",
      "4        False       True  \n"
     ]
    }
   ],
   "source": [
    "one_hot_encoded_df = pd.get_dummies(df, columns=['Color'])\n",
    "print(one_hot_encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d222a-7f53-4646-a1e1-efc8cfda5da3",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "Stop words are common words in a language that are often filtered out before or after processing natural language data. These words are typically the most frequent and carry little semantic weight, meaning they don't contribute much to the meaning of a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4642184-ab99-4fbe-8443-6fc0fa603164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd44bc30-bae9-47e9-ac16-dfa4317f767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: This is a sample sentence, showing off the stop words filtration.\n",
      "Filtered Sentence: sample sentence , showing stop words filtration .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "words = word_tokenize(sentence)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Filtered Sentence:\", \" \".join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a3b1b-57b0-4e99-a4cb-ee589c1471ef",
   "metadata": {},
   "source": [
    "# Word vectors\n",
    "Word vectors are mathematical representations of words as continuous vectors in a high-dimensional space. These vectors capture the semantic meaning of words by positioning them in such a way that words with similar meanings are close to each other in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bb5c5-0ec9-46c6-94d7-4fde2ebbd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5cd63509-6335-4a2a-b5fe-b60c57c8e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f227037c-418e-41de-b05f-5eff30f1eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data=[]\n",
    "for document in documents:\n",
    "    tokenized_data.append(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c00078c-72f9-466a-8cc7-0a5294eef8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model\n",
    "model = Word2Vec(tokenized_data, vector_size=100, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8440d76e-7c65-4a6e-b9ae-c120d7240ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.6205509e-03  3.6664880e-03  5.1899054e-03  5.7425159e-03\n",
      "  7.4683712e-03 -6.1678356e-03  1.1054418e-03  6.0482449e-03\n",
      " -2.8410859e-03 -6.1737327e-03 -4.1060109e-04 -8.3700540e-03\n",
      " -5.5999835e-03  7.1053654e-03  3.3535317e-03  7.2259018e-03\n",
      "  6.8013361e-03  7.5322888e-03 -3.7902421e-03 -5.6239933e-04\n",
      "  2.3480400e-03 -4.5200605e-03  8.3894283e-03 -9.8599615e-03\n",
      "  6.7658601e-03  2.9142674e-03 -4.9328064e-03  4.3991935e-03\n",
      " -1.7402744e-03  6.7124371e-03  9.9663734e-03 -4.3635760e-03\n",
      " -5.9996103e-04 -5.6965039e-03  3.8506347e-03  2.7867861e-03\n",
      "  6.8917973e-03  6.1020418e-03  9.5395558e-03  9.2743067e-03\n",
      "  7.8984145e-03 -6.9901976e-03 -9.1569778e-03 -3.5512337e-04\n",
      " -3.0998648e-03  7.8954669e-03  5.9394771e-03 -1.5460390e-03\n",
      "  1.5107916e-03  1.7902239e-03  7.8186011e-03 -9.5114801e-03\n",
      " -2.0595833e-04  3.4690076e-03 -9.3987427e-04  8.3821919e-03\n",
      "  9.0115890e-03  6.5369126e-03 -7.1222446e-04  7.7108950e-03\n",
      " -8.5348953e-03  3.2071467e-03 -4.6378332e-03 -5.0894269e-03\n",
      "  3.5893617e-03  5.3717438e-03  7.7711120e-03 -5.7665622e-03\n",
      "  7.4335197e-03  6.6267615e-03 -3.7099777e-03 -8.7460522e-03\n",
      "  5.4384125e-03  6.5107006e-03 -7.8783382e-04 -6.7098294e-03\n",
      " -7.0860516e-03 -2.4968935e-03  5.1436163e-03 -3.6657534e-03\n",
      " -9.3710721e-03  3.8265863e-03  4.8843161e-03 -6.4293337e-03\n",
      "  1.2088331e-03 -2.0755965e-03  2.4012410e-05 -9.8852329e-03\n",
      "  2.6929996e-03 -4.7505689e-03  1.0874966e-03 -1.5760274e-03\n",
      "  2.1975380e-03 -7.8827394e-03 -2.7167653e-03  2.6637891e-03\n",
      "  5.3476626e-03 -2.3917479e-03 -9.5105674e-03  4.5058238e-03]\n"
     ]
    }
   ],
   "source": [
    "word=\"NLP\"\n",
    "word_vector = model.wv[word]\n",
    "\n",
    "print(word_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
