{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f80a8b-ea7b-4574-8964-8298227c4549",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "Support Vector Machine (SVM) used for both classification and regression tasks. It  is particularly well-suited for binary classification tasks and is known for its ability to create a decision boundary (or hyperplane) that maximizes the margin between two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f5d5a9-9d36-41de-ad99-1887a75aab59",
   "metadata": {},
   "source": [
    "SVM is a discriminative classifier that works by finding the `optimal hyperplane` (a line in 2D, a plane in 3D, or a higher-dimensional space) that best separates the data into different classes. It aims to create the `largest possible margin` between the data points of different classes.\n",
    "- __Linear SVM:__ When data is linearly separable, SVM finds a straight line (or hyperplane) that separates the two classes.\n",
    "- __Non-Linear SVM:__ When data is not linearly separable, SVM uses a technique called kernel trick to map the data into a higher-dimensional space where it can be linearly separated.\n",
    "\n",
    "The goal is to find a hyperplane which classify the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c8326-4310-4c13-93b5-ec297aef7650",
   "metadata": {},
   "source": [
    "# Key Concepts\n",
    "__Hyperplane:__ A hyperplane is a decision boundary that separates the data points into different classes. In 2D, this is a line, and in 3D, it is a plane. In higher dimensions, it's a hyperplane.\n",
    "\n",
    "__Support Vectors:__ The data points that are closest to the hyperplane and define the margin are called support vectors. These points are crucial because they directly influence the position and orientation of the hyperplane.\n",
    "\n",
    "__Kernel:__ The kernel is a mathematical function used in SVM to map input data into a higher-dimensional feature space. This allows the SVM to find a hyperplane in cases where data points are not linearly separable in the original space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "## Classification\n",
    "__Margin:__ The margin is the distance between the hyperplane and the closest data points of any class. SVM tries to maximize this margin, aiming to create a wide gap between the classes, making the classifier more robust and less prone to overfitting.\n",
    "## Regression\n",
    "**Epsilon Tube**: SVR introduces an epsilon tube (or margin) around the predicted function. The goal is to find a function that fits within this tube, which allows for some margin of error in the predictions. The width of this tube is determined by the epsilon parameter (ε).\n",
    "\n",
    "**Loss Function**: SVR uses a different loss function called the ε-insensitive loss function. It ignores errors that fall within the epsilon margin. The model is penalized only when predictions fall outside this margin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
