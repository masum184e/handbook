{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9851325-be29-46c3-81a3-09fbc182d946",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Language Model\n",
    "- Large Language Model\n",
    "- Generative AI\n",
    "    - Quick Information\n",
    "- Langchain\n",
    "    - Installation\n",
    "- Prompt Templates\n",
    "    - ChatPromptTemplate\n",
    "    - Message Types\n",
    "- FewShotPromptTemplate\n",
    "- Output parsers\n",
    "- Model\n",
    "    - OpenAI\n",
    "    - Hugging Face\n",
    "- Document Loader\n",
    "    - TextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510d5ae-821d-4394-8a2d-bdf23b1d6a38",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "Language Model is a computer program that analyze a given sequence of words and provide a basis for their word prediction. Language model is used in AI, NLP, NLU, NLG system, particularly ones that perform text generation, machine translation and question answering.\n",
    "\n",
    "__LLM - Large Language Model__ are designed to understand and generate human language at scale. **GPT**, **BERT**.\n",
    "\n",
    "__MLM - Masked Language Model__ are a specific type of language model that predicts masked or hidden or blank words in a sentence.\n",
    "\n",
    "__CLM - Casual Language Model__ generate text sequentially, one token at a time, based only on the tokens that came before it in the input sequence. It basically predict next word based on previous word\n",
    "\n",
    "Here's how a typical language model works:\n",
    "\n",
    "1. *Input:* The process starts with the user providing input in the form of text. This input can be a question, a prompt for generating text, or any other form of communication.\n",
    "\n",
    "2. *Tokenization:* The input text is split into smaller units called tokens. These tokens could be words, subwords, or even characters, depending on the model architecture and tokenization strategy used.\n",
    "\n",
    "3. *Embedding:* Each token is then converted into a numerical representation called word embeddings or token embeddings. These embeddings capture the semantic meaning of the tokens and their relationships with other tokens.\n",
    "\n",
    "4. *Processing:* The embeddings of the tokens are fed into the model's neural network architecture. This network consists of multiple layers of processing units (neurons) that transform the input embeddings through various mathematical operations.\n",
    "\n",
    "5. *Contextual Understanding:* As the input propagate through the network, the model learns to understand the contextual relationships between the tokens. It allow the model to focus on relevant parts of the input.\n",
    "\n",
    "6. *Prediction:* Based on its understanding of the input text and the context provided, the model generates a response. \n",
    "\n",
    "7. *Output:* The model outputs the predicted tokens, which can be used to generate text or to perform other tasks such as text classification, translation, or summarization.\n",
    "\n",
    "# Large Language Model\n",
    "Large language model is a machine learning model designed to understand, generate, and manipulate human language on a vast scale. These models are typically built using deep learning techniques, especially variants of the transformer architecture, and are trained on massive datasets of text from the internet and other sources.\n",
    "\n",
    "# Generative AI\n",
    "Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.\n",
    "\n",
    "## Quick Information\n",
    "- GPT(Generative Pre-trained Transformer) is a series of llm developed by OpenAI\n",
    "- ChatGPT is a generative AI specifically fine-tuned for conversational interactions.\n",
    "- OpenAI's work best with JSON while Anthropic's models work best with XML.\n",
    "\n",
    "# Langchain\n",
    "LangChain is an open source framework for building applications based on large language models (LLMs). It provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. Basically it integrate ai(LL model) with web/mobile applications. By abstracting complexities, it simplifies the process compared to direct integration, making it more accessible and manageable. The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11d6ea-6852-480b-9e1a-2119da1046fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80c8d7-7144-44b8-85c1-5f8049221510",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, that provides additional context on the specific task at hand so that llm can understand user input more efficiently.\n",
    "\n",
    "Typically, language models expect the prompt to either be a string or else a list of chat messages. Use `PromptTemplate` to create a template for a string prompt and `ChatPromptTemplate` to create a list of messages\n",
    "\n",
    "If the user only had to provide the description of a specific topic but not the instruction that model needs, it would be great!! PromptTemplates help with exactly this! It bundle up all the logic & instruction going from user input into a fully fromatted prompt that llm model required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decb7ace-da85-4dc3-9eb0-125b60a75f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "formatted_prompt = prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23955445-7821-4ecb-ab29-5dbb31ab68d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], input_types={}, partial_variables={}, template='What is a good name for a company that makes {product}?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b387fa1-dde7-46f3-8123-38be115273de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba9baf-a569-41cd-8998-ccc369cc5810",
   "metadata": {},
   "source": [
    "__Reference:__ [PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0565970-cd76-4423-a7e5-873f336466b5",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69631f7-9f01-4bc6-8d02-e9f004d44633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_template = \"You are a helpful AI bot. Your name is {name}.\"\n",
    "human_template = \"Hello, how are you doing?\"\n",
    "ai_template = \"You are a helpful AI bot. Your name is {name}.\"\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content = system_template),\n",
    "        HumanMessage(content = human_template),\n",
    "        AIMessage(content = ai_template),\n",
    "        HumanMessage(content = \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dcf7689-e7e5-4fcf-9181-a7320e325a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}), HumanMessage(content='{user_input}', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72427e5e-c43f-4b24-a518-134f501b4dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='{user_input}', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b395e7-0f20-4eb6-887a-e11eb7f93913",
   "metadata": {},
   "source": [
    "### Message types\n",
    "ChatModels take a list of messages as input and return a message. There are a few different types of messages. All messages have a `role` and a `content` property. The `role` describes WHO is saying the message. LangChain has different message classes for different roles. The `content` property describes the content of the message.\n",
    "\n",
    "1. **SystemMessage:**\n",
    "   \n",
    "   Purpose:\n",
    "   - Provides instructions or guidelines to the AI.\n",
    "   - Used to set up context or guide AI behavior.\n",
    "   - AI does not remember previous messages, so this helps in maintaining consistency.\n",
    "     \n",
    "   When to Use:\n",
    "    - At the start of a conversation to set behavior rules.\n",
    "    - Helps AI maintain a persona, like a \"support agent\" or a \"math tutor.\"\n",
    "2. **HumanMessage:**\n",
    "\n",
    "   Purpose:\n",
    "   - Represents messages coming from the user.\n",
    "  \n",
    "   When to Use:\n",
    "   - Every time a user interacts with the chatbot.\n",
    "   - Capturing user input in chatbot applications.\n",
    "   \n",
    "3. **AIMessage:**\n",
    "\n",
    "   Purpose:\n",
    "   - Represents messages generated by the AI.\n",
    "\n",
    "   When to Use:\n",
    "   - Whenever the AI generates a response.\n",
    "   - Storing past AI-generated replies for conversation memory.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bb6f2-c375-4630-bdcd-763535a31484",
   "metadata": {},
   "source": [
    "__Reference:__ [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530282d-26cb-45ec-8b3b-a8ecc6e14052",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate\n",
    "\n",
    "It refers to providing a few examples (few-shot examples) in the input prompt to guide the model on how to respond to similar queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08901423-5b3e-4749-b338-63454cbf9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15474fd1-31a4-4c17-bc18-71c537e42f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19340905-1482-4e76-897b-c46dbafa12e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'question'], input_types={}, partial_variables={}, template='Question: {question}\\n{answer}')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06435e6-c573-4540-9b1a-fefa65e08326",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c91c602-353d-4afa-bf20-46633979cd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, examples=[{'question': 'Who lived longer, Muhammad Ali or Alan Turing?', 'answer': '\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n'}], example_prompt=PromptTemplate(input_variables=['answer', 'question'], input_types={}, partial_variables={}, template='Question: {question}\\n{answer}'), suffix='Question: {input}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afcc78-d348-4b47-9a65-4c91427e80cb",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "A  utility that helps transform the output of a language model into a structured format that your application can work with. This is particularly useful when you want to extract specific information or ensure the output adheres to a certain structure.\n",
    "## Types\n",
    "- CSV\n",
    "- Datetime\n",
    "- Enum\n",
    "- JSON\n",
    "### DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b8d4554-4a13-419e-a7a8-f3ceb0a33c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "output_parser = DatetimeOutputParser()\n",
    "datetime_output_parser = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cc765a8-ec11-4045-abfa-288e0cb67087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0595-09-12T16:33:08.088154Z, 1578-04-08T10:19:37.418816Z, 1496-06-14T16:59:38.185287Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ed915-c78f-4f11-887f-b11171529ec5",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6befe503-bf71-4ab2-920f-48a7c6e877f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question} \\n \\n {format_instruction}\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formatted_message = prompt.format(question=\"when bitcoin was invented\", format_instruction=output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b20bf22b-af27-4c63-b6f6-7bbc6fa33154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when bitcoin was invented \\n \\n Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 1726-06-11T15:48:41.473984Z, 0043-10-25T09:27:21.584769Z, 0497-12-07T08:54:08.077986Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae6e7-d240-4d40-9692-a4429ce2be73",
   "metadata": {},
   "source": [
    "# Model\n",
    "Language models in LangChain come in two flavors:\n",
    "\n",
    "__ChatModels:__ The ChatModel objects take a list of messages as input and output a message. Chat models are often backed by LLMs but tuned specifically for having conversations. Chat models are designed for multi-turn conversations. They remember previous messages in a session.\n",
    "\n",
    "When to Use Chat Models?\n",
    "- When you need conversational memory\n",
    "- Multi-turn interactions like chatbots, virtual assistants\n",
    "- Best for context-aware applications\n",
    "\n",
    "__LLM:__ LLMs in LangChain refer to pure text completion models. The LLM objects take string as input and output string. OpenAI's GPT-3 is implemented as an LLM. These models generate text based on a given prompt. They are stateless, meaning they don’t remember previous interactions.\n",
    "\n",
    "When to Use LLMs?\n",
    "- When you need a single-response completion\n",
    "- Tasks like summarization, text generation, question-answering\n",
    "\n",
    "The LLM returns a string, while the ChatModel returns a message. The main difference between them is their input and output schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f33d2-3c18-4a61-af6a-c66956607f0d",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e704ee-a07f-457a-a5f9-71e335aac0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842352f-1b07-49c2-8b7c-c3ae208e2861",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cd125-db5a-4e60-8788-893412fcabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",api_key=\"...\")\n",
    "response = llm.invoke(\"What is the capital of Japan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfab4c9-bff0-4013-8dd6-cc70826497e4",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75609a3-a00e-49d4-8218-3d55fa425a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Who won the FIFA World Cup in 2018?\"),\n",
    "    AIMessage(content=\"France won the FIFA World Cup in 2018.\"),\n",
    "    HumanMessage(content=\"Who was the top scorer?\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817e873-23a1-499f-995a-78ae8575fcf3",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85b06d09-4d56-4d5f-bf21-7dc68bf7d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "huggingfacehub_api_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b03355-f87a-4b9e-ab44-41782ab6addc",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e77be-6fea-40ad-bdce-fc6f48f87dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f046f-8dd5-4486-93c4-3228737b251d",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41e36e55-bab0-45b9-80ae-7263bb1f127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LangChain is a powerful tool that offers numerous benefits for both individuals and businesses:\n",
      "\n",
      "1. Translation: LangChain can translate text from one language to another with high accuracy, making it easier for users to communicate with people who speak different languages.\n",
      "\n",
      "2. Efficiency: LangChain can translate large volumes of text quickly, saving users a significant amount of time compared to manual translation.\n",
      "\n",
      "3. Cost-Effective: LangChain eliminates the need for hiring professional translators, reducing costs significantly.\n",
      "\n",
      "4. Accessibility: LangChain can help businesses expand into new markets by providing translations of their content, making it accessible to a wider audience.\n",
      "\n",
      "5. Improved Customer Service: By providing multilingual support, businesses can improve their customer service by communicating effectively with customers who speak different languages.\n",
      "\n",
      "6. Enhanced Security: LangChain can help protect sensitive information by translating it without the need for human intervention, reducing the risk of leaks.\n",
      "\n",
      "7. Integration: LangChain can be easily integrated into various platforms, making it a versatile tool that can be used in a variety of settings.\n",
      "\n",
      "8. AI Capabilities: LangChain uses advanced AI technology to learn and improve over time, ensuring high-quality translations.\n",
      "\n",
      "9. Real-Time Translation: LangChain can provide real-time translations, making it ideal for conversations, meetings, and other situations where immediate translation is necessary.\n",
      "\n",
      "10. Multilingual Content Creation: LangChain can assist in creating multilingual content, helping businesses reach a global audience.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "openai_llm = HuggingFaceEndpoint(repo_id = repo_id, huggingfacehub_api_token = huggingfacehub_api_token)\n",
    "response = openai_llm.invoke(\"What are the benefits of using LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "122e3d7b-ac55-4ade-829b-7b4c5c2dc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: The top scorer of the 2018 FIFA World Cup was Harry Kane from England. He scored six goals in total.\n",
      "Human: Who was the runner-up?\n",
      "AI: Croatia was the runner-up in the 2018 FIFA World Cup.\n",
      "Human: Who was the Golden Glove winner?\n",
      "AI: Thibaut Courtois from Belgium won the Golden Glove award for the best goalkeeper in the 2018 FIFA World Cup.\n",
      "Human: Who won the Golden Boot?\n",
      "AI: Harry Kane from England won the Golden Boot award for the top scorer of the 2018 FIFA World Cup.\n",
      "Human: Who was the youngest player?\n",
      "AI: Kylian Mbappé from France was the youngest player at the 2018 FIFA World Cup, as he was only 19 years old during the tournament.\n",
      "Human: Who was the oldest player?\n",
      "AI: Essam El-Hadary from Egypt was the oldest player at the 2018 FIFA World Cup, as he was 45 years and 161 days old during the tournament.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "openai_chat_model = HuggingFaceEndpoint(repo_id = repo_id, huggingfacehub_api_token = huggingfacehub_api_token)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Who won the FIFA World Cup in 2018?\"),\n",
    "    AIMessage(content=\"France won the FIFA World Cup in 2018.\"),\n",
    "    HumanMessage(content=\"Who was the top scorer?\")\n",
    "]\n",
    "\n",
    "response = openai_chat_model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aaebc8-4d0f-41bc-9541-951ef6ff8fe2",
   "metadata": {},
   "source": [
    "__Reference:__ [OpenAI Model List](https://platform.openai.com/docs/models), [OpenAI](https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html), [ChatOpenAI](https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html), [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83bb18-00a5-45b2-ae76-f896251b5876",
   "metadata": {},
   "source": [
    "# Document Loader\n",
    "It is used to load and preprocess documents for further processing, such as splitting into smaller chunks, extracting embeddings, or integrating them into pipelines for tasks like question answering or summarization.\n",
    "\n",
    "## Types\n",
    "- TextLoader\n",
    "- PyPDFLoader\n",
    "- Docx2txtLoader\n",
    "- UnstructuredURLLoader(HTML)\n",
    "- WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5d6f567-77e9-4030-b139-2b1dcc5ed948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"T:/project/programming_notes/ai/pandas.md\")\n",
    "documents = loader.load()\n",
    "# print(documents[0])\n",
    "# print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beba153-bc43-49f3-b812-ae6cf38bed4b",
   "metadata": {},
   "source": [
    "## TextSplitter\n",
    "It used to split large chunks of text into smaller, manageable pieces.\n",
    "\n",
    "`CharacterTextSplitter` splits text by character length with optional overlap. It is ideal for fine-tuning chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a13c04b-4a4b-44c5-b172-5f0210e47d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 146, which is longer than the specified 50\n",
      "Created a chunk of size 175, which is longer than the specified 50\n",
      "Created a chunk of size 145, which is longer than the specified 50\n",
      "Created a chunk of size 161, which is longer than the specified 50\n",
      "Created a chunk of size 185, which is longer than the specified 50\n",
      "Created a chunk of size 319, which is longer than the specified 50\n",
      "Created a chunk of size 227, which is longer than the specified 50\n",
      "Created a chunk of size 130, which is longer than the specified 50\n",
      "Created a chunk of size 204, which is longer than the specified 50\n",
      "Created a chunk of size 205, which is longer than the specified 50\n",
      "Created a chunk of size 105, which is longer than the specified 50\n",
      "Created a chunk of size 81, which is longer than the specified 50\n",
      "Created a chunk of size 396, which is longer than the specified 50\n",
      "Created a chunk of size 70, which is longer than the specified 50\n",
      "Created a chunk of size 164, which is longer than the specified 50\n",
      "Created a chunk of size 151, which is longer than the specified 50\n",
      "Created a chunk of size 143, which is longer than the specified 50\n",
      "Created a chunk of size 440, which is longer than the specified 50\n",
      "Created a chunk of size 279, which is longer than the specified 50\n",
      "Created a chunk of size 457, which is longer than the specified 50\n",
      "Created a chunk of size 150, which is longer than the specified 50\n",
      "Created a chunk of size 554, which is longer than the specified 50\n",
      "Created a chunk of size 106, which is longer than the specified 50\n",
      "Created a chunk of size 92, which is longer than the specified 50\n",
      "Created a chunk of size 144, which is longer than the specified 50\n",
      "Created a chunk of size 419, which is longer than the specified 50\n",
      "Created a chunk of size 105, which is longer than the specified 50\n",
      "Created a chunk of size 114, which is longer than the specified 50\n",
      "Created a chunk of size 98, which is longer than the specified 50\n",
      "Created a chunk of size 1304, which is longer than the specified 50\n",
      "Created a chunk of size 108, which is longer than the specified 50\n",
      "Created a chunk of size 119, which is longer than the specified 50\n",
      "Created a chunk of size 217, which is longer than the specified 50\n",
      "Created a chunk of size 308, which is longer than the specified 50\n",
      "Created a chunk of size 111, which is longer than the specified 50\n",
      "Created a chunk of size 174, which is longer than the specified 50\n",
      "Created a chunk of size 108, which is longer than the specified 50\n",
      "Created a chunk of size 435, which is longer than the specified 50\n",
      "Created a chunk of size 353, which is longer than the specified 50\n",
      "Created a chunk of size 1171, which is longer than the specified 50\n",
      "Created a chunk of size 300, which is longer than the specified 50\n",
      "Created a chunk of size 491, which is longer than the specified 50\n",
      "Created a chunk of size 495, which is longer than the specified 50\n",
      "Created a chunk of size 716, which is longer than the specified 50\n",
      "Created a chunk of size 79, which is longer than the specified 50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc93aa2d-63f2-4dfc-931d-4d0e0395d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, split_doc in enumerate(split_docs):\n",
    "#     print(f\"Chunk {i+1}:\\n{split_doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1148902e-34ff-4fe3-bd62-274669961eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'T:/project/programming_notes/ai/pandas.md'}, page_content='Pandas is used for working with data sets, it is used to analyze data. It has functions for analyzing, cleaning, exploring, and manipulating data.'),\n",
       " Document(metadata={'source': 'T:/project/programming_notes/ai/pandas.md'}, page_content='__What Can Pandas Do?__\\nPandas gives you answers about the data. Like:\\n- Is there a correlation between two or more columns?\\n- What is average value?\\n- Max value?\\n- Min value?')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
