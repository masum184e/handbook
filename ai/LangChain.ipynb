{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaeeabcb-c25b-4ec3-b67b-dc041ff79e55",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "Language Model is a computer program that analyze a given sequence of words and provide a basis for their word prediction. Language model is used in AI, NLP, NLU, NLG system, particularly ones that perform text generation, machine translation and question answering.\n",
    "\n",
    "__LLM - Large Language Model__ are are designed to understand and generate human language at scale. **GPT**, **BERT**.\n",
    "\n",
    "__MLM - Masked Language Model__ are a specific type of language model that predicts masked or hidden or blank words in a sentence.\n",
    "\n",
    "__CLM - Casual Language Model__ generate text sequentially, one token at a time, based only on the tokens that came before it in the input sequence. It basically predict next word based on previous word\n",
    "\n",
    "Here's how a typical language model works:\n",
    "\n",
    "1. *Input:* The process starts with the user providing input in the form of text. This input can be a question, a prompt for generating text, or any other form of communication.\n",
    "\n",
    "2. *Tokenization:* The input text is split into smaller units called tokens. These tokens could be words, subwords, or even characters, depending on the model architecture and tokenization strategy used.\n",
    "\n",
    "3. *Embedding:* Each token is then converted into a numerical representation called word embeddings or token embeddings. These embeddings capture the semantic meaning of the tokens and their relationships with other tokens.\n",
    "\n",
    "4. *Processing:* The embeddings of the tokens are fed into the model's neural network architecture. This network consists of multiple layers of processing units (neurons) that transform the input embeddings through various mathematical operations.\n",
    "\n",
    "5. *Contextual Understanding:* As the input propagate through the network, the model learns to understand the contextual relationships between the tokens. It allow the model to focus on relevant parts of the input.\n",
    "\n",
    "6. *Prediction:* Based on its understanding of the input text and the context provided, the model generates a response. \n",
    "\n",
    "7. *Output:* The model outputs the predicted tokens, which can be used to generate text or to perform other tasks such as text classification, translation, or summarization.\n",
    "\n",
    "# Large Language Model\n",
    "Large language model is a machine learning model designed to understand, generate, and manipulate human language on a vast scale. These models are typically built using deep learning techniques, especially variants of the transformer architecture, and are trained on massive datasets of text from the internet and other sources.\n",
    "\n",
    "# Generative AI\n",
    "Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.\n",
    "\n",
    "## Quick Information\n",
    "- GPT(Generative Pre-trained Transformer) is a series of llm developed by OpenAI\n",
    "- ChatGPT is a generative AI specifically fine-tuned for conversational interactions.\n",
    "- OpenAI's work best with JSON while Anthropic's models work best with XML.\n",
    "\n",
    "# Langchain\n",
    "LangChain is an open source framework for building applications based on large language models (LLMs). It provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. Basically it integrate ai(LLm model) with web/mobile applications. By abstracting complexities, it simplifies the process compared to direct integration, making it more accessible and manageable. The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e2339-c533-4c01-bff6-d8764c50a90d",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6018cd1-1df1-44d4-a75a-3798e0f758b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed616e-3c49-4d49-8526-41199f900cc4",
   "metadata": {},
   "source": [
    "# Model\n",
    "Language models in LangChain come in two flavors:\n",
    "\n",
    "__ChatModels:__ The ChatModel objects take a list of messages as input and output a message. Chat models are often backed by LLMs but tuned specifically for having conversations. \n",
    "\n",
    "__LLM:__ LLMs in LangChain refer to pure text completion models. The LLM objects take string as input and output string. OpenAI's GPT-3 is implemented as an LLM.\n",
    "\n",
    "The LLM returns a string, while the ChatModel returns a message. The main difference between them is their input and output schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced32722-3843-4573-8bd6-e692f12eaa04",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dda141-2ad8-45a6-95b5-f335418bc2c0",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5461e-7a4e-4b2e-ab34-1fdbdd682487",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd6d1b-6212-465f-b433-6508296839bf",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741969d1-8acf-4ef2-96ae-f43bcde3c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "huggingfacehub_api_token = \"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\"\n",
    "llm = HuggingFaceEndpoint(repo_id = repo_id, huggingfacehub_api_token = huggingfacehub_api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabfdc99-5575-46f5-ae96-cf68cf0d2262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: \n",
      "\n",
      "1. SockPop\n",
      "2. Vibrant Soles\n",
      "3. Rainbow Ripples\n",
      "4. Kaleidosock\n",
      "5. SockStudio\n",
      "6. HueHop\n",
      "7. SockSplash\n",
      "8. PaintSock\n",
      "9. Colorful Crew\n",
      "10. SockVibe\n",
      "11. SockPulse\n",
      "12. SockFusion\n",
      "13. SockSpark\n",
      "14. SockSplash\n",
      "15. SockBurst\n",
      "16. SockWave\n",
      "17. SockBreeze\n",
      "18. SockGlow\n",
      "19. SockWave\n",
      "20. SockRipple\n",
      "21. SockSplash\n",
      "22. SockPulse\n",
      "23. SockSpark\n",
      "24. SockSplash\n",
      "25. SockBurst\n",
      "26. SockWave\n",
      "27. SockBreeze\n",
      "28. SockGlow\n",
      "29. SockFusion\n",
      "30. SockVibe\n",
      "31. SockStitch\n",
      "32. SockDazzle\n",
      "33. SockFizz\n",
      "34. SockWave\n",
      "35. SockRipple\n",
      "36. SockSplash\n",
      "37. SockPulse\n",
      "38. SockSpark\n",
      "39. SockSplash\n",
      "40. SockBurst\n",
      "41. SockWave\n",
      "42. SockBreeze\n",
      "43. SockGlow\n",
      "44. SockFusion\n",
      "45. SockVibe\n",
      "46. SockStitch\n",
      "47. SockDazzle\n",
      "48. SockFizz\n",
      "49. SockWave\n",
      "50. SockRipple\n",
      "51. SockSplash\n",
      "52. SockPulse\n",
      "53. SockSpark\n",
      "54. SockSplash\n",
      "55. SockBurst\n",
      "56. SockWave\n",
      "57. SockBreeze\n",
      "58. SockGlow\n",
      "59. SockFusion\n",
      "60. SockVibe\n",
      "61. SockStitch\n",
      "62. SockDazzle\n",
      "63. S\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(\"LLM Response: \"+llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5586b8-4715-4abc-9352-91a5998eff7d",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def52e83-533d-4424-a8fc-40c4dd75751c",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8feb49-4c9d-434e-b944-15300e89e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9093e-7171-48dc-a319-8d142b001c95",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8f23b-b212-466f-8342-0bcb4bd6ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",api_key=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52794a17-9fe2-4cc2-a815-dfd49c41b0ed",
   "metadata": {},
   "source": [
    "__Reference:__ [OpenAI Model List](https://platform.openai.com/docs/models), [OpenAI](https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html), [ChatOpenAI](https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html), [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1716cad-bfda-4e1c-81f3-5cf67d513bc3",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5fe1a-7401-442e-8fb3-2a9b3798e885",
   "metadata": {},
   "source": [
    "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, that provides additional context on the specific task at hand so that llm can understand user input more efficiently.\n",
    "\n",
    "Typically, language models expect the prompt to either be a string or else a list of chat messages. Use `PromptTemplate` to create a template for a string prompt and `ChatPromptTemplate` to create a list of messages\n",
    "\n",
    "If the user only had to provide the description of a specific topic but not the instruction that model needs, it would be great!! PromptTemplates help with exactly this! It bundle up all the logic & instruction going from user input into a fully fromatted prompt that llm model required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6c605d-7d38-42a7-a9ce-cefc023eda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "formatted_prompt = prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d60e737-94b6-43f9-8017-a8f171a73e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], input_types={}, partial_variables={}, template='What is a good name for a company that makes {product}?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20380cc-2cc6-4a13-a4ba-fee37af4a6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8afb3d6f-aba2-4eb8-820b-919b12a01d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Vibrant Sole\n",
      "2. Colorful Comfort Co.\n",
      "3. Socktacular\n",
      "4. Rainbow Sock Co.\n",
      "5. Chromatic Feet\n",
      "6. Happy Socks Co.\n",
      "7. Bold & Bright Socks\n",
      "8. Sock Chameleon\n",
      "9. Spectrum Socks\n",
      "10. HueHue Socks\n",
      "11. Kaleidosock Co.\n",
      "12. Jazzy Socks\n",
      "13. PopSock Co.\n",
      "14. Fancy Feet Socks\n",
      "15. Pixel Socks\n",
      "16. Sockarina Socks\n",
      "17. Funky Feet Socks\n",
      "18. Vivid Vibe Socks\n",
      "19. Whimsical Socks Co.\n",
      "20. Groovy Socks Co.\n",
      "21. Trendy Feet Co.\n",
      "22. Sock Society\n",
      "23. Sock Revolution\n",
      "24. Sock Style Co.\n",
      "25. Sock Swag Co.\n",
      "26. Sock Fusion Co.\n",
      "27. Sock Pop Co.\n",
      "28. Sock Dynamo\n",
      "29. Sock Explosion Co.\n",
      "30. Sock Wave Co.\n",
      "31. Sock Surge Co.\n",
      "32. Sock Spark Co.\n",
      "33. Sock Wave Co.\n",
      "34. Sock Ripple Co.\n",
      "35. Sock Flow Co.\n",
      "36. Sock Pulse Co.\n",
      "37. Sock Beat Co.\n",
      "38. Sock Groove Co.\n",
      "39. Sock Vibe Co.\n",
      "40. Sock Trend Co.\n",
      "41. Sock Pulse Co.\n",
      "42. Sock Fusion Co.\n",
      "43. Sock Wave Co.\n",
      "44. Sock Splash Co.\n",
      "45. Sock Dazzle Co.\n",
      "46. Sock Splash Co.\n",
      "47. Sock Blast Co.\n",
      "48. Sock Dynamo Co.\n",
      "49. Sock Surge Co.\n",
      "50. Sock Wave Co.\n",
      "51. Sock Ripple Co.\n",
      "52. Sock Flow Co.\n",
      "53. Sock Pulse Co.\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(formatted_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd124287-c2e1-44a1-b317-9e35cb6f4363",
   "metadata": {},
   "source": [
    "__Reference:__ [PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e2a88-a321-4cf9-b451-fd265f62fc52",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "681a403a-c1f9-4baf-aadd-066b93e53cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c917dab0-8bae-4c5c-867b-7eb7623a02a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8f612c-2197-4a24-99a4-9e9db2d094e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: My name is Bob.\n",
      "Human: Nice to meet you, Bob. I have a question. What is the capital of Australia?\n",
      "AI: The capital of Australia is Canberra.\n",
      "Human: Thank you. I was just studying for a test.\n",
      "AI: You're welcome! If you have any more questions or need help with anything else, just let me know.\n",
      "Human: I have another question. What is the population of Australia?\n",
      "AI: As of 2021, the population of Australia is approximately 25.6 million people.\n",
      "Human: Thank you again. I really appreciate your help.\n",
      "AI: You're welcome! I'm here to help. If you have any more questions, don't hesitate to ask. Have a great day!\n",
      "Human: You too, Bob. Have a great day!\n",
      "AI: You too! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(formatted_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef1b49-bf4c-496a-baf1-ca7241c621b2",
   "metadata": {},
   "source": [
    "__Reference:__ [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bb299-3ae4-4211-a440-98f5d95e6491",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "A  utility that helps transform the output of a language model into a structured format that your application can work with. This is particularly useful when you want to extract specific information or ensure the output adheres to a certain structure.\n",
    "## Types\n",
    "- CSV\n",
    "- Datetime\n",
    "- Enum\n",
    "- JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a08ad-3d09-4d3c-b876-c5447f01688c",
   "metadata": {},
   "source": [
    "### DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82965bf7-0230-45b0-83bf-a26e52cd956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 1542-07-11T02:52:26.823624Z, 0704-09-05T00:17:25.772335Z, 1081-04-08T14:45:43.675335Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "output_parser = DatetimeOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033b698-ca18-4737-8592-abeda5eab302",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81163c9-4fa1-4654-9911-1cad29e2b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question} \\n \\n {format_instruction}\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formatted_message = prompt.format(question=\"when bitcoin was invented\", format_instruction=output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53b40ad-8c87-4f8f-888a-59721199a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when bitcoin was invented \\n \\n Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0157-10-23T18:42:53.895165Z, 0806-04-02T10:13:00.506428Z, 0745-01-29T09:47:43.888432Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1f0e7e-e217-4a9d-b6de-bb19c52a0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0157-10-23T18:42:53.895165Z\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The datetime string is in the ISO 8601 extended format with microseconds and nanoseconds. The pattern used to create this string is:\n",
      "\n",
      "Y - year (four digits)\n",
      "- - separator\n",
      "m - month (zero-padded with a leading zero if less than 10)\n",
      "- - separator\n",
      "d - day of the month (zero-padded with a leading zero if less than 10)\n",
      "T - separator\n",
      "H - hour (24-hour clock, zero-padded with a leading zero if less than 10)\n",
      ": - separator\n",
      "M - minute (zero-padded with a leading zero if less than 10)\n",
      ": - separator\n",
      "S - second (zero-padded with a leading zero if less than 10)\n",
      ". - separator\n",
      "f - microsecond (up to six digits)\n",
      "Z - time zone offset (Z represents UTC or Zulu time)\n",
      "\n",
      "In the case of the provided examples, the year is in the range of 2015 to 2074, so we can assume the year 2015 as the starting point for our datetime string. To create a datetime string for bitcoin's invention in 2008, we would adjust the year, month, and day accordingly.\n",
      "\n",
      "First, let's find the date of bitcoin's invention:\n",
      "\n",
      "https://en.bitcoin.it/wiki/History_of_bitcoin\n",
      "\n",
      "\"On 31 October 2008, a paper was published under the pseudonym Satoshi Nakamoto, describing the Bitcoin digital currency.\"\n",
      "\n",
      "So, the date of bitcoin's invention is October 31, 2008.\n",
      "\n",
      "Now, let's create the datetime string:\n",
      "\n",
      "Y = 2008\n",
      "m = 10 (October)\n",
      "d = 31\n",
      "H = 0 (midnight)\n",
      "M = 0\n",
      "S = 0\n",
      "f = 000000 (random microseconds)\n",
      "Z = Zulu time (Z)\n",
      "\n",
      "Putting it all together:\n",
      "\n",
      "2008\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(formatted_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c5ca3-c2e0-43a3-a19d-4bbd41acf7d6",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate\n",
    "\n",
    "It refers to providing a few examples (few-shot examples) in the input prompt to guide the model on how to respond to similar queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "072f0b7d-3f34-40ce-b668-3d901119de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b67c8a76-03ee-4e2d-8a8d-fbd79683b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e557a06-f77b-43d5-8c8a-367c655e5662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['answer', 'question'] input_types={} partial_variables={} template='Question: {question}\\n{answer}'\n"
     ]
    }
   ],
   "source": [
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb5aadb4-107d-483f-b030-51200addfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f223d6d4-a365-49dc-87a2-3c94dd03a936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9672a2bc-6548-4f5b-9e2e-992508abb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_message = prompt.format(input=\"Who was the father of Mary Ball Washington?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc5e4232-ff5c-4781-a167-f8eaf479d7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "print(formatted_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b7a7c47-f60e-412d-aa5b-40768f5fd9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Intermediate answer: Mary Ball Washington's father was Augustine Washington.\n",
      "\n",
      "Question: Who was the father of George Washington?\n",
      "\n",
      "Intermediate answer: George Washington's father was Augustine Washington.\n",
      "\n",
      "Question: So who was the father of Mary Ball Washington and George Washington?\n",
      "\n",
      "Intermediate answer: Both Mary Ball Washington and George Washington had the same father, Augustine Washington.\n",
      "\n",
      "Question: Who was the father of both Mary Ball Washington and George Washington?\n",
      "\n",
      "Final answer: Augustine Washington was the father of both Mary Ball Washington and George Washington.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(formatted_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc328b-82a3-4792-bac1-41c8af6bf317",
   "metadata": {},
   "source": [
    "# Document Loader\n",
    "It is used to load and preprocess documents for further processing, such as splitting into smaller chunks, extracting embeddings, or integrating them into pipelines for tasks like question answering or summarization.\n",
    "\n",
    "## Types\n",
    "- TextLoader\n",
    "- PyPDFLoader\n",
    "- Docx2txtLoader\n",
    "- UnstructuredURLLoader(HTML)\n",
    "- WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e24ec496-2ce0-47c1-a73a-1a3840dbced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'pandas.md'}, page_content='Pandas is used for working with data sets, it is used to analyze data. It has functions for analyzing, cleaning, exploring, and manipulating data.\\n\\n__What Can Pandas Do?__\\nPandas gives you answers about the data. Like:\\n- Is there a correlation between two or more columns?\\n- What is average value?\\n- Max value?\\n- Min value?\\n\\nPandas are also able to delete rows that are not relevant, or contains wrong values, like empty or NULL values. This is called cleaning the data.\\n\\n- `pandas.__version__` - return the version of pandas \\n- `pandas.DataFrame()` - \\n- `pd.options.display.max_rows` - return or set the system maximum number of row\\n\\n# Series\\nA Pandas Series is like a column in a table. \\n```\\nname=[\"Jack\",\"John\",\"Mark\",\"Zuck\"]\\nstudent_table = pd.Series(name)\\n```\\nThe above script generate name column in student table.\\n\\nyou can access each column value by their index.\\n\\nyou can also overried the index label with your custom label by `index` argument.\\n```\\nstudent_table = pd.Series(name,index = [\"a\", \"b\", \"c\",\"d\"])\\n```\\nnow you can access it both by number index as well as you custom index.\\n```\\nprint(student_table[\"a\"])\\nprint(student_table.iloc[0]) # student_table[0] will be removed\\n```\\n\\nif you use dictornary instead of list, the keys of the dictionary become the labels\\n```\\nname={\"a\":\"Jack\",\"b\":\"John\",\"c\":\"Mark\",\"d\":\"Zuck\"}\\n```\\nif label is set `index` argument is used to get specific item, if not, it set label.\\n\\nThe length of data and the length of `index` argument should be same. The data used in __series__ or __dataframe__ can be anytype.\\n\\n# DataFrame\\nA Pandas DataFrame is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns.\\n```\\ndata = {\\n  \"calories\": [420, 380, 390],\\n  \"duration\": [50, 40, 45]\\n}\\n```\\n\\n`loc` attribute return one or more specified row\\n```\\nprint(df.loc[0]) # return first row\\nprint(df.loc[[0,2]]) # return specified index - 0th, 2nd\\nprint(df.loc[0:2]) # return list of indexes from 0 to 2\\n```\\n\\nyou can override the label in dataframe also\\n```\\ndf = pd.DataFrame(data, index = [\"a\", \"b\", \"c\",\"d\"])\\n```\\n\\n# File Reading\\n## CSV\\n`pandas.read_csv()` - read csv file and store in data frame\\n\\nIf you have a large DataFrame with many rows, Pandas will only return the first 5 rows, and the last 5 rows\\n```\\ndf = pd.read_csv(\\'data.csv\\')\\nprint(df)\\n```\\n`dataframe.toString()` - return entire dataframe as string representation, it will output the entire DataFrame to the console.\\n```\\ndf = pd.read_csv(\\'data.csv\\')\\nprint(df.to_string())\\n```\\n`dataframe.to_csv(filename)` - use to generate csv file\\n\\n## JSON \\n`pandas.read_json()` - read json file and store in data frame\\n\\nJSON objects have the same format as Python dictionaries. If your JSON code is not in a file, but in a Python Dictionary, you can load it into a DataFrame directly:\\n\\n# Data Cleaning\\nData cleaning means fixing bad data in your data set. Bad data could be:\\n- Empty cells\\n- Data in wrong format\\n- Wrong data\\n- Duplicates\\n\\n- `isna()` - detect missing values and return boolean datafram\\n- `isna().sum()` - detect and count the number of missing values for each column\\n\\n## Remove Rows\\n`df.dropna()` - return a new Data Frame with no empty cells\\n```\\ndf = pd.read_csv(\\'data.csv\\')\\nnew_df = df.dropna()\\nprint(new_df.to_string())\\n```\\nIf you want to change the original DataFrame, use the `inplace = True` argument:\\n```\\ndf = pd.read_csv(\\'data.csv\\')\\ndf.dropna(inplace = True)\\nprint(df.to_string())\\n```\\nIt will NOT return a new DataFrame, but it will remove all rows containing NULL values from the original DataFrame.\\n\\nif you want to remove row, based on any specified column, specify it with subset attribute. Like if Date column have null value you want remove the row, then use it\\n```\\ndf.dropna(subset=[\\'Date\\'], inplace = True)\\n```\\nif any other column have null value, those will not be removed.\\n\\n## Replace Empty Values\\n`fillna()` method allows to replace empty cells with a value.\\n```\\ndf.fillna(130, inplace = True)\\n```\\nIt will replace all empty cells in the whole data frame. If you want to replace empty value on specified column, then specify them.\\n```\\ndf[\"Calories\"].fillna(130, inplace = True)\\n```\\n__For multiple column:__\\n```\\nfill_values = {\\n    \"Calories\": 130,\\n    \"Protein\": 0.0,\\n    \"Fat\": 0.0\\n}\\ndf.fillna(value=fill_values, inplace=True)\\n```\\n\\n## Replacing with Mean, Median, Mode\\n```\\nx = df[\"Calories\"].mean() # you can also use median(), mode()[0]\\ndf[\"Calories\"].fillna(x, inplace = True)\\n```\\n\\n## Forward & Backward Filling\\nForward filling fills missing values with the last known value that appeared before the NaN. This is useful when you assume that the previous value is the best estimate for the missing value. Backward filling fills missing values with the next known value that appears after the NaN. This is useful when you assume that the next value is the best estimate for the missing value.. It is useful in time series data or other sequential data.\\n```\\ndf_ffilled = df.fillna(method=\\'ffill\\')\\ndf_bfilled = df.fillna(method=\\'bfill\\')\\n```\\n\\n# Wrong Format\\nCells with data of wrong format can make it difficult, or even impossible, to analyze data.\\n\\n- `to_datetime()` is used to format the date\\n```\\ndf[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'])\\n```\\n\\n# Wrong Data\\nWrong data does not have to be empty cells or wrong format, it can just be wrong, like if someone registered \"199\" instead of 1.99.\\n\\n## Replacing Value\\nFor small data sets you might be able to replace the wrong data one by one, but not for big data set\\n```\\ndf.loc[7, \\'Duration\\'] = 45\\n```\\nTo replace wrong data for larger data sets you can create some rules, e.g. set some boundaries for legal values, and replace any values that are outside of the boundaries.\\n```\\nfor x in df.index:\\n  if df.loc[x, \"Duration\"] > 120:\\n    df.loc[x, \"Duration\"] = 120\\n```\\n\\n## Remove Row\\n```\\nfor x in df.index:\\n  if df.loc[x, \"Duration\"] > 120:\\n    df.drop(x, inplace = True)\\n```\\n\\n# Removing Duplicates\\n- `duplicated()` method returns a Boolean values for each row\\n```\\nprint(df.duplicated())\\n```\\n\\n- `drop_duplicates()` is used to remove duplicate value\\n```\\ndf.drop_duplicates(inplace = True)\\n```\\n\\n# Inspecting Data\\n- `info()` - provide summary\\n- `head()` - return the first nth rows\\n- `tail()` - return the last nth rows\\n- `shape` - return a tuple represent the dimension\\n- `describe()` - generates descriptive statistics like max, min, mean, std etc for numerical columns by default.\\n- `columns` - returns an Index object containing the column labels \\n- `index` - return the index(row label) range\\n- `dtypes` - return data type of each column\\n- `isnull()` - return boolean value indicating whether each value is NaN\\n- `notnull()` - return boolean value indicating whether each value is not NaN\\n- `sum()`,`mean()`,`median()`,`std()`,`min()`,`max()` - apply on each column and return value according to their name\\n- `value.counts()` - return a series containing counts of unique values for a given column. Ex: `df[\\'column_name\\'].value_counts()`, if you want to consider `NaN` as unique value put `dropna=False` argument.\\n- `sample()` - return random samples, you can specify the number of row with n attribute\\n- `corr()` - computes pairwise correlation of columns, excluding NA/null values.\\n- `nunique()` - returns the number of unique values for each column.\\n- `df[\\'column_name\\'].str.string_method()` - perform string related operation like `len()`, `upper()`, `lower()`, `substr()`, `replace()` etc.t\\n\\nall the method ignore `NaN` value while calculating, `skipna=False` argument is use to consider `NaN` value.\\n\\n__Perform Aggregations:__\\n```\\ncustom_agg=df..agg({\\n  \"A\": [\"sum\", \"mean\"],\\n  \"B\": [\"min\", \"max\"],\\n  \"C\": \"count\"\\n})\\n```\\n\\n# Column Selection\\n- `df.columnName` or `df[\"column name\"]` return specified column\\n- `df[[\"first\",\"second\"]]` select multiple column\\n- `df[\\'column_name\\'].apply(func)` - applies a function to each value in the column.\\n\\nyou can apply all the inspection method to column\\n\\n## Column Insertion\\n1. __Assigning a New Column Directly:__ You can directly assign a new column by specifying the column name and the values. If the column name already exists, this will overwrite the existing column.\\n```\\ndf = pd.DataFrame({\\n    \"A\": [1, 2, 3],\\n    \"B\": [4, 5, 6]\\n})\\ndf[\"C\"] = [7, 8, 9]\\n```\\n\\n2. __insert() method:__ allow to insert a column at a specific location\\n```\\ndf.insert(1, \"D\", [10, 11, 12])\\n```\\n\\n3. __assign() method:__ return new dataframe with additional column. it does not modify the original dataframe uunless you reassign it.\\n```\\ndf = df.assign(E=[13, 14, 15])\\n```\\n\\n4. __Index Based Assignment:__ you can use `loc()` or `iloc()` method.\\n```\\ndf.loc[:, \"F\"] = [16, 17, 18]\\n```\\n\\n## Column Updation\\n1. __Direct Assignment:__\\n```\\ndf = pd.DataFrame({\\n    \"A\": [1, 2, 3],\\n    \"B\": [4, 5, 6]\\n})\\ndf[\"A\"] = [10, 20, 30]\\n```\\n- `df[\"A\"] = 10` - update all value of a column\\n- `df.loc[df[\\'A\\'] == 2, \\'B\\'] = 10` - update specific field of a column\\n2. __replace():__\\n```\\ndf[\"A\"] = df[\"A\"].replace({10: 100, 99: 999})\\n```\\n3. __update():__\\n```\\nupdate_series = pd.Series([200, 300], index=[0, 2])\\ndf[\"A\"].update(update_series)\\n```\\n\\n## Column Deletion\\n1. __drop():__\\n```\\ndf.drop(columns=[\\'B\\'], inplace=True)\\n```\\n2. __del:__\\n```\\ndel df[\\'B\\']\\n```\\n3. __pop():__\\n```\\ndroppedCol = df.pop(\\'B\\')\\n```\\n4. __dropna():__ `axis=1` parameter in `dropna()` method is use to remove column which have empty value, `axis=0` is use to remove row which have empty value. Default value of axis parameter is 0\\n\\n# Feature Scaling\\nFeature Scaling is a technique to standardize the independent features present in the data in a fixed range. \\n### Standardization (Z-score normalization)\\nStandardization scales the features such that they have a mean of 0 and a standard deviation of 1. This method is useful when the features have different units or vastly different ranges. It is used when the data follows a normal distribution\\n```\\nscaler = StandardScaler()\\ndf_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\\n```\\n### Min-Max Scaling (Normalization)\\nMin-Max scaling scales the features to a fixed range, usually 0 to 1. This method is useful when the data is not normally distributed or the model requires the data in a specific range like neural networks\\n```\\nscaler = MinMaxScaler()\\ndf_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\\n```\\n### Robust Scaling\\nRobust scaling uses the median and the interquartile range (IQR) for scaling, making it robust to outliers. This method is useful when the data contains many outliers.\\n```\\nscaler = RobustScaler()\\ndf_robust_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\\n```\\n\\n### MaxAbs Scaling\\nMaxAbs scaling scales each feature by its maximum absolute value, preserving the sparsity of the data. This method is useful for data that is sparse (contains many zeros).\\n```\\nscaler = MaxAbsScaler()\\ndf_maxabs_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\\n```\\n\\n# Filtering\\n1. __Boolean Indexing:__\\n```\\nfiltered_df = df[df[\\'A\\'] > 2] # Filter rows where column __A__ is greater than 2\\n```\\n2. __query():__\\n```\\nfiltered_df = df.query(\\'A > 2\\')\\n```\\n3. __Combining Multiple Conditions:__\\n```\\nfiltered_df = df[(df[\\'A\\'] > 2) & (df[\\'C\\'] == \\'foo\\')]\\n```\\n4. __loc[]:__\\n```\\ndf.loc[df[\\'A\\'] > 2]\\n```\\n5. __isin():__ filter rows based on multiple specific values in a column.\\n```\\nfiltered_df = df[df[\\'A\\'].isin([2,4])] # retur data, df[\\'A\\'].isin([2,4]) return boolean\\n```\\n\\n# Transformation\\n1. __apply():__\\nIt allows to apply a function along the axis (rows or columns).\\n```\\ndf_sum = df.apply(lambda row: row.sum(), axis=1) # apply on row\\ndf_sum = df.apply(lambda col: col.sum(), axis=0) # apply on column\\n```\\n2. __applymap():__\\nIt allows to apply a function to each element.\\n```\\ndf_incremented = df.applymap(lambda x: x + 1)\\n```\\n3. __map():__\\nIt allows to apply a function to each element of a Series.\\n```\\ndf[\\'A\\'] = df[\\'A\\'].map(lambda x: x * 2)\\n```\\n4. __transform():__\\n\\n# Merging\\n### Inner Join\\nonly includes rows with matching keys in both DataFrames\\n```\\nmerged_inner = pd.merge(df1, df2, on=\\'id\\')\\n```\\n### Left Join\\nincludes all rows from the left DataFrame and matched rows from the right DataFrame\\n```\\nmerged_left = pd.merge(df1, df2, on=\\'id\\', how=\\'left\\')\\n```\\n### Right Join\\nincludes all rows from the right DataFrame and matched rows from the left DataFrame\\n```\\nmerged_right = pd.merge(df1, df2, on=\\'id\\', how=\\'right\\')\\n```\\n### Outer Join\\nincludes all rows when there is a match in one of the DataFrames\\n```\\nmerged_outer = pd.merge(df1, df2, on=\\'id\\', how=\\'outer\\')\\n```\\nif you have common column name, specify it with `on` attribute, if not, then use `left_on` and `right_on` attribute \\n\\n`joined = df1.join(df2, how=\\'inner\\')` is used to join dataframes on their index\\n\\n# Concatenating\\n### Rows\\n```\\nconcat_rows = pd.concat([df1, df2])\\n```\\n### Columns\\n```\\nconcat_cols = pd.concat([df1, df3], axis=1)\\n```')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"pandas.md\")\n",
    "documents = loader.load()\n",
    "print(documents)\n",
    "# print(documents[0])\n",
    "# print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19a101-4d50-4710-bbc7-051736d1970f",
   "metadata": {},
   "source": [
    "## TextSplitter\n",
    "It used to split large chunks of text into smaller, manageable pieces.\n",
    "\n",
    "`CharacterTextSplitter` splits text by character length with optional overlap. It is ideal for fine-tuning chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40850ef1-6306-48ae-b49f-e249339d11da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 146, which is longer than the specified 50\n",
      "Created a chunk of size 175, which is longer than the specified 50\n",
      "Created a chunk of size 145, which is longer than the specified 50\n",
      "Created a chunk of size 161, which is longer than the specified 50\n",
      "Created a chunk of size 185, which is longer than the specified 50\n",
      "Created a chunk of size 319, which is longer than the specified 50\n",
      "Created a chunk of size 227, which is longer than the specified 50\n",
      "Created a chunk of size 130, which is longer than the specified 50\n",
      "Created a chunk of size 204, which is longer than the specified 50\n",
      "Created a chunk of size 205, which is longer than the specified 50\n",
      "Created a chunk of size 105, which is longer than the specified 50\n",
      "Created a chunk of size 81, which is longer than the specified 50\n",
      "Created a chunk of size 396, which is longer than the specified 50\n",
      "Created a chunk of size 70, which is longer than the specified 50\n",
      "Created a chunk of size 164, which is longer than the specified 50\n",
      "Created a chunk of size 151, which is longer than the specified 50\n",
      "Created a chunk of size 143, which is longer than the specified 50\n",
      "Created a chunk of size 440, which is longer than the specified 50\n",
      "Created a chunk of size 279, which is longer than the specified 50\n",
      "Created a chunk of size 457, which is longer than the specified 50\n",
      "Created a chunk of size 150, which is longer than the specified 50\n",
      "Created a chunk of size 554, which is longer than the specified 50\n",
      "Created a chunk of size 106, which is longer than the specified 50\n",
      "Created a chunk of size 92, which is longer than the specified 50\n",
      "Created a chunk of size 144, which is longer than the specified 50\n",
      "Created a chunk of size 419, which is longer than the specified 50\n",
      "Created a chunk of size 105, which is longer than the specified 50\n",
      "Created a chunk of size 114, which is longer than the specified 50\n",
      "Created a chunk of size 98, which is longer than the specified 50\n",
      "Created a chunk of size 1304, which is longer than the specified 50\n",
      "Created a chunk of size 108, which is longer than the specified 50\n",
      "Created a chunk of size 119, which is longer than the specified 50\n",
      "Created a chunk of size 217, which is longer than the specified 50\n",
      "Created a chunk of size 308, which is longer than the specified 50\n",
      "Created a chunk of size 111, which is longer than the specified 50\n",
      "Created a chunk of size 174, which is longer than the specified 50\n",
      "Created a chunk of size 108, which is longer than the specified 50\n",
      "Created a chunk of size 435, which is longer than the specified 50\n",
      "Created a chunk of size 353, which is longer than the specified 50\n",
      "Created a chunk of size 1171, which is longer than the specified 50\n",
      "Created a chunk of size 300, which is longer than the specified 50\n",
      "Created a chunk of size 491, which is longer than the specified 50\n",
      "Created a chunk of size 495, which is longer than the specified 50\n",
      "Created a chunk of size 716, which is longer than the specified 50\n",
      "Created a chunk of size 79, which is longer than the specified 50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8ee592b-225b-4d76-8f36-67318f91ce9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Pandas is used for working with data sets, it is used to analyze data. It has functions for analyzing, cleaning, exploring, and manipulating data.\n",
      "\n",
      "Chunk 2:\n",
      "__What Can Pandas Do?__\n",
      "Pandas gives you answers about the data. Like:\n",
      "- Is there a correlation between two or more columns?\n",
      "- What is average value?\n",
      "- Max value?\n",
      "- Min value?\n",
      "\n",
      "Chunk 3:\n",
      "Pandas are also able to delete rows that are not relevant, or contains wrong values, like empty or NULL values. This is called cleaning the data.\n",
      "\n",
      "Chunk 4:\n",
      "- `pandas.__version__` - return the version of pandas \n",
      "- `pandas.DataFrame()` - \n",
      "- `pd.options.display.max_rows` - return or set the system maximum number of row\n",
      "\n",
      "Chunk 5:\n",
      "# Series\n",
      "A Pandas Series is like a column in a table. \n",
      "```\n",
      "name=[\"Jack\",\"John\",\"Mark\",\"Zuck\"]\n",
      "student_table = pd.Series(name)\n",
      "```\n",
      "The above script generate name column in student table.\n",
      "\n",
      "Chunk 6:\n",
      "you can access each column value by their index.\n",
      "\n",
      "Chunk 7:\n",
      "you can also overried the index label with your custom label by `index` argument.\n",
      "```\n",
      "student_table = pd.Series(name,index = [\"a\", \"b\", \"c\",\"d\"])\n",
      "```\n",
      "now you can access it both by number index as well as you custom index.\n",
      "```\n",
      "print(student_table[\"a\"])\n",
      "print(student_table.iloc[0]) # student_table[0] will be removed\n",
      "```\n",
      "\n",
      "Chunk 8:\n",
      "if you use dictornary instead of list, the keys of the dictionary become the labels\n",
      "```\n",
      "name={\"a\":\"Jack\",\"b\":\"John\",\"c\":\"Mark\",\"d\":\"Zuck\"}\n",
      "```\n",
      "if label is set `index` argument is used to get specific item, if not, it set label.\n",
      "\n",
      "Chunk 9:\n",
      "The length of data and the length of `index` argument should be same. The data used in __series__ or __dataframe__ can be anytype.\n",
      "\n",
      "Chunk 10:\n",
      "# DataFrame\n",
      "A Pandas DataFrame is a 2 dimensional data structure, like a 2 dimensional array, or a table with rows and columns.\n",
      "```\n",
      "data = {\n",
      "  \"calories\": [420, 380, 390],\n",
      "  \"duration\": [50, 40, 45]\n",
      "}\n",
      "```\n",
      "\n",
      "Chunk 11:\n",
      "`loc` attribute return one or more specified row\n",
      "```\n",
      "print(df.loc[0]) # return first row\n",
      "print(df.loc[[0,2]]) # return specified index - 0th, 2nd\n",
      "print(df.loc[0:2]) # return list of indexes from 0 to 2\n",
      "```\n",
      "\n",
      "Chunk 12:\n",
      "you can override the label in dataframe also\n",
      "```\n",
      "df = pd.DataFrame(data, index = [\"a\", \"b\", \"c\",\"d\"])\n",
      "```\n",
      "\n",
      "Chunk 13:\n",
      "# File Reading\n",
      "## CSV\n",
      "`pandas.read_csv()` - read csv file and store in data frame\n",
      "\n",
      "Chunk 14:\n",
      "If you have a large DataFrame with many rows, Pandas will only return the first 5 rows, and the last 5 rows\n",
      "```\n",
      "df = pd.read_csv('data.csv')\n",
      "print(df)\n",
      "```\n",
      "`dataframe.toString()` - return entire dataframe as string representation, it will output the entire DataFrame to the console.\n",
      "```\n",
      "df = pd.read_csv('data.csv')\n",
      "print(df.to_string())\n",
      "```\n",
      "`dataframe.to_csv(filename)` - use to generate csv file\n",
      "\n",
      "Chunk 15:\n",
      "## JSON \n",
      "`pandas.read_json()` - read json file and store in data frame\n",
      "\n",
      "Chunk 16:\n",
      "JSON objects have the same format as Python dictionaries. If your JSON code is not in a file, but in a Python Dictionary, you can load it into a DataFrame directly:\n",
      "\n",
      "Chunk 17:\n",
      "# Data Cleaning\n",
      "Data cleaning means fixing bad data in your data set. Bad data could be:\n",
      "- Empty cells\n",
      "- Data in wrong format\n",
      "- Wrong data\n",
      "- Duplicates\n",
      "\n",
      "Chunk 18:\n",
      "- `isna()` - detect missing values and return boolean datafram\n",
      "- `isna().sum()` - detect and count the number of missing values for each column\n",
      "\n",
      "Chunk 19:\n",
      "## Remove Rows\n",
      "`df.dropna()` - return a new Data Frame with no empty cells\n",
      "```\n",
      "df = pd.read_csv('data.csv')\n",
      "new_df = df.dropna()\n",
      "print(new_df.to_string())\n",
      "```\n",
      "If you want to change the original DataFrame, use the `inplace = True` argument:\n",
      "```\n",
      "df = pd.read_csv('data.csv')\n",
      "df.dropna(inplace = True)\n",
      "print(df.to_string())\n",
      "```\n",
      "It will NOT return a new DataFrame, but it will remove all rows containing NULL values from the original DataFrame.\n",
      "\n",
      "Chunk 20:\n",
      "if you want to remove row, based on any specified column, specify it with subset attribute. Like if Date column have null value you want remove the row, then use it\n",
      "```\n",
      "df.dropna(subset=['Date'], inplace = True)\n",
      "```\n",
      "if any other column have null value, those will not be removed.\n",
      "\n",
      "Chunk 21:\n",
      "## Replace Empty Values\n",
      "`fillna()` method allows to replace empty cells with a value.\n",
      "```\n",
      "df.fillna(130, inplace = True)\n",
      "```\n",
      "It will replace all empty cells in the whole data frame. If you want to replace empty value on specified column, then specify them.\n",
      "```\n",
      "df[\"Calories\"].fillna(130, inplace = True)\n",
      "```\n",
      "__For multiple column:__\n",
      "```\n",
      "fill_values = {\n",
      "    \"Calories\": 130,\n",
      "    \"Protein\": 0.0,\n",
      "    \"Fat\": 0.0\n",
      "}\n",
      "df.fillna(value=fill_values, inplace=True)\n",
      "```\n",
      "\n",
      "Chunk 22:\n",
      "## Replacing with Mean, Median, Mode\n",
      "```\n",
      "x = df[\"Calories\"].mean() # you can also use median(), mode()[0]\n",
      "df[\"Calories\"].fillna(x, inplace = True)\n",
      "```\n",
      "\n",
      "Chunk 23:\n",
      "## Forward & Backward Filling\n",
      "Forward filling fills missing values with the last known value that appeared before the NaN. This is useful when you assume that the previous value is the best estimate for the missing value. Backward filling fills missing values with the next known value that appears after the NaN. This is useful when you assume that the next value is the best estimate for the missing value.. It is useful in time series data or other sequential data.\n",
      "```\n",
      "df_ffilled = df.fillna(method='ffill')\n",
      "df_bfilled = df.fillna(method='bfill')\n",
      "```\n",
      "\n",
      "Chunk 24:\n",
      "# Wrong Format\n",
      "Cells with data of wrong format can make it difficult, or even impossible, to analyze data.\n",
      "\n",
      "Chunk 25:\n",
      "- `to_datetime()` is used to format the date\n",
      "```\n",
      "df['Date'] = pd.to_datetime(df['Date'])\n",
      "```\n",
      "\n",
      "Chunk 26:\n",
      "# Wrong Data\n",
      "Wrong data does not have to be empty cells or wrong format, it can just be wrong, like if someone registered \"199\" instead of 1.99.\n",
      "\n",
      "Chunk 27:\n",
      "## Replacing Value\n",
      "For small data sets you might be able to replace the wrong data one by one, but not for big data set\n",
      "```\n",
      "df.loc[7, 'Duration'] = 45\n",
      "```\n",
      "To replace wrong data for larger data sets you can create some rules, e.g. set some boundaries for legal values, and replace any values that are outside of the boundaries.\n",
      "```\n",
      "for x in df.index:\n",
      "  if df.loc[x, \"Duration\"] > 120:\n",
      "    df.loc[x, \"Duration\"] = 120\n",
      "```\n",
      "\n",
      "Chunk 28:\n",
      "## Remove Row\n",
      "```\n",
      "for x in df.index:\n",
      "  if df.loc[x, \"Duration\"] > 120:\n",
      "    df.drop(x, inplace = True)\n",
      "```\n",
      "\n",
      "Chunk 29:\n",
      "# Removing Duplicates\n",
      "- `duplicated()` method returns a Boolean values for each row\n",
      "```\n",
      "print(df.duplicated())\n",
      "```\n",
      "\n",
      "Chunk 30:\n",
      "- `drop_duplicates()` is used to remove duplicate value\n",
      "```\n",
      "df.drop_duplicates(inplace = True)\n",
      "```\n",
      "\n",
      "Chunk 31:\n",
      "# Inspecting Data\n",
      "- `info()` - provide summary\n",
      "- `head()` - return the first nth rows\n",
      "- `tail()` - return the last nth rows\n",
      "- `shape` - return a tuple represent the dimension\n",
      "- `describe()` - generates descriptive statistics like max, min, mean, std etc for numerical columns by default.\n",
      "- `columns` - returns an Index object containing the column labels \n",
      "- `index` - return the index(row label) range\n",
      "- `dtypes` - return data type of each column\n",
      "- `isnull()` - return boolean value indicating whether each value is NaN\n",
      "- `notnull()` - return boolean value indicating whether each value is not NaN\n",
      "- `sum()`,`mean()`,`median()`,`std()`,`min()`,`max()` - apply on each column and return value according to their name\n",
      "- `value.counts()` - return a series containing counts of unique values for a given column. Ex: `df['column_name'].value_counts()`, if you want to consider `NaN` as unique value put `dropna=False` argument.\n",
      "- `sample()` - return random samples, you can specify the number of row with n attribute\n",
      "- `corr()` - computes pairwise correlation of columns, excluding NA/null values.\n",
      "- `nunique()` - returns the number of unique values for each column.\n",
      "- `df['column_name'].str.string_method()` - perform string related operation like `len()`, `upper()`, `lower()`, `substr()`, `replace()` etc.t\n",
      "\n",
      "Chunk 32:\n",
      "all the method ignore `NaN` value while calculating, `skipna=False` argument is use to consider `NaN` value.\n",
      "\n",
      "Chunk 33:\n",
      "__Perform Aggregations:__\n",
      "```\n",
      "custom_agg=df..agg({\n",
      "  \"A\": [\"sum\", \"mean\"],\n",
      "  \"B\": [\"min\", \"max\"],\n",
      "  \"C\": \"count\"\n",
      "})\n",
      "```\n",
      "\n",
      "Chunk 34:\n",
      "# Column Selection\n",
      "- `df.columnName` or `df[\"column name\"]` return specified column\n",
      "- `df[[\"first\",\"second\"]]` select multiple column\n",
      "- `df['column_name'].apply(func)` - applies a function to each value in the column.\n",
      "\n",
      "Chunk 35:\n",
      "you can apply all the inspection method to column\n",
      "\n",
      "Chunk 36:\n",
      "## Column Insertion\n",
      "1. __Assigning a New Column Directly:__ You can directly assign a new column by specifying the column name and the values. If the column name already exists, this will overwrite the existing column.\n",
      "```\n",
      "df = pd.DataFrame({\n",
      "    \"A\": [1, 2, 3],\n",
      "    \"B\": [4, 5, 6]\n",
      "})\n",
      "df[\"C\"] = [7, 8, 9]\n",
      "```\n",
      "\n",
      "Chunk 37:\n",
      "2. __insert() method:__ allow to insert a column at a specific location\n",
      "```\n",
      "df.insert(1, \"D\", [10, 11, 12])\n",
      "```\n",
      "\n",
      "Chunk 38:\n",
      "3. __assign() method:__ return new dataframe with additional column. it does not modify the original dataframe uunless you reassign it.\n",
      "```\n",
      "df = df.assign(E=[13, 14, 15])\n",
      "```\n",
      "\n",
      "Chunk 39:\n",
      "4. __Index Based Assignment:__ you can use `loc()` or `iloc()` method.\n",
      "```\n",
      "df.loc[:, \"F\"] = [16, 17, 18]\n",
      "```\n",
      "\n",
      "Chunk 40:\n",
      "## Column Updation\n",
      "1. __Direct Assignment:__\n",
      "```\n",
      "df = pd.DataFrame({\n",
      "    \"A\": [1, 2, 3],\n",
      "    \"B\": [4, 5, 6]\n",
      "})\n",
      "df[\"A\"] = [10, 20, 30]\n",
      "```\n",
      "- `df[\"A\"] = 10` - update all value of a column\n",
      "- `df.loc[df['A'] == 2, 'B'] = 10` - update specific field of a column\n",
      "2. __replace():__\n",
      "```\n",
      "df[\"A\"] = df[\"A\"].replace({10: 100, 99: 999})\n",
      "```\n",
      "3. __update():__\n",
      "```\n",
      "update_series = pd.Series([200, 300], index=[0, 2])\n",
      "df[\"A\"].update(update_series)\n",
      "```\n",
      "\n",
      "Chunk 41:\n",
      "## Column Deletion\n",
      "1. __drop():__\n",
      "```\n",
      "df.drop(columns=['B'], inplace=True)\n",
      "```\n",
      "2. __del:__\n",
      "```\n",
      "del df['B']\n",
      "```\n",
      "3. __pop():__\n",
      "```\n",
      "droppedCol = df.pop('B')\n",
      "```\n",
      "4. __dropna():__ `axis=1` parameter in `dropna()` method is use to remove column which have empty value, `axis=0` is use to remove row which have empty value. Default value of axis parameter is 0\n",
      "\n",
      "Chunk 42:\n",
      "# Feature Scaling\n",
      "Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. \n",
      "### Standardization (Z-score normalization)\n",
      "Standardization scales the features such that they have a mean of 0 and a standard deviation of 1. This method is useful when the features have different units or vastly different ranges. It is used when the data follows a normal distribution\n",
      "```\n",
      "scaler = StandardScaler()\n",
      "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "```\n",
      "### Min-Max Scaling (Normalization)\n",
      "Min-Max scaling scales the features to a fixed range, usually 0 to 1. This method is useful when the data is not normally distributed or the model requires the data in a specific range like neural networks\n",
      "```\n",
      "scaler = MinMaxScaler()\n",
      "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "```\n",
      "### Robust Scaling\n",
      "Robust scaling uses the median and the interquartile range (IQR) for scaling, making it robust to outliers. This method is useful when the data contains many outliers.\n",
      "```\n",
      "scaler = RobustScaler()\n",
      "df_robust_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "```\n",
      "\n",
      "Chunk 43:\n",
      "### MaxAbs Scaling\n",
      "MaxAbs scaling scales each feature by its maximum absolute value, preserving the sparsity of the data. This method is useful for data that is sparse (contains many zeros).\n",
      "```\n",
      "scaler = MaxAbsScaler()\n",
      "df_maxabs_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
      "```\n",
      "\n",
      "Chunk 44:\n",
      "# Filtering\n",
      "1. __Boolean Indexing:__\n",
      "```\n",
      "filtered_df = df[df['A'] > 2] # Filter rows where column __A__ is greater than 2\n",
      "```\n",
      "2. __query():__\n",
      "```\n",
      "filtered_df = df.query('A > 2')\n",
      "```\n",
      "3. __Combining Multiple Conditions:__\n",
      "```\n",
      "filtered_df = df[(df['A'] > 2) & (df['C'] == 'foo')]\n",
      "```\n",
      "4. __loc[]:__\n",
      "```\n",
      "df.loc[df['A'] > 2]\n",
      "```\n",
      "5. __isin():__ filter rows based on multiple specific values in a column.\n",
      "```\n",
      "filtered_df = df[df['A'].isin([2,4])] # retur data, df['A'].isin([2,4]) return boolean\n",
      "```\n",
      "\n",
      "Chunk 45:\n",
      "# Transformation\n",
      "1. __apply():__\n",
      "It allows to apply a function along the axis (rows or columns).\n",
      "```\n",
      "df_sum = df.apply(lambda row: row.sum(), axis=1) # apply on row\n",
      "df_sum = df.apply(lambda col: col.sum(), axis=0) # apply on column\n",
      "```\n",
      "2. __applymap():__\n",
      "It allows to apply a function to each element.\n",
      "```\n",
      "df_incremented = df.applymap(lambda x: x + 1)\n",
      "```\n",
      "3. __map():__\n",
      "It allows to apply a function to each element of a Series.\n",
      "```\n",
      "df['A'] = df['A'].map(lambda x: x * 2)\n",
      "```\n",
      "4. __transform():__\n",
      "\n",
      "Chunk 46:\n",
      "# Merging\n",
      "### Inner Join\n",
      "only includes rows with matching keys in both DataFrames\n",
      "```\n",
      "merged_inner = pd.merge(df1, df2, on='id')\n",
      "```\n",
      "### Left Join\n",
      "includes all rows from the left DataFrame and matched rows from the right DataFrame\n",
      "```\n",
      "merged_left = pd.merge(df1, df2, on='id', how='left')\n",
      "```\n",
      "### Right Join\n",
      "includes all rows from the right DataFrame and matched rows from the left DataFrame\n",
      "```\n",
      "merged_right = pd.merge(df1, df2, on='id', how='right')\n",
      "```\n",
      "### Outer Join\n",
      "includes all rows when there is a match in one of the DataFrames\n",
      "```\n",
      "merged_outer = pd.merge(df1, df2, on='id', how='outer')\n",
      "```\n",
      "if you have common column name, specify it with `on` attribute, if not, then use `left_on` and `right_on` attribute\n",
      "\n",
      "Chunk 47:\n",
      "`joined = df1.join(df2, how='inner')` is used to join dataframes on their index\n",
      "\n",
      "Chunk 48:\n",
      "# Concatenating\n",
      "### Rows\n",
      "```\n",
      "concat_rows = pd.concat([df1, df2])\n",
      "```\n",
      "### Columns\n",
      "```\n",
      "concat_cols = pd.concat([df1, df3], axis=1)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, split_doc in enumerate(split_docs):\n",
    "    print(f\"Chunk {i+1}:\\n{split_doc.page_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
