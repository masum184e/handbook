{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9851325-be29-46c3-81a3-09fbc182d946",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Language Model\n",
    "- Large Language Model\n",
    "- Generative AI\n",
    "    - Quick Information\n",
    "- Langchain\n",
    "    - Installation\n",
    "- Prompt Templates\n",
    "    - ChatPromptTemplate\n",
    "    - Message Types\n",
    "- FewShotPromptTemplate\n",
    "- Output parsers\n",
    "- Model\n",
    "    - OpenAI\n",
    "    - Hugging Face\n",
    "- Document Loader\n",
    "    - TextSplitter\n",
    "- Chains\n",
    "- Memory\n",
    "    - ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510d5ae-821d-4394-8a2d-bdf23b1d6a38",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "Language Model is a computer program that analyze a given sequence of words and provide a basis for their word prediction. Language model is used in AI, NLP, NLU, NLG system, particularly ones that perform text generation, machine translation and question answering.\n",
    "\n",
    "__LLM - Large Language Model__ are designed to understand and generate human language at scale. **GPT**, **BERT**.\n",
    "\n",
    "__MLM - Masked Language Model__ are a specific type of language model that predicts masked or hidden or blank words in a sentence.\n",
    "\n",
    "__CLM - Casual Language Model__ generate text sequentially, one token at a time, based only on the tokens that came before it in the input sequence. It basically predict next word based on previous word\n",
    "\n",
    "Here's how a typical language model works:\n",
    "\n",
    "1. *Input:* The process starts with the user providing input in the form of text. This input can be a question, a prompt for generating text, or any other form of communication.\n",
    "\n",
    "2. *Tokenization:* The input text is split into smaller units called tokens. These tokens could be words, subwords, or even characters, depending on the model architecture and tokenization strategy used.\n",
    "\n",
    "3. *Embedding:* Each token is then converted into a numerical representation called word embeddings or token embeddings. These embeddings capture the semantic meaning of the tokens and their relationships with other tokens.\n",
    "\n",
    "4. *Processing:* The embeddings of the tokens are fed into the model's neural network architecture. This network consists of multiple layers of processing units (neurons) that transform the input embeddings through various mathematical operations.\n",
    "\n",
    "5. *Contextual Understanding:* As the input propagate through the network, the model learns to understand the contextual relationships between the tokens. It allow the model to focus on relevant parts of the input.\n",
    "\n",
    "6. *Prediction:* Based on its understanding of the input text and the context provided, the model generates a response. \n",
    "\n",
    "7. *Output:* The model outputs the predicted tokens, which can be used to generate text or to perform other tasks such as text classification, translation, or summarization.\n",
    "\n",
    "# Large Language Model\n",
    "Large language model is a machine learning model designed to understand, generate, and manipulate human language on a vast scale. These models are typically built using deep learning techniques, especially variants of the transformer architecture, and are trained on massive datasets of text from the internet and other sources.\n",
    "\n",
    "# Generative AI\n",
    "Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.\n",
    "\n",
    "## Quick Information\n",
    "- GPT(Generative Pre-trained Transformer) is a series of llm developed by OpenAI\n",
    "- ChatGPT is a generative AI specifically fine-tuned for conversational interactions.\n",
    "- OpenAI's work best with JSON while Anthropic's models work best with XML.\n",
    "\n",
    "# Langchain\n",
    "LangChain is an open source framework for building applications based on large language models (LLMs). It provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. Basically it integrate ai(LL model) with web/mobile applications. By abstracting complexities, it simplifies the process compared to direct integration, making it more accessible and manageable. The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11d6ea-6852-480b-9e1a-2119da1046fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80c8d7-7144-44b8-85c1-5f8049221510",
   "metadata": {},
   "source": [
    "# Prompt Templates\n",
    "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, that provides additional context on the specific task at hand so that llm can understand user input more efficiently.\n",
    "\n",
    "Typically, language models expect the prompt to either be a string or else a list of chat messages. Use `PromptTemplate` to create a template for a string prompt and `ChatPromptTemplate` to create a list of messages\n",
    "\n",
    "If the user only had to provide the description of a specific topic but not the instruction that model needs, it would be great!! PromptTemplates help with exactly this! It bundle up all the logic & instruction going from user input into a fully fromatted prompt that llm model required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decb7ace-da85-4dc3-9eb0-125b60a75f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "formatted_prompt = prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23955445-7821-4ecb-ab29-5dbb31ab68d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], input_types={}, partial_variables={}, template='What is a good name for a company that makes {product}?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b387fa1-dde7-46f3-8123-38be115273de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba9baf-a569-41cd-8998-ccc369cc5810",
   "metadata": {},
   "source": [
    "__Reference:__ [PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0565970-cd76-4423-a7e5-873f336466b5",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "Each chat message is associated with content, and an additional parameter called `role`. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c69631f7-9f01-4bc6-8d02-e9f004d44633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_template = \"You are a helpful AI bot. Your name is {name}.\"\n",
    "human_template = \"Hello, how are you doing?\"\n",
    "ai_template = \"You are a helpful AI bot. Your name is {name}.\"\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content = system_template),\n",
    "        HumanMessage(content = human_template),\n",
    "        AIMessage(content = ai_template),\n",
    "        HumanMessage(content = \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "formatted_messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dcf7689-e7e5-4fcf-9181-a7320e325a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], input_types={}, partial_variables={}, messages=[SystemMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}), HumanMessage(content='{user_input}', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72427e5e-c43f-4b24-a518-134f501b4dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You are a helpful AI bot. Your name is {name}.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='{user_input}', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b395e7-0f20-4eb6-887a-e11eb7f93913",
   "metadata": {},
   "source": [
    "### Message types\n",
    "ChatModels take a list of messages as input and return a message. There are a few different types of messages. All messages have a `role` and a `content` property. The `role` describes WHO is saying the message. LangChain has different message classes for different roles. The `content` property describes the content of the message.\n",
    "\n",
    "1. **SystemMessage:**\n",
    "   \n",
    "   Purpose:\n",
    "   - Provides instructions or guidelines to the AI.\n",
    "   - Used to set up context or guide AI behavior.\n",
    "   - AI does not remember previous messages, so this helps in maintaining consistency.\n",
    "     \n",
    "   When to Use:\n",
    "    - At the start of a conversation to set behavior rules.\n",
    "    - Helps AI maintain a persona, like a \"support agent\" or a \"math tutor.\"\n",
    "2. **HumanMessage:**\n",
    "\n",
    "   Purpose:\n",
    "   - Represents messages coming from the user.\n",
    "  \n",
    "   When to Use:\n",
    "   - Every time a user interacts with the chatbot.\n",
    "   - Capturing user input in chatbot applications.\n",
    "   \n",
    "3. **AIMessage:**\n",
    "\n",
    "   Purpose:\n",
    "   - Represents messages generated by the AI.\n",
    "\n",
    "   When to Use:\n",
    "   - Whenever the AI generates a response.\n",
    "   - Storing past AI-generated replies for conversation memory.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bb6f2-c375-4630-bdcd-763535a31484",
   "metadata": {},
   "source": [
    "__Reference:__ [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530282d-26cb-45ec-8b3b-a8ecc6e14052",
   "metadata": {},
   "source": [
    "# FewShotPromptTemplate\n",
    "\n",
    "It refers to providing a few examples (few-shot examples) in the input prompt to guide the model on how to respond to similar queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08901423-5b3e-4749-b338-63454cbf9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15474fd1-31a4-4c17-bc18-71c537e42f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19340905-1482-4e76-897b-c46dbafa12e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'question'], input_types={}, partial_variables={}, template='Question: {question}\\n{answer}')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d06435e6-c573-4540-9b1a-fefa65e08326",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c91c602-353d-4afa-bf20-46633979cd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, examples=[{'question': 'Who lived longer, Muhammad Ali or Alan Turing?', 'answer': '\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Muhammad Ali when he died?\\nIntermediate answer: Muhammad Ali was 74 years old when he died.\\nFollow up: How old was Alan Turing when he died?\\nIntermediate answer: Alan Turing was 41 years old when he died.\\nSo the final answer is: Muhammad Ali\\n'}], example_prompt=PromptTemplate(input_variables=['answer', 'question'], input_types={}, partial_variables={}, template='Question: {question}\\n{answer}'), suffix='Question: {input}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afcc78-d348-4b47-9a65-4c91427e80cb",
   "metadata": {},
   "source": [
    "# Output parsers\n",
    "A  utility that helps transform the output of a language model into a structured format that your application can work with. This is particularly useful when you want to extract specific information or ensure the output adheres to a certain structure.\n",
    "## Types\n",
    "- CSV\n",
    "- Datetime\n",
    "- Enum\n",
    "- JSON\n",
    "### DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b8d4554-4a13-419e-a7a8-f3ceb0a33c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "output_parser = DatetimeOutputParser()\n",
    "datetime_output_parser = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cc765a8-ec11-4045-abfa-288e0cb67087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 1802-10-02T00:32:31.351295Z, 1917-09-21T04:59:10.263771Z, 1442-09-01T23:42:06.103897Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ed915-c78f-4f11-887f-b11171529ec5",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6befe503-bf71-4ab2-920f-48a7c6e877f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{question} \\n \\n {format_instruction}\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formatted_message = prompt.format(question=\"when bitcoin was invented\", format_instruction=output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20bf22b-af27-4c63-b6f6-7bbc6fa33154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"when bitcoin was invented \\n \\n Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 0016-12-30T15:20:24.750449Z, 1617-10-04T04:12:45.365784Z, 0694-05-29T07:50:44.438988Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae6e7-d240-4d40-9692-a4429ce2be73",
   "metadata": {},
   "source": [
    "# Model\n",
    "Language models in LangChain come in two flavors:\n",
    "\n",
    "__ChatModels:__ The ChatModel objects take a list of messages as input and output a message. Chat models are often backed by LLMs but tuned specifically for having conversations. Chat models are designed for multi-turn conversations. They remember previous messages in a session.\n",
    "\n",
    "When to Use Chat Models?\n",
    "- When you need conversational memory\n",
    "- Multi-turn interactions like chatbots, virtual assistants\n",
    "- Best for context-aware applications\n",
    "\n",
    "__LLM:__ LLMs in LangChain refer to pure text completion models. The LLM objects take string as input and output string. OpenAI's GPT-3 is implemented as an LLM. These models generate text based on a given prompt. They are stateless, meaning they don’t remember previous interactions.\n",
    "\n",
    "When to Use LLMs?\n",
    "- When you need a single-response completion\n",
    "- Tasks like summarization, text generation, question-answering\n",
    "\n",
    "The LLM returns a string, while the ChatModel returns a message. The main difference between them is their input and output schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f33d2-3c18-4a61-af6a-c66956607f0d",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e704ee-a07f-457a-a5f9-71e335aac0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842352f-1b07-49c2-8b7c-c3ae208e2861",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cd125-db5a-4e60-8788-893412fcabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",api_key=\"...\")\n",
    "response = openai_llm.invoke(\"What is the capital of Japan?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfab4c9-bff0-4013-8dd6-cc70826497e4",
   "metadata": {},
   "source": [
    "### Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75609a3-a00e-49d4-8218-3d55fa425a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Who won the FIFA World Cup in 2018?\"),\n",
    "    AIMessage(content=\"France won the FIFA World Cup in 2018.\"),\n",
    "    HumanMessage(content=\"Who was the top scorer?\")\n",
    "]\n",
    "\n",
    "response = openai_llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817e873-23a1-499f-995a-78ae8575fcf3",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b06d09-4d56-4d5f-bf21-7dc68bf7d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "huggingfacehub_api_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b03355-f87a-4b9e-ab44-41782ab6addc",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e77be-6fea-40ad-bdce-fc6f48f87dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f046f-8dd5-4486-93c4-3228737b251d",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e36e55-bab0-45b9-80ae-7263bb1f127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangChain is a framework for developing applications powered by language models. Here are some benefits of using it:\n",
      "\n",
      "1. **Simplified Language Model Interaction**: LangChain simplifies the process of interacting with language models by providing a structured way to define prompts, manage inputs and outputs, and handle errors.\n",
      "\n",
      "2. **Chain Together Prompts**: LangChain allows you to chain prompts together, enabling you to create complex workflows and pipelines. This is particularly useful when you need to break down a task into multiple steps.\n",
      "\n",
      "3. **Customizable and Extensible**: LangChain is designed to be customizable and extensible. You can define your own prompts, adjust parameters, and even create your own chains.\n",
      "\n",
      "4. **Integration with Various Language Models**: LangChain supports integration with various language models, including those from Hugging Face, and can also work with API-based models like those provided by services like OpenAI.\n",
      "\n",
      "5. **Easy to Use and Documented**: LangChain has a user-friendly API and is well-documented, making it easy to get started with. It also has a growing community and ecosystem.\n",
      "\n",
      "6. **Flexibility**: LangChain can be used in a variety of ways, from simple prompt engineering to complex agent-based systems.\n",
      "\n",
      "7. **Prompts as First-Class Citizens**: In LangChain, prompts are first-class citizens, meaning they can be defined, reused, and even composed with other prompts.\n",
      "\n",
      "Here's a simple example of how to use LangChain to define and use a prompt:\n",
      "\n",
      "```python\n",
      "from langchain import PromptTemplate\n",
      "\n",
      "template = \"Once upon a time in the {time}, there was a {character} who {action}.\"\n",
      "prompt = PromptTemplate(input_variables=[\"time\", \"character\", \"action\"], template=template)\n",
      "\n",
      "output = prompt.format(time=\"old days\", character=\"a brave knight\", action=\"fought a dragon\")\n",
      "print(output)\n",
      "```\n",
      "\n",
      "In this example, we define a prompt template with placeholders for \"time\", \"character\", and \"action\". We then use the `format` method to fill in these placeholders and generate a story.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "huggingface_llm = HuggingFaceEndpoint(repo_id = repo_id, huggingfacehub_api_token = huggingfacehub_api_token, task=\"text-generation\")\n",
    "response = huggingface_llm.invoke(\"What are the benefits of using LangChain?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "122e3d7b-ac55-4ade-829b-7b4c5c2dc13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Who won the FIFA World Cup in 2018?\"),\n",
    "    AIMessage(content=\"France won the FIFA World Cup in 2018.\"),\n",
    "    HumanMessage(content=\"Who was the top scorer?\")\n",
    "]\n",
    "\n",
    "response = huggingface_llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aaebc8-4d0f-41bc-9541-951ef6ff8fe2",
   "metadata": {},
   "source": [
    "__Reference:__ [OpenAI Model List](https://platform.openai.com/docs/models), [OpenAI](https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html), [ChatOpenAI](https://api.python.langchain.com/en/latest/llms/langchain_openai.llms.base.OpenAI.html), [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83bb18-00a5-45b2-ae76-f896251b5876",
   "metadata": {},
   "source": [
    "# Document Loader\n",
    "It is used to load and preprocess documents for further processing, such as splitting into smaller chunks, extracting embeddings, or integrating them into pipelines for tasks like question answering or summarization.\n",
    "\n",
    "## Types\n",
    "- TextLoader\n",
    "- PyPDFLoader\n",
    "- Docx2txtLoader\n",
    "- UnstructuredURLLoader(HTML)\n",
    "- WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d6f567-77e9-4030-b139-2b1dcc5ed948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"T:/project/programming_notes/ai/pandas.md\")\n",
    "documents = loader.load()\n",
    "# print(documents[0])\n",
    "# print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beba153-bc43-49f3-b812-ae6cf38bed4b",
   "metadata": {},
   "source": [
    "## TextSplitter\n",
    "It used to split large chunks of text into smaller, manageable pieces.\n",
    "\n",
    "`CharacterTextSplitter` splits text by character length with optional overlap. It is ideal for fine-tuning chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a13c04b-4a4b-44c5-b172-5f0210e47d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 146, which is longer than the specified 50\n",
      "Created a chunk of size 175, which is longer than the specified 50\n",
      "Created a chunk of size 145, which is longer than the specified 50\n",
      "Created a chunk of size 161, which is longer than the specified 50\n",
      "Created a chunk of size 185, which is longer than the specified 50\n",
      "Created a chunk of size 319, which is longer than the specified 50\n",
      "Created a chunk of size 227, which is longer than the specified 50\n",
      "Created a chunk of size 130, which is longer than the specified 50\n",
      "Created a chunk of size 204, which is longer than the specified 50\n",
      "Created a chunk of size 205, which is longer than the specified 50\n",
      "Created a chunk of size 105, which is longer than the specified 50\n",
      "Created a chunk of size 81, which is longer than the specified 50\n",
      "Created a chunk of size 396, which is longer than the specified 50\n",
      "Created a chunk of size 70, which is longer than the specified 50\n",
      "Created a chunk of size 164, which is longer than the specified 50\n",
      "Created a chunk of size 151, which is longer than the specified 50\n",
      "Created a chunk of size 143, which is longer than the specified 50\n",
      "Created a chunk of size 440, which is longer than the specified 50\n",
      "Created a chunk of size 279, which is longer than the specified 50\n",
      "Created a chunk of size 457, which is longer than the specified 50\n",
      "Created a chunk of size 150, which is longer than the specified 50\n",
      "Created a chunk of size 554, which is longer than the specified 50\n",
      "Created a chunk of size 106, which is longer than the specified 50\n",
      "Created a chunk of size 92, which is longer than the specified 50\n",
      "Created a chunk of size 144, which is longer than the specified 50\n",
      "Created a chunk of size 419, which is longer than the specified 50\n",
      "Created a chunk of size 105, which is longer than the specified 50\n",
      "Created a chunk of size 114, which is longer than the specified 50\n",
      "Created a chunk of size 98, which is longer than the specified 50\n",
      "Created a chunk of size 1304, which is longer than the specified 50\n",
      "Created a chunk of size 108, which is longer than the specified 50\n",
      "Created a chunk of size 119, which is longer than the specified 50\n",
      "Created a chunk of size 217, which is longer than the specified 50\n",
      "Created a chunk of size 308, which is longer than the specified 50\n",
      "Created a chunk of size 111, which is longer than the specified 50\n",
      "Created a chunk of size 174, which is longer than the specified 50\n",
      "Created a chunk of size 108, which is longer than the specified 50\n",
      "Created a chunk of size 435, which is longer than the specified 50\n",
      "Created a chunk of size 353, which is longer than the specified 50\n",
      "Created a chunk of size 1171, which is longer than the specified 50\n",
      "Created a chunk of size 300, which is longer than the specified 50\n",
      "Created a chunk of size 491, which is longer than the specified 50\n",
      "Created a chunk of size 495, which is longer than the specified 50\n",
      "Created a chunk of size 716, which is longer than the specified 50\n",
      "Created a chunk of size 79, which is longer than the specified 50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc93aa2d-63f2-4dfc-931d-4d0e0395d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, split_doc in enumerate(split_docs):\n",
    "#     print(f\"Chunk {i+1}:\\n{split_doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1148902e-34ff-4fe3-bd62-274669961eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'T:/project/programming_notes/ai/pandas.md'}, page_content='Pandas is used for working with data sets, it is used to analyze data. It has functions for analyzing, cleaning, exploring, and manipulating data.'),\n",
       " Document(metadata={'source': 'T:/project/programming_notes/ai/pandas.md'}, page_content='__What Can Pandas Do?__\\nPandas gives you answers about the data. Like:\\n- Is there a correlation between two or more columns?\\n- What is average value?\\n- Max value?\\n- Min value?')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c43b8-6642-4041-bac9-c5ed1fcf3259",
   "metadata": {},
   "source": [
    "# Chains\n",
    "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step where response from an LLM is act as an input from another LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2f53cf0-fc90-46f5-915c-6203c56073e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "\n",
    "title_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a short and catchy blog title (no more than 10 words) about {topic}. ONLY return the title, nothing else.\"\n",
    ")\n",
    "title_chain = LLMChain(llm=huggingface_llm, prompt=title_prompt, output_key=\"title\")  # Output title only\n",
    "\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\"],\n",
    "    template=\"Write a detailed blog post based on the title: {title}.\"\n",
    ")\n",
    "content_chain = LLMChain(llm=huggingface_llm, prompt=content_prompt, output_key=\"content\")  # Output content\n",
    "\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[title_chain, content_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"title\", \"content\"],\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "topic = \"Blockchain Technology\"\n",
    "response = sequential_chain.invoke({\"topic\": topic})\n",
    "\n",
    "# print(f\"📝 Topic: {topic}\")\n",
    "# print(f\"📌 Title: {response['title']}\")\n",
    "# print(f\"✍️ Blog Content: {response['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa54e45-3c42-43ad-8875-be9b0dc24fb9",
   "metadata": {},
   "source": [
    "# Memory\n",
    "LangChain provides memory as a way to store and manage conversational history across multiple interactions with an LLM. By default, LLMs process each input independently (stateless), meaning they do not remember previous interactions.\n",
    "\n",
    "## Types of Memory\n",
    "1. **Simple Memory - `ConversationBufferMemory`**\n",
    "    - Stores past interactions in a buffer (list of messages).\n",
    "    - Can be useful for short conversations but may become inefficient as memory grows.\n",
    "2. **Token-limited Memory - `ConversationBufferWindowMemory`**\n",
    "    - Maintains only the last 'N' interactions to avoid excessive memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544ab33-452b-42fd-9add-2b93b2e93745",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f698a875-b924-453e-8648-8923c8e1d598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, your name is Masum Billah.\\nHuman: Thank you. I will remember that Langchain is built on Python for future reference.\\nAI: You're welcome, Masum Billah! I'm glad I could help. If you have any other questions, feel free to ask!\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "buffer_memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=huggingface_llm, memory=buffer_memory)\n",
    "\n",
    "conversation.run(\"Hello! How are you?\")\n",
    "conversation.run(\"My name is Masum Billah\")\n",
    "conversation.run(\"Can you guide me to learn langchain\")\n",
    "conversation.run(\"Can you remember what is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28463d04-b149-4e13-8091-5d0493e19d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(buffer_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b4e35-367b-4e67-b1ad-a183ca8d5091",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd813781-429b-406e-b4f5-a94b5b48db5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, your name is Masum Billah.\\nHuman: Thank you. I will remember that Langchain is built on Python for future reference.\\nAI: You're welcome, Masum Billah! I'm glad I could help. If you have any other questions, feel free to ask!\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "buffer_window_memory = ConversationBufferWindowMemory(k=3)\n",
    "conversation = ConversationChain(llm=huggingface_llm, memory=buffer_window_memory)\n",
    "\n",
    "buffer_window_memory.clear()\n",
    "\n",
    "conversation.run(\"Hello! How are you?\")\n",
    "conversation.run(\"My name is Masum Billah\")\n",
    "conversation.run(\"Can you guide me to learn langchain\")\n",
    "conversation.run(\"Can you remember what is my name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0caf84f-27e2-4e1c-b79e-99b1c1e90ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(buffer_window_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2269d-6a21-4493-b35f-9af71ecc3a50",
   "metadata": {},
   "source": [
    "## ChatMessageHistory\n",
    "It is used to store and manage conversational history as a list of messages. It helps in maintaining chat history without requiring a full-fledged memory module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a526d82c-0cdb-4ce6-80af-ccf205c80b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: Hello, how are you?\n",
      "ai: I'm good! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "chat_history.add_user_message(\"Hello, how are you?\")\n",
    "chat_history.add_ai_message(\"I'm good! How can I assist you today?\")\n",
    "\n",
    "for msg in chat_history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
