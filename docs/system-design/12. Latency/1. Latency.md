

Latency in system design refers to the time it takes for a request to travel from the client to the server and back with a response. It is a key performance metric that reflects how responsive a system is.

## Types of Latency

1. **Network Latency:** Time taken for data to travel across the network(optical cable) from client to server and back.
2. **Processing Latency:** Time taken by the server or application to process a request.
3. **Queueing Latency:** Time a request waits in a queue before being processed (e.g., due to load, rate limits, etc.).
4. **Disk I/O Latency:** Time required to read/write data from storage systems.
5. **Database Latency:** Time taken to query the database and return the result.

## Why Latency Matters

- User Experience (slower pages, timeouts)
- System Throughput
- Real-time systems (e.g., video conferencing, trading platforms)
- Scalability

## Latency Breakdown

Letâ€™s assume a web app that fetches user data:

```shell
Client <--> API Server <--> Database
```

Typical latency breakdown might look like:

- Network: 50ms (client to server)
- Processing: 20ms
- DB Query: 100ms
- Response Time: 50ms (back to client)

Total Latency: ~220ms

## Example of Latency

```js
const express = require("express");
const app = express();

const port = 3000;

// Simulated DB query with delay
function fakeDBQuery(userId) {
  return new Promise((resolve) => {
    setTimeout(() => {
      resolve({ id: userId, name: "Masum Billah", role: "Admin" });
    }, 100); // 100ms DB latency
  });
}

app.get("/user/:id", async (req, res) => {
  const start = Date.now(); // Start timer

  const user = await fakeDBQuery(req.params.id);

  const end = Date.now(); // End timer
  const latency = end - start;

  res.json({
    user,
    latency: `${latency}ms`, // Show total processing latency
  });
});

app.listen(port, () => {
  console.log(`Server running on http://localhost:${port}`);
});
```

## How to Reduce Latency

| Area              | Techniques                                           |
| ----------------- | ---------------------------------------------------- |
| **Network**       | Use CDNs, HTTP/2, minimize payload size              |
| **Processing**    | Optimize algorithms, reduce blocking code            |
| **Database**      | Use indexes, caching (e.g., Redis), optimize queries |
| **Queueing**      | Add workers, increase concurrency                    |
| **Architecture**  | Use microservices, load balancers, edge servers      |
| **Caching**       | Avoid repeated DB queries                            |
| **Load Balancer** | Distribute traffic                                   |
| **Rate limiting** | Throttle excessive calls                             |

Try to achieve 100ms latency.
