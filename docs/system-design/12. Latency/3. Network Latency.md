

Network latency refers to the time delay experienced in a system when data is transmitted from one point to another over a network. It is a critical aspect of distributed systems, client-server architectures, and microservices, where components communicate over networks.

- Measured typically in milliseconds (ms).
- Often described as RTT (Round Trip Time) – the time it takes for a request to go from sender to receiver and back.

## Components of Network Latency

1. **Propagation Delay**
   - Time for a signal to travel through the medium (e.g., fiber optics).
   - Depends on distance and medium speed.
   - Example: Light travels ~200,000 km/s in fiber → 1000 km ≈ 5 ms.
2. **Transmission Delay**
   - Time to push bits onto the wire.
   - Depends on packet size and bandwidth.
   - Formula: Packet size / Bandwidth
3. **Processing Delay**
   - Time for routers/switches to process packet headers and routing logic.
4. **Queuing Delay**
   - Time packets wait in queue due to network congestion or traffic shaping.

## Techniques to Reduce Network Latency

| Technique                           | Description                                             |
| ----------------------------------- | ------------------------------------------------------- |
| **Caching**                         | Use CDN/memory cache to reduce server round trips.      |
| **Data Aggregation**                | Combine multiple service calls into one request.        |
| **CDNs (Content Delivery Network)** | Place content closer to user geolocation.               |
| **Asynchronous Processing**         | Offload non-critical tasks to background queues.        |
| **Connection Optimization**         | Keep-alive, HTTP/2 multiplexing, gRPC over HTTP/2, etc. |
| **Edge Computing**                  | Move computation closer to users/devices.               |
| **Compression**                     | Reduce data size over the network.                      |
