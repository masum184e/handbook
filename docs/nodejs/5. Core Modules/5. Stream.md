A stream is an abstract interface in Node.js used to read or write data sequentially, piece by piece, instead of loading the entire data into memory at once.

Streams are provided by the core `stream` module, and many core APIs (HTTP, FS, Zlib) are built on top of it.

```
const { Readable, Writable, Duplex, Transform } = require("stream");
```

Why streams exist

- Without streams:
  - Large files must be fully loaded into memory
  - Memory usage spikes
  - Performance suffers
- With streams:
  - Data is processed in chunks
  - Low memory usage
  - High performance

## Stream vs Buffer (Important Concept)

| Method                   | Purpose          |
| ------------------------ | ---------------- |
| `os.platform()`          | OS platform      |
| `os.type()`              | OS name          |
| `os.arch()`              | CPU architecture |
| `os.cpus()`              | CPU info         |
| `os.totalmem()`          | Total memory     |
| `os.freemem()`           | Free memory      |
| `os.uptime()`            | System uptime    |
| `os.networkInterfaces()` | Network details  |
| `os.homedir()`           | Home directory   |
| `os.tmpdir()`            | Temp directory   |
| `os.EOL`                 | Line ending      |

## Types of Streams in Node.js

Node.js provides four main stream types:

| Type      | Description  | Example                  |
| --------- | ------------ | ------------------------ |
| Readable  | Read data    | `fs.createReadStream()`  |
| Writable  | Write data   | `fs.createWriteStream()` |
| Duplex    | Read & write | TCP sockets              |
| Transform | Modify data  | Compression, encryption  |

## Readable Streams

Example: Reading a File Stream

```
const fs = require("fs");

const readStream = fs.createReadStream("data.txt", {
  encoding: "utf8",
  highWaterMark: 16
});

readStream.on("data", chunk => {
  console.log("Chunk:", chunk);
});

readStream.on("end", () => {
  console.log("Finished reading file");
});

readStream.on("error", err => {
  console.error(err);
});
```

- `data` → emitted for each chunk
- `highWaterMark` → chunk size (bytes)
- `end` → no more data
- `error` → stream error

## Writable Streams

Example: Writing to a File Stream

```
const fs = require("fs");

const writeStream = fs.createWriteStream("output.txt");

writeStream.write("Hello ");
writeStream.write("Node Streams!");
writeStream.end();
```

- `write()` sends data to stream
- `end()` closes the stream
- Data is written gradually

## Piping Streams (MOST IMPORTANT FEATURE)

Example: Copy a File Efficiently

```
const fs = require("fs");

const readStream = fs.createReadStream("bigfile.txt");
const writeStream = fs.createWriteStream("copy.txt");

readStream.pipe(writeStream);
```

Why `pipe()` matters

- Automatically manages:
  - Backpressure
  - Errors
  - Flow control
- Cleaner and safer

## Backpressure (Critical Concept)

Backpressure happens when:

- Data is produced faster than it can be consumed

Streams handle this internally when using `pipe()`.

Without handling backpressure:

- Memory grows uncontrollably
- App can crash

## Duplex Streams

A Duplex stream can read and write.

Example: TCP Socket (Conceptual)

```
const net = require("net");

const server = net.createServer(socket => {
  socket.write("Hello client");
  socket.on("data", data => {
    console.log("Received:", data.toString());
  });
});

server.listen(3000);
```

## Transform Streams

Transform streams modify data as it passes through.

Example: Uppercase Transform

```
const { Transform } = require("stream");

const upperCase = new Transform({
  transform(chunk, encoding, callback) {
    callback(null, chunk.toString().toUpperCase());
  }
});

process.stdin
  .pipe(upperCase)
  .pipe(process.stdout);
```

- Input → transformed → output
- Same stream reads and writes

## Streams with HTTP

Example: Stream a File to Browser

```
const fs = require("fs");
const http = require("http");

http.createServer((req, res) => {
  const stream = fs.createReadStream("video.mp4");

  res.writeHead(200, { "Content-Type": "video/mp4" });
  stream.pipe(res);
}).listen(3000);
```

Why this is ideal

- Starts sending immediately
- Handles large files efficiently

## Stream Modes

Binary Mode (Default)

```
fs.createReadStream("file.bin");
```

Object Mode

```
const { Readable } = require("stream");

const readable = new Readable({
  objectMode: true,
  read() {
    this.push({ id: 1 });
    this.push(null);
  }
});
```

## Error Handling (Always Required)

```
readStream.on("error", err => {
  console.error("Stream error:", err);
});
```

When piping:

```
readStream.pipe(writeStream).on("error", console.error);
```

## Streams Internals (High Level)

- Streams extend `EventEmitter`
- Key events: `data`, `end`, `error`, `finish`
- Controlled by Node’s event loop

## When Should You Use Streams?

- Use streams when:
  - Working with large files
  - Handling network data
  - Processing real-time data
- Avoid streams when:
  - Data is small and simple
  - Logic becomes harder than necessary

## Summary Cheat Sheet

| Feature      | Purpose                   |
| ------------ | ------------------------- |
| Streams      | Chunk-based data handling |
| Readable     | Consume data              |
| Writable     | Produce data              |
| Duplex       | Read & write              |
| Transform    | Modify data               |
| pipe()       | Connect streams           |
| Backpressure | Flow control              |
